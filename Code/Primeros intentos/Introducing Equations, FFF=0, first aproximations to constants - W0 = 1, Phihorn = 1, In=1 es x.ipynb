{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "pd.options.plotting.backend = 'plotly'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cuadrados 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-5e7b1d497cad>:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pddopen5 = pd.read_csv('OPEN/txt/5x5/5x5pdd.txt', delimiter='\t\t', names=['r', 'señal open 5x5', 'error open 5x5'])\n",
      "<ipython-input-2-5e7b1d497cad>:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pcropen5 = pd.read_csv('OPEN/txt/5x5/5x5pro-cr-100.txt', delimiter='\t\t', names=['r', 'señal open 5x5', 'error open 5x5'])\n",
      "<ipython-input-2-5e7b1d497cad>:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pinopen5 = pd.read_csv('OPEN/txt/5x5/5x5pro-in-100.txt', delimiter='\t\t', names=['r', 'señal open 5x5', 'error open 5x5'])\n",
      "<ipython-input-2-5e7b1d497cad>:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pddFFF5 = pd.read_csv('FFF/txt/5x5/5x5pdd.txt', delimiter='\t\t', names=['r', 'señal fff 5x5', 'error fff 5x5'])\n",
      "<ipython-input-2-5e7b1d497cad>:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pcrFFF5 = pd.read_csv('FFF/txt/5x5/5x5pro-cr-100.txt', delimiter='\t\t', names=['r', 'señal fff 5x5', 'error fff 5x5'])\n",
      "<ipython-input-2-5e7b1d497cad>:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pinFFF5 = pd.read_csv('FFF/txt/5x5/5x5pro-in-100.txt', delimiter='\t\t', names=['r', 'señal fff 5x5', 'error fff 5x5'])\n"
     ]
    }
   ],
   "source": [
    "pddopen5 = pd.read_csv('OPEN/txt/5x5/5x5pdd.txt', delimiter='\t\t', names=['r', 'señal open 5x5', 'error open 5x5'])\n",
    "pcropen5 = pd.read_csv('OPEN/txt/5x5/5x5pro-cr-100.txt', delimiter='\t\t', names=['r', 'señal open 5x5', 'error open 5x5'])\n",
    "pinopen5 = pd.read_csv('OPEN/txt/5x5/5x5pro-in-100.txt', delimiter='\t\t', names=['r', 'señal open 5x5', 'error open 5x5'])\n",
    "\n",
    "pddFFF5 = pd.read_csv('FFF/txt/5x5/5x5pdd.txt', delimiter='\t\t', names=['r', 'señal fff 5x5', 'error fff 5x5'])\n",
    "pcrFFF5 = pd.read_csv('FFF/txt/5x5/5x5pro-cr-100.txt', delimiter='\t\t', names=['r', 'señal fff 5x5', 'error fff 5x5'])\n",
    "pinFFF5 = pd.read_csv('FFF/txt/5x5/5x5pro-in-100.txt', delimiter='\t\t', names=['r', 'señal fff 5x5', 'error fff 5x5'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10x10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-5559e724810d>:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pddopen10 = pd.read_csv('OPEN/txt/10x10/10x10pdd.txt', delimiter='\t\t', names=['r', 'señal open 10x10', 'error open 10x10'])\n",
      "<ipython-input-3-5559e724810d>:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pcropen10 = pd.read_csv('OPEN/txt/10x10/10x10pro-cr-100.txt', delimiter='\t\t', names=['r', 'señal open 10x10', 'error open 10x10'])\n",
      "<ipython-input-3-5559e724810d>:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pinopen10 = pd.read_csv('OPEN/txt/10x10/10x10pro-in-100.txt', delimiter='\t\t', names=['r', 'señal open 10x10', 'error open 10x10'])\n",
      "<ipython-input-3-5559e724810d>:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pddFFF10 = pd.read_csv('FFF/txt/10x10/10x10pdd.txt', delimiter='\t\t', names=['r', 'señal fff 10x10', 'error fff 10x10'])\n",
      "<ipython-input-3-5559e724810d>:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pcrFFF10 = pd.read_csv('FFF/txt/10x10/10x10pro-cr-100.txt', delimiter='\t\t', names=['r', 'señal fff 10x10', 'error fff 10x10'])\n",
      "<ipython-input-3-5559e724810d>:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pinFFF10 = pd.read_csv('FFF/txt/10x10/10x10pro-in-100.txt', delimiter='\t\t', names=['r', 'señal fff 10x10', 'error fff 10x10'])\n"
     ]
    }
   ],
   "source": [
    "pddopen10 = pd.read_csv('OPEN/txt/10x10/10x10pdd.txt', delimiter='\t\t', names=['r', 'señal open 10x10', 'error open 10x10'])\n",
    "pcropen10 = pd.read_csv('OPEN/txt/10x10/10x10pro-cr-100.txt', delimiter='\t\t', names=['r', 'señal open 10x10', 'error open 10x10'])\n",
    "pinopen10 = pd.read_csv('OPEN/txt/10x10/10x10pro-in-100.txt', delimiter='\t\t', names=['r', 'señal open 10x10', 'error open 10x10'])\n",
    "\n",
    "pddFFF10 = pd.read_csv('FFF/txt/10x10/10x10pdd.txt', delimiter='\t\t', names=['r', 'señal fff 10x10', 'error fff 10x10'])\n",
    "pcrFFF10 = pd.read_csv('FFF/txt/10x10/10x10pro-cr-100.txt', delimiter='\t\t', names=['r', 'señal fff 10x10', 'error fff 10x10'])\n",
    "pinFFF10 = pd.read_csv('FFF/txt/10x10/10x10pro-in-100.txt', delimiter='\t\t', names=['r', 'señal fff 10x10', 'error fff 10x10'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20x20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-fc4605168945>:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pddopen20 = pd.read_csv('OPEN/txt/20x20/20x20pdd.txt', delimiter='\t\t', names=['r', 'señal open 20x20', 'error open 20x20'])\n",
      "<ipython-input-4-fc4605168945>:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pcropen20 = pd.read_csv('OPEN/txt/20x20/20x20pro-cr-100.txt', delimiter='\t\t', names=['r', 'señal open 20x20', 'error open 20x20'])\n",
      "<ipython-input-4-fc4605168945>:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pinopen20 = pd.read_csv('OPEN/txt/20x20/20x20pro-in-100.txt', delimiter='\t\t', names=['r', 'señal open 20x20', 'error open 20x20'])\n",
      "<ipython-input-4-fc4605168945>:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pddFFF20 = pd.read_csv('FFF/txt/20x20/20x20pdd.txt', delimiter='\t\t', names=['r', 'señal fff 20x20', 'error fff 20x20'])\n",
      "<ipython-input-4-fc4605168945>:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pcrFFF20 = pd.read_csv('FFF/txt/20x20/20x20pro-cr-100.txt', delimiter='\t\t', names=['r', 'señal fff 20x20', 'error fff 20x20'])\n",
      "<ipython-input-4-fc4605168945>:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pinFFF20 = pd.read_csv('FFF/txt/20x20/20x20pro-in-100.txt', delimiter='\t\t', names=['r', 'señal fff 20x20', 'error fff 20x20'])\n"
     ]
    }
   ],
   "source": [
    "pddopen20 = pd.read_csv('OPEN/txt/20x20/20x20pdd.txt', delimiter='\t\t', names=['r', 'señal open 20x20', 'error open 20x20'])\n",
    "pcropen20 = pd.read_csv('OPEN/txt/20x20/20x20pro-cr-100.txt', delimiter='\t\t', names=['r', 'señal open 20x20', 'error open 20x20'])\n",
    "pinopen20 = pd.read_csv('OPEN/txt/20x20/20x20pro-in-100.txt', delimiter='\t\t', names=['r', 'señal open 20x20', 'error open 20x20'])\n",
    "\n",
    "pddFFF20 = pd.read_csv('FFF/txt/20x20/20x20pdd.txt', delimiter='\t\t', names=['r', 'señal fff 20x20', 'error fff 20x20'])\n",
    "pcrFFF20 = pd.read_csv('FFF/txt/20x20/20x20pro-cr-100.txt', delimiter='\t\t', names=['r', 'señal fff 20x20', 'error fff 20x20'])\n",
    "pinFFF20 = pd.read_csv('FFF/txt/20x20/20x20pro-in-100.txt', delimiter='\t\t', names=['r', 'señal fff 20x20', 'error fff 20x20'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 40x40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-38d7031e937e>:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pddopen40 = pd.read_csv('OPEN/txt/40x40/40x40pdd.txt', delimiter='\t\t', names=['r', 'señal open 40x40', 'error open 40x40'])\n",
      "<ipython-input-5-38d7031e937e>:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pcropen40 = pd.read_csv('OPEN/txt/40x40/40x40pro-cr-100.txt', delimiter='\t\t', names=['r', 'señal open 40x40', 'error open 40x40'])\n",
      "<ipython-input-5-38d7031e937e>:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pinopen40 = pd.read_csv('OPEN/txt/40x40/40x40pro-in-100.txt', delimiter='\t\t', names=['r', 'señal open 40x40', 'error open 40x40'])\n",
      "<ipython-input-5-38d7031e937e>:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pddFFF40 = pd.read_csv('FFF/txt/40x40/40x40pdd.txt', delimiter='\t\t', names=['r', 'señal fff 40x40', 'error fff 40x40'])\n",
      "<ipython-input-5-38d7031e937e>:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pcrFFF40 = pd.read_csv('FFF/txt/40x40/40x40pro-cr-100.txt', delimiter='\t\t', names=['r', 'señal fff 40x40', 'error fff 40x40'])\n",
      "<ipython-input-5-38d7031e937e>:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pinFFF40 = pd.read_csv('FFF/txt/40x40/40x40pro-in-100.txt', delimiter='\t\t', names=['r', 'señal fff 40x40', 'error fff 40x40'])\n"
     ]
    }
   ],
   "source": [
    "pddopen40 = pd.read_csv('OPEN/txt/40x40/40x40pdd.txt', delimiter='\t\t', names=['r', 'señal open 40x40', 'error open 40x40'])\n",
    "pcropen40 = pd.read_csv('OPEN/txt/40x40/40x40pro-cr-100.txt', delimiter='\t\t', names=['r', 'señal open 40x40', 'error open 40x40'])\n",
    "pinopen40 = pd.read_csv('OPEN/txt/40x40/40x40pro-in-100.txt', delimiter='\t\t', names=['r', 'señal open 40x40', 'error open 40x40'])\n",
    "\n",
    "pddFFF40 = pd.read_csv('FFF/txt/40x40/40x40pdd.txt', delimiter='\t\t', names=['r', 'señal fff 40x40', 'error fff 40x40'])\n",
    "pcrFFF40 = pd.read_csv('FFF/txt/40x40/40x40pro-cr-100.txt', delimiter='\t\t', names=['r', 'señal fff 40x40', 'error fff 40x40'])\n",
    "pinFFF40 = pd.read_csv('FFF/txt/40x40/40x40pro-in-100.txt', delimiter='\t\t', names=['r', 'señal fff 40x40', 'error fff 40x40'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cuadrados 115"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-b3dbae33331e>:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pddopen5_115 = pd.read_csv('OPEN/txt/5x5/5x5pdd.txt', delimiter='\t\t', names=['r', 'señal open 5x5', 'error open 5x5'])\n",
      "<ipython-input-6-b3dbae33331e>:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pcropen5_115 = pd.read_csv('OPEN/txt/5x5/5x5pro-cr-115.txt', delimiter='\t\t', names=['r', 'señal open 5x5', 'error open 5x5'])\n",
      "<ipython-input-6-b3dbae33331e>:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pinopen5_115 = pd.read_csv('OPEN/txt/5x5/5x5pro-in-115.txt', delimiter='\t\t', names=['r', 'señal open 5x5', 'error open 5x5'])\n",
      "<ipython-input-6-b3dbae33331e>:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pddFFF5_115 = pd.read_csv('FFF/txt/5x5/5x5pdd.txt', delimiter='\t\t', names=['r', 'señal fff 5x5', 'error fff 5x5'])\n",
      "<ipython-input-6-b3dbae33331e>:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pcrFFF5_115 = pd.read_csv('FFF/txt/5x5/5x5pro-cr-115.txt', delimiter='\t\t', names=['r', 'señal fff 5x5', 'error fff 5x5'])\n",
      "<ipython-input-6-b3dbae33331e>:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pinFFF5_115 = pd.read_csv('FFF/txt/5x5/5x5pro-in-115.txt', delimiter='\t\t', names=['r', 'señal fff 5x5', 'error fff 5x5'])\n"
     ]
    }
   ],
   "source": [
    "pddopen5_115 = pd.read_csv('OPEN/txt/5x5/5x5pdd.txt', delimiter='\t\t', names=['r', 'señal open 5x5', 'error open 5x5'])\n",
    "pcropen5_115 = pd.read_csv('OPEN/txt/5x5/5x5pro-cr-115.txt', delimiter='\t\t', names=['r', 'señal open 5x5', 'error open 5x5'])\n",
    "pinopen5_115 = pd.read_csv('OPEN/txt/5x5/5x5pro-in-115.txt', delimiter='\t\t', names=['r', 'señal open 5x5', 'error open 5x5'])\n",
    "\n",
    "pddFFF5_115 = pd.read_csv('FFF/txt/5x5/5x5pdd.txt', delimiter='\t\t', names=['r', 'señal fff 5x5', 'error fff 5x5'])\n",
    "pcrFFF5_115 = pd.read_csv('FFF/txt/5x5/5x5pro-cr-115.txt', delimiter='\t\t', names=['r', 'señal fff 5x5', 'error fff 5x5'])\n",
    "pinFFF5_115 = pd.read_csv('FFF/txt/5x5/5x5pro-in-115.txt', delimiter='\t\t', names=['r', 'señal fff 5x5', 'error fff 5x5'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10x10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-876a524bf11a>:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pddopen10_115 = pd.read_csv('OPEN/txt/10x10/10x10pdd.txt', delimiter='\t\t', names=['r', 'señal open 10x10', 'error open 10x10'])\n",
      "<ipython-input-7-876a524bf11a>:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pcropen10_115 = pd.read_csv('OPEN/txt/10x10/10x10pro-cr-115.txt', delimiter='\t\t', names=['r', 'señal open 10x10', 'error open 10x10'])\n",
      "<ipython-input-7-876a524bf11a>:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pinopen10_115 = pd.read_csv('OPEN/txt/10x10/10x10pro-in-115.txt', delimiter='\t\t', names=['r', 'señal open 10x10', 'error open 10x10'])\n",
      "<ipython-input-7-876a524bf11a>:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pddFFF10_115 = pd.read_csv('FFF/txt/10x10/10x10pdd.txt', delimiter='\t\t', names=['r', 'señal fff 10x10', 'error fff 10x10'])\n",
      "<ipython-input-7-876a524bf11a>:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pcrFFF10_115 = pd.read_csv('FFF/txt/10x10/10x10pro-cr-115.txt', delimiter='\t\t', names=['r', 'señal fff 10x10', 'error fff 10x10'])\n",
      "<ipython-input-7-876a524bf11a>:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pinFFF10_115 = pd.read_csv('FFF/txt/10x10/10x10pro-in-115.txt', delimiter='\t\t', names=['r', 'señal fff 10x10', 'error fff 10x10'])\n"
     ]
    }
   ],
   "source": [
    "pddopen10_115 = pd.read_csv('OPEN/txt/10x10/10x10pdd.txt', delimiter='\t\t', names=['r', 'señal open 10x10', 'error open 10x10'])\n",
    "pcropen10_115 = pd.read_csv('OPEN/txt/10x10/10x10pro-cr-115.txt', delimiter='\t\t', names=['r', 'señal open 10x10', 'error open 10x10'])\n",
    "pinopen10_115 = pd.read_csv('OPEN/txt/10x10/10x10pro-in-115.txt', delimiter='\t\t', names=['r', 'señal open 10x10', 'error open 10x10'])\n",
    "\n",
    "pddFFF10_115 = pd.read_csv('FFF/txt/10x10/10x10pdd.txt', delimiter='\t\t', names=['r', 'señal fff 10x10', 'error fff 10x10'])\n",
    "pcrFFF10_115 = pd.read_csv('FFF/txt/10x10/10x10pro-cr-115.txt', delimiter='\t\t', names=['r', 'señal fff 10x10', 'error fff 10x10'])\n",
    "pinFFF10_115 = pd.read_csv('FFF/txt/10x10/10x10pro-in-115.txt', delimiter='\t\t', names=['r', 'señal fff 10x10', 'error fff 10x10'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20x20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-97b1090201f8>:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pddopen20_115 = pd.read_csv('OPEN/txt/20x20/20x20pdd.txt', delimiter='\t\t', names=['r', 'señal open 20x20', 'error open 20x20'])\n",
      "<ipython-input-8-97b1090201f8>:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pcropen20_115 = pd.read_csv('OPEN/txt/20x20/20x20pro-cr-115.txt', delimiter='\t\t', names=['r', 'señal open 20x20', 'error open 20x20'])\n",
      "<ipython-input-8-97b1090201f8>:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pinopen20_115 = pd.read_csv('OPEN/txt/20x20/20x20pro-in-115.txt', delimiter='\t\t', names=['r', 'señal open 20x20', 'error open 20x20'])\n",
      "<ipython-input-8-97b1090201f8>:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pddFFF20_115 = pd.read_csv('FFF/txt/20x20/20x20pdd.txt', delimiter='\t\t', names=['r', 'señal fff 20x20', 'error fff 20x20'])\n",
      "<ipython-input-8-97b1090201f8>:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pcrFFF20_115 = pd.read_csv('FFF/txt/20x20/20x20pro-cr-115.txt', delimiter='\t\t', names=['r', 'señal fff 20x20', 'error fff 20x20'])\n",
      "<ipython-input-8-97b1090201f8>:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pinFFF20_115 = pd.read_csv('FFF/txt/20x20/20x20pro-in-115.txt', delimiter='\t\t', names=['r', 'señal fff 20x20', 'error fff 20x20'])\n"
     ]
    }
   ],
   "source": [
    "pddopen20_115 = pd.read_csv('OPEN/txt/20x20/20x20pdd.txt', delimiter='\t\t', names=['r', 'señal open 20x20', 'error open 20x20'])\n",
    "pcropen20_115 = pd.read_csv('OPEN/txt/20x20/20x20pro-cr-115.txt', delimiter='\t\t', names=['r', 'señal open 20x20', 'error open 20x20'])\n",
    "pinopen20_115 = pd.read_csv('OPEN/txt/20x20/20x20pro-in-115.txt', delimiter='\t\t', names=['r', 'señal open 20x20', 'error open 20x20'])\n",
    "\n",
    "pddFFF20_115 = pd.read_csv('FFF/txt/20x20/20x20pdd.txt', delimiter='\t\t', names=['r', 'señal fff 20x20', 'error fff 20x20'])\n",
    "pcrFFF20_115 = pd.read_csv('FFF/txt/20x20/20x20pro-cr-115.txt', delimiter='\t\t', names=['r', 'señal fff 20x20', 'error fff 20x20'])\n",
    "pinFFF20_115 = pd.read_csv('FFF/txt/20x20/20x20pro-in-115.txt', delimiter='\t\t', names=['r', 'señal fff 20x20', 'error fff 20x20'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 40x40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-5deaefc013ea>:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pddopen40_115 = pd.read_csv('OPEN/txt/40x40/40x40pdd.txt', delimiter='\t\t', names=['r', 'señal open 40x40', 'error open 40x40'])\n",
      "<ipython-input-9-5deaefc013ea>:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pcropen40_115 = pd.read_csv('OPEN/txt/40x40/40x40pro-cr-115.txt', delimiter='\t\t', names=['r', 'señal open 40x40', 'error open 40x40'])\n",
      "<ipython-input-9-5deaefc013ea>:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pinopen40_115 = pd.read_csv('OPEN/txt/40x40/40x40pro-in-115.txt', delimiter='\t\t', names=['r', 'señal open 40x40', 'error open 40x40'])\n",
      "<ipython-input-9-5deaefc013ea>:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pddFFF40_115 = pd.read_csv('FFF/txt/40x40/40x40pdd.txt', delimiter='\t\t', names=['r', 'señal fff 40x40', 'error fff 40x40'])\n",
      "<ipython-input-9-5deaefc013ea>:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pcrFFF40_115 = pd.read_csv('FFF/txt/40x40/40x40pro-cr-115.txt', delimiter='\t\t', names=['r', 'señal fff 40x40', 'error fff 40x40'])\n",
      "<ipython-input-9-5deaefc013ea>:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pinFFF40_115 = pd.read_csv('FFF/txt/40x40/40x40pro-in-115.txt', delimiter='\t\t', names=['r', 'señal fff 40x40', 'error fff 40x40'])\n"
     ]
    }
   ],
   "source": [
    "pddopen40_115 = pd.read_csv('OPEN/txt/40x40/40x40pdd.txt', delimiter='\t\t', names=['r', 'señal open 40x40', 'error open 40x40'])\n",
    "pcropen40_115 = pd.read_csv('OPEN/txt/40x40/40x40pro-cr-115.txt', delimiter='\t\t', names=['r', 'señal open 40x40', 'error open 40x40'])\n",
    "pinopen40_115 = pd.read_csv('OPEN/txt/40x40/40x40pro-in-115.txt', delimiter='\t\t', names=['r', 'señal open 40x40', 'error open 40x40'])\n",
    "\n",
    "pddFFF40_115 = pd.read_csv('FFF/txt/40x40/40x40pdd.txt', delimiter='\t\t', names=['r', 'señal fff 40x40', 'error fff 40x40'])\n",
    "pcrFFF40_115 = pd.read_csv('FFF/txt/40x40/40x40pro-cr-115.txt', delimiter='\t\t', names=['r', 'señal fff 40x40', 'error fff 40x40'])\n",
    "pinFFF40_115 = pd.read_csv('FFF/txt/40x40/40x40pro-in-115.txt', delimiter='\t\t', names=['r', 'señal fff 40x40', 'error fff 40x40'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 115"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-8e01dae11aa5>:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pddopen5_85 = pd.read_csv('OPEN/txt/5x5/5x5pdd.txt', delimiter='\t\t', names=['r', 'señal open 5x5', 'error open 5x5'])\n",
      "<ipython-input-10-8e01dae11aa5>:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pcropen5_85 = pd.read_csv('OPEN/txt/5x5/5x5pro-cr-85.txt', delimiter='\t\t', names=['r', 'señal open 5x5', 'error open 5x5'])\n",
      "<ipython-input-10-8e01dae11aa5>:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pinopen5_85 = pd.read_csv('OPEN/txt/5x5/5x5pro-in-85.txt', delimiter='\t\t', names=['r', 'señal open 5x5', 'error open 5x5'])\n",
      "<ipython-input-10-8e01dae11aa5>:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pddFFF5_85 = pd.read_csv('FFF/txt/5x5/5x5pdd.txt', delimiter='\t\t', names=['r', 'señal fff 5x5', 'error fff 5x5'])\n",
      "<ipython-input-10-8e01dae11aa5>:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pcrFFF5_85 = pd.read_csv('FFF/txt/5x5/5x5pro-cr-85.txt', delimiter='\t\t', names=['r', 'señal fff 5x5', 'error fff 5x5'])\n",
      "<ipython-input-10-8e01dae11aa5>:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pinFFF5_85 = pd.read_csv('FFF/txt/5x5/5x5pro-in-85.txt', delimiter='\t\t', names=['r', 'señal fff 5x5', 'error fff 5x5'])\n"
     ]
    }
   ],
   "source": [
    "pddopen5_85 = pd.read_csv('OPEN/txt/5x5/5x5pdd.txt', delimiter='\t\t', names=['r', 'señal open 5x5', 'error open 5x5'])\n",
    "pcropen5_85 = pd.read_csv('OPEN/txt/5x5/5x5pro-cr-85.txt', delimiter='\t\t', names=['r', 'señal open 5x5', 'error open 5x5'])\n",
    "pinopen5_85 = pd.read_csv('OPEN/txt/5x5/5x5pro-in-85.txt', delimiter='\t\t', names=['r', 'señal open 5x5', 'error open 5x5'])\n",
    "\n",
    "pddFFF5_85 = pd.read_csv('FFF/txt/5x5/5x5pdd.txt', delimiter='\t\t', names=['r', 'señal fff 5x5', 'error fff 5x5'])\n",
    "pcrFFF5_85 = pd.read_csv('FFF/txt/5x5/5x5pro-cr-85.txt', delimiter='\t\t', names=['r', 'señal fff 5x5', 'error fff 5x5'])\n",
    "pinFFF5_85 = pd.read_csv('FFF/txt/5x5/5x5pro-in-85.txt', delimiter='\t\t', names=['r', 'señal fff 5x5', 'error fff 5x5'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10x10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-efc97f103422>:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pddopen10_85 = pd.read_csv('OPEN/txt/10x10/10x10pdd.txt', delimiter='\t\t', names=['r', 'señal open 10x10', 'error open 10x10'])\n",
      "<ipython-input-11-efc97f103422>:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pcropen10_85 = pd.read_csv('OPEN/txt/10x10/10x10pro-cr-85.txt', delimiter='\t\t', names=['r', 'señal open 10x10', 'error open 10x10'])\n",
      "<ipython-input-11-efc97f103422>:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pinopen10_85 = pd.read_csv('OPEN/txt/10x10/10x10pro-in-85.txt', delimiter='\t\t', names=['r', 'señal open 10x10', 'error open 10x10'])\n",
      "<ipython-input-11-efc97f103422>:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pddFFF10_85 = pd.read_csv('FFF/txt/10x10/10x10pdd.txt', delimiter='\t\t', names=['r', 'señal fff 10x10', 'error fff 10x10'])\n",
      "<ipython-input-11-efc97f103422>:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pcrFFF10_85 = pd.read_csv('FFF/txt/10x10/10x10pro-cr-85.txt', delimiter='\t\t', names=['r', 'señal fff 10x10', 'error fff 10x10'])\n",
      "<ipython-input-11-efc97f103422>:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pinFFF10_85 = pd.read_csv('FFF/txt/10x10/10x10pro-in-85.txt', delimiter='\t\t', names=['r', 'señal fff 10x10', 'error fff 10x10'])\n"
     ]
    }
   ],
   "source": [
    "pddopen10_85 = pd.read_csv('OPEN/txt/10x10/10x10pdd.txt', delimiter='\t\t', names=['r', 'señal open 10x10', 'error open 10x10'])\n",
    "pcropen10_85 = pd.read_csv('OPEN/txt/10x10/10x10pro-cr-85.txt', delimiter='\t\t', names=['r', 'señal open 10x10', 'error open 10x10'])\n",
    "pinopen10_85 = pd.read_csv('OPEN/txt/10x10/10x10pro-in-85.txt', delimiter='\t\t', names=['r', 'señal open 10x10', 'error open 10x10'])\n",
    "\n",
    "pddFFF10_85 = pd.read_csv('FFF/txt/10x10/10x10pdd.txt', delimiter='\t\t', names=['r', 'señal fff 10x10', 'error fff 10x10'])\n",
    "pcrFFF10_85 = pd.read_csv('FFF/txt/10x10/10x10pro-cr-85.txt', delimiter='\t\t', names=['r', 'señal fff 10x10', 'error fff 10x10'])\n",
    "pinFFF10_85 = pd.read_csv('FFF/txt/10x10/10x10pro-in-85.txt', delimiter='\t\t', names=['r', 'señal fff 10x10', 'error fff 10x10'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20x20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-a68ef06485ef>:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pddopen20_85 = pd.read_csv('OPEN/txt/20x20/20x20pdd.txt', delimiter='\t\t', names=['r', 'señal open 20x20', 'error open 20x20'])\n",
      "<ipython-input-12-a68ef06485ef>:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pcropen20_85 = pd.read_csv('OPEN/txt/20x20/20x20pro-cr-85.txt', delimiter='\t\t', names=['r', 'señal open 20x20', 'error open 20x20'])\n",
      "<ipython-input-12-a68ef06485ef>:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pinopen20_85 = pd.read_csv('OPEN/txt/20x20/20x20pro-in-85.txt', delimiter='\t\t', names=['r', 'señal open 20x20', 'error open 20x20'])\n",
      "<ipython-input-12-a68ef06485ef>:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pddFFF20_85 = pd.read_csv('FFF/txt/20x20/20x20pdd.txt', delimiter='\t\t', names=['r', 'señal fff 20x20', 'error fff 20x20'])\n",
      "<ipython-input-12-a68ef06485ef>:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pcrFFF20_85 = pd.read_csv('FFF/txt/20x20/20x20pro-cr-85.txt', delimiter='\t\t', names=['r', 'señal fff 20x20', 'error fff 20x20'])\n",
      "<ipython-input-12-a68ef06485ef>:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pinFFF20_85 = pd.read_csv('FFF/txt/20x20/20x20pro-in-85.txt', delimiter='\t\t', names=['r', 'señal fff 20x20', 'error fff 20x20'])\n"
     ]
    }
   ],
   "source": [
    "pddopen20_85 = pd.read_csv('OPEN/txt/20x20/20x20pdd.txt', delimiter='\t\t', names=['r', 'señal open 20x20', 'error open 20x20'])\n",
    "pcropen20_85 = pd.read_csv('OPEN/txt/20x20/20x20pro-cr-85.txt', delimiter='\t\t', names=['r', 'señal open 20x20', 'error open 20x20'])\n",
    "pinopen20_85 = pd.read_csv('OPEN/txt/20x20/20x20pro-in-85.txt', delimiter='\t\t', names=['r', 'señal open 20x20', 'error open 20x20'])\n",
    "\n",
    "pddFFF20_85 = pd.read_csv('FFF/txt/20x20/20x20pdd.txt', delimiter='\t\t', names=['r', 'señal fff 20x20', 'error fff 20x20'])\n",
    "pcrFFF20_85 = pd.read_csv('FFF/txt/20x20/20x20pro-cr-85.txt', delimiter='\t\t', names=['r', 'señal fff 20x20', 'error fff 20x20'])\n",
    "pinFFF20_85 = pd.read_csv('FFF/txt/20x20/20x20pro-in-85.txt', delimiter='\t\t', names=['r', 'señal fff 20x20', 'error fff 20x20'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 40x40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-a1eb063f6d0d>:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pddopen40_85 = pd.read_csv('OPEN/txt/40x40/40x40pdd.txt', delimiter='\t\t', names=['r', 'señal open 40x40', 'error open 40x40'])\n",
      "<ipython-input-13-a1eb063f6d0d>:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pcropen40_85 = pd.read_csv('OPEN/txt/40x40/40x40pro-cr-85.txt', delimiter='\t\t', names=['r', 'señal open 40x40', 'error open 40x40'])\n",
      "<ipython-input-13-a1eb063f6d0d>:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pinopen40_85 = pd.read_csv('OPEN/txt/40x40/40x40pro-in-85.txt', delimiter='\t\t', names=['r', 'señal open 40x40', 'error open 40x40'])\n",
      "<ipython-input-13-a1eb063f6d0d>:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pddFFF40_85 = pd.read_csv('FFF/txt/40x40/40x40pdd.txt', delimiter='\t\t', names=['r', 'señal fff 40x40', 'error fff 40x40'])\n",
      "<ipython-input-13-a1eb063f6d0d>:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pcrFFF40_85 = pd.read_csv('FFF/txt/40x40/40x40pro-cr-85.txt', delimiter='\t\t', names=['r', 'señal fff 40x40', 'error fff 40x40'])\n",
      "<ipython-input-13-a1eb063f6d0d>:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pinFFF40_85 = pd.read_csv('FFF/txt/40x40/40x40pro-in-85.txt', delimiter='\t\t', names=['r', 'señal fff 40x40', 'error fff 40x40'])\n"
     ]
    }
   ],
   "source": [
    "pddopen40_85 = pd.read_csv('OPEN/txt/40x40/40x40pdd.txt', delimiter='\t\t', names=['r', 'señal open 40x40', 'error open 40x40'])\n",
    "pcropen40_85 = pd.read_csv('OPEN/txt/40x40/40x40pro-cr-85.txt', delimiter='\t\t', names=['r', 'señal open 40x40', 'error open 40x40'])\n",
    "pinopen40_85 = pd.read_csv('OPEN/txt/40x40/40x40pro-in-85.txt', delimiter='\t\t', names=['r', 'señal open 40x40', 'error open 40x40'])\n",
    "\n",
    "pddFFF40_85 = pd.read_csv('FFF/txt/40x40/40x40pdd.txt', delimiter='\t\t', names=['r', 'señal fff 40x40', 'error fff 40x40'])\n",
    "pcrFFF40_85 = pd.read_csv('FFF/txt/40x40/40x40pro-cr-85.txt', delimiter='\t\t', names=['r', 'señal fff 40x40', 'error fff 40x40'])\n",
    "pinFFF40_85 = pd.read_csv('FFF/txt/40x40/40x40pro-in-85.txt', delimiter='\t\t', names=['r', 'señal fff 40x40', 'error fff 40x40'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Z = 100; In = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFF5 = pinFFF5.iloc[:,:2]\n",
    "FFF5['FFF'] = 1\n",
    "FFF5.columns = ['r', 'señal', 'FFF']\n",
    "FFF5['Area'] = 25\n",
    "FFF5['Limit'] = 25\n",
    "\n",
    "Open5 = pinopen5.iloc[:,:2]\n",
    "Open5['FFF'] = 0\n",
    "Open5.columns = ['r', 'señal', 'FFF']\n",
    "Open5['Area'] = 25\n",
    "Open5['Limit'] = 25\n",
    "\n",
    "FFF10 = pinFFF10.iloc[:,:2]\n",
    "FFF10['FFF'] = 1\n",
    "FFF10.columns = ['r', 'señal', 'FFF']\n",
    "FFF10['Area'] = 100\n",
    "FFF10['Limit'] = 50\n",
    "\n",
    "Open10 = pinopen10.iloc[:,:2]\n",
    "Open10['FFF'] = 0\n",
    "Open10.columns = ['r', 'señal', 'FFF']\n",
    "Open10['Area'] = 100\n",
    "Open10['Limit'] = 50\n",
    "\n",
    "FFF20 = pinFFF20.iloc[:,:2]\n",
    "FFF20['FFF'] = 1\n",
    "FFF20.columns = ['r', 'señal', 'FFF']\n",
    "FFF20['Area'] = 400\n",
    "FFF20['Limit'] = 100\n",
    "\n",
    "Open20 = pinopen20.iloc[:,:2]\n",
    "Open20['FFF'] = 0\n",
    "Open20.columns = ['r', 'señal', 'FFF']\n",
    "Open20['Area'] = 400\n",
    "Open20['Limit'] = 100\n",
    "\n",
    "FFF40 = pinFFF40.iloc[:,:2]\n",
    "FFF40['FFF'] = 1\n",
    "FFF40.columns = ['r', 'señal', 'FFF']\n",
    "FFF40['Area'] = 1600\n",
    "FFF40['Limit'] = 200\n",
    "\n",
    "Open40 = pinopen40.iloc[:,:2]\n",
    "Open40['FFF'] = 0\n",
    "Open40.columns = ['r', 'señal', 'FFF']\n",
    "Open40['Area'] = 1600\n",
    "Open40['Limit'] = 200\n",
    "\n",
    "MixedIN = pd.concat([Open5, FFF5, Open10, FFF10, Open20, FFF20, Open40, FFF40]) \n",
    "MixedIN['In'] = 1\n",
    "MixedIN['Z'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Z = 100; In = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFF5 = pcrFFF5.iloc[:,:2]\n",
    "FFF5['FFF'] = 1\n",
    "FFF5.columns = ['r', 'señal', 'FFF']\n",
    "FFF5['Area'] = 25\n",
    "FFF5['Limit'] = 25\n",
    "\n",
    "Open5 = pcropen5.iloc[:,:2]\n",
    "Open5['FFF'] = 0\n",
    "Open5.columns = ['r', 'señal', 'FFF']\n",
    "Open5['Area'] = 25\n",
    "Open5['Limit'] = 25\n",
    "\n",
    "FFF10 = pcrFFF10.iloc[:,:2]\n",
    "FFF10['FFF'] = 1\n",
    "FFF10.columns = ['r', 'señal', 'FFF']\n",
    "FFF10['Area'] = 100\n",
    "FFF10['Limit'] = 50\n",
    "\n",
    "Open10 = pcropen10.iloc[:,:2]\n",
    "Open10['FFF'] = 0\n",
    "Open10.columns = ['r', 'señal', 'FFF']\n",
    "Open10['Area'] = 100\n",
    "Open10['Limit'] = 50\n",
    "\n",
    "FFF20 = pcrFFF20.iloc[:,:2]\n",
    "FFF20['FFF'] = 1\n",
    "FFF20.columns = ['r', 'señal', 'FFF']\n",
    "FFF20['Area'] = 400\n",
    "FFF20['Limit'] = 100\n",
    "\n",
    "Open20 = pcropen20.iloc[:,:2]\n",
    "Open20['FFF'] = 0\n",
    "Open20.columns = ['r', 'señal', 'FFF']\n",
    "Open20['Area'] = 400\n",
    "Open20['Limit'] = 100\n",
    "\n",
    "FFF40 = pcrFFF40.iloc[:,:2]\n",
    "FFF40['FFF'] = 1\n",
    "FFF40.columns = ['r', 'señal', 'FFF']\n",
    "FFF40['Area'] = 1600\n",
    "FFF40['Limit'] = 200\n",
    "\n",
    "Open40 = pcropen40.iloc[:,:2]\n",
    "Open40['FFF'] = 0\n",
    "Open40.columns = ['r', 'señal', 'FFF']\n",
    "Open40['Area'] = 1600\n",
    "Open40['Limit'] = 200\n",
    "\n",
    "MixedCR = pd.concat([Open5, FFF5, Open10, FFF10, Open20, FFF20, Open40, FFF40]) \n",
    "MixedCR['In'] = 0\n",
    "MixedCR['Z'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Z = 85; In = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFF5 = pinFFF5_85.iloc[:,:2]\n",
    "FFF5['FFF'] = 1\n",
    "FFF5.columns = ['r', 'señal', 'FFF']\n",
    "FFF5['Area'] = 25\n",
    "FFF5['Limit'] = 25\n",
    "\n",
    "Open5 = pinopen5_85.iloc[:,:2]\n",
    "Open5['FFF'] = 0\n",
    "Open5.columns = ['r', 'señal', 'FFF']\n",
    "Open5['Area'] = 25\n",
    "Open5['Limit'] = 25\n",
    "\n",
    "FFF10 = pinFFF10_85.iloc[:,:2]\n",
    "FFF10['FFF'] = 1\n",
    "FFF10.columns = ['r', 'señal', 'FFF']\n",
    "FFF10['Area'] = 100\n",
    "FFF10['Limit'] = 50\n",
    "\n",
    "Open10 = pinopen10_85.iloc[:,:2]\n",
    "Open10['FFF'] = 0\n",
    "Open10.columns = ['r', 'señal', 'FFF']\n",
    "Open10['Area'] = 100\n",
    "Open10['Limit'] = 50\n",
    "\n",
    "FFF20 = pinFFF20_85.iloc[:,:2]\n",
    "FFF20['FFF'] = 1\n",
    "FFF20.columns = ['r', 'señal', 'FFF']\n",
    "FFF20['Area'] = 400\n",
    "FFF20['Limit'] = 100\n",
    "\n",
    "Open20 = pinopen20_85.iloc[:,:2]\n",
    "Open20['FFF'] = 0\n",
    "Open20.columns = ['r', 'señal', 'FFF']\n",
    "Open20['Area'] = 400\n",
    "Open20['Limit'] = 100\n",
    "\n",
    "FFF40 = pinFFF40_85.iloc[:,:2]\n",
    "FFF40['FFF'] = 1\n",
    "FFF40.columns = ['r', 'señal', 'FFF']\n",
    "FFF40['Area'] = 1600\n",
    "FFF40['Limit'] = 200\n",
    "\n",
    "Open40 = pinopen40_85.iloc[:,:2]\n",
    "Open40['FFF'] = 0\n",
    "Open40.columns = ['r', 'señal', 'FFF']\n",
    "Open40['Area'] = 1600\n",
    "Open40['Limit'] = 200\n",
    "\n",
    "MixedIN85 = pd.concat([Open5, FFF5, Open10, FFF10, Open20, FFF20, Open40, FFF40]) \n",
    "MixedIN85['In'] = 1\n",
    "MixedIN85['Z'] = 85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFF5 = pcrFFF5_85.iloc[:,:2]\n",
    "FFF5['FFF'] = 1\n",
    "FFF5.columns = ['r', 'señal', 'FFF']\n",
    "FFF5['Area'] = 25\n",
    "FFF5['Limit'] = 25\n",
    "\n",
    "Open5 = pcropen5_85.iloc[:,:2]\n",
    "Open5['FFF'] = 0\n",
    "Open5.columns = ['r', 'señal', 'FFF']\n",
    "Open5['Area'] = 25\n",
    "Open5['Limit'] = 25\n",
    "\n",
    "FFF10 = pcrFFF10_85.iloc[:,:2]\n",
    "FFF10['FFF'] = 1\n",
    "FFF10.columns = ['r', 'señal', 'FFF']\n",
    "FFF10['Area'] = 100\n",
    "FFF10['Limit'] = 50\n",
    "\n",
    "Open10 = pcropen10_85.iloc[:,:2]\n",
    "Open10['FFF'] = 0\n",
    "Open10.columns = ['r', 'señal', 'FFF']\n",
    "Open10['Area'] = 100\n",
    "Open10['Limit'] = 50\n",
    "\n",
    "FFF20 = pcrFFF20_85.iloc[:,:2]\n",
    "FFF20['FFF'] = 1\n",
    "FFF20.columns = ['r', 'señal', 'FFF']\n",
    "FFF20['Area'] = 400\n",
    "FFF20['Limit'] = 100\n",
    "\n",
    "Open20 = pcropen20_85.iloc[:,:2]\n",
    "Open20['FFF'] = 0\n",
    "Open20.columns = ['r', 'señal', 'FFF']\n",
    "Open20['Area'] = 400\n",
    "Open20['Limit'] = 100\n",
    "\n",
    "FFF40 = pcrFFF40_85.iloc[:,:2]\n",
    "FFF40['FFF'] = 1\n",
    "FFF40.columns = ['r', 'señal', 'FFF']\n",
    "FFF40['Area'] = 1600\n",
    "FFF40['Limit'] = 200\n",
    "\n",
    "Open40 = pcropen40_85.iloc[:,:2]\n",
    "Open40['FFF'] = 0\n",
    "Open40.columns = ['r', 'señal', 'FFF']\n",
    "Open40['Area'] = 1600\n",
    "Open40['Limit'] = 200\n",
    "\n",
    "MixedCR85 = pd.concat([Open5, FFF5, Open10, FFF10, Open20, FFF20, Open40, FFF40]) \n",
    "MixedCR85['In'] = 0\n",
    "MixedCR85['Z'] = 85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFF5 = pinFFF5_115.iloc[:,:2]\n",
    "FFF5['FFF'] = 1\n",
    "FFF5.columns = ['r', 'señal', 'FFF']\n",
    "FFF5['Area'] = 25\n",
    "FFF5['Limit'] = 25\n",
    "\n",
    "Open5 = pinopen5_115.iloc[:,:2]\n",
    "Open5['FFF'] = 0\n",
    "Open5.columns = ['r', 'señal', 'FFF']\n",
    "Open5['Area'] = 25\n",
    "Open5['Limit'] = 25\n",
    "\n",
    "FFF10 = pinFFF10_115.iloc[:,:2]\n",
    "FFF10['FFF'] = 1\n",
    "FFF10.columns = ['r', 'señal', 'FFF']\n",
    "FFF10['Area'] = 100\n",
    "FFF10['Limit'] = 50\n",
    "\n",
    "Open10 = pinopen10_115.iloc[:,:2]\n",
    "Open10['FFF'] = 0\n",
    "Open10.columns = ['r', 'señal', 'FFF']\n",
    "Open10['Area'] = 100\n",
    "Open10['Limit'] = 50\n",
    "\n",
    "FFF20 = pinFFF20_115.iloc[:,:2]\n",
    "FFF20['FFF'] = 1\n",
    "FFF20.columns = ['r', 'señal', 'FFF']\n",
    "FFF20['Area'] = 400\n",
    "FFF20['Limit'] = 100\n",
    "\n",
    "Open20 = pinopen20_115.iloc[:,:2]\n",
    "Open20['FFF'] = 0\n",
    "Open20.columns = ['r', 'señal', 'FFF']\n",
    "Open20['Area'] = 400\n",
    "Open20['Limit'] = 100\n",
    "\n",
    "FFF40 = pinFFF40_115.iloc[:,:2]\n",
    "FFF40['FFF'] = 1\n",
    "FFF40.columns = ['r', 'señal', 'FFF']\n",
    "FFF40['Area'] = 1600\n",
    "FFF40['Limit'] = 200\n",
    "\n",
    "Open40 = pinopen40_115.iloc[:,:2]\n",
    "Open40['FFF'] = 0\n",
    "Open40.columns = ['r', 'señal', 'FFF']\n",
    "Open40['Area'] = 1600\n",
    "Open40['Limit'] = 200\n",
    "\n",
    "MixedIN115 = pd.concat([Open5, FFF5, Open10, FFF10, Open20, FFF20, Open40, FFF40]) \n",
    "MixedIN115['In'] = 1\n",
    "MixedIN115['Z'] = 115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFF5 = pcrFFF5_115.iloc[:,:2]\n",
    "FFF5['FFF'] = 1\n",
    "FFF5.columns = ['r', 'señal', 'FFF']\n",
    "FFF5['Area'] = 25\n",
    "FFF5['Limit'] = 25\n",
    "\n",
    "Open5 = pcropen5_115.iloc[:,:2]\n",
    "Open5['FFF'] = 0\n",
    "Open5.columns = ['r', 'señal', 'FFF']\n",
    "Open5['Area'] = 25\n",
    "Open5['Limit'] = 25\n",
    "\n",
    "FFF10 = pcrFFF10_115.iloc[:,:2]\n",
    "FFF10['FFF'] = 1\n",
    "FFF10.columns = ['r', 'señal', 'FFF']\n",
    "FFF10['Area'] = 100\n",
    "FFF10['Limit'] = 50\n",
    "\n",
    "Open10 = pcropen10_115.iloc[:,:2]\n",
    "Open10['FFF'] = 0\n",
    "Open10.columns = ['r', 'señal', 'FFF']\n",
    "Open10['Area'] = 100\n",
    "Open10['Limit'] = 50\n",
    "\n",
    "FFF20 = pcrFFF20_115.iloc[:,:2]\n",
    "FFF20['FFF'] = 1\n",
    "FFF20.columns = ['r', 'señal', 'FFF']\n",
    "FFF20['Area'] = 400\n",
    "FFF20['Limit'] = 100\n",
    "\n",
    "Open20 = pcropen20_115.iloc[:,:2]\n",
    "Open20['FFF'] = 0\n",
    "Open20.columns = ['r', 'señal', 'FFF']\n",
    "Open20['Area'] = 400\n",
    "Open20['Limit'] = 100\n",
    "\n",
    "FFF40 = pcrFFF40_115.iloc[:,:2]\n",
    "FFF40['FFF'] = 1\n",
    "FFF40.columns = ['r', 'señal', 'FFF']\n",
    "FFF40['Area'] = 1600\n",
    "FFF40['Limit'] = 200\n",
    "\n",
    "Open40 = pcropen40_115.iloc[:,:2]\n",
    "Open40['FFF'] = 0\n",
    "Open40.columns = ['r', 'señal', 'FFF']\n",
    "Open40['Area'] = 1600\n",
    "Open40['Limit'] = 200\n",
    "\n",
    "MixedCR115 = pd.concat([Open5, FFF5, Open10, FFF10, Open20, FFF20, Open40, FFF40]) \n",
    "MixedCR115['In'] = 0\n",
    "MixedCR115['Z'] = 115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mixed = pd.concat([MixedCR, MixedIN, MixedCR85, MixedIN85, MixedCR115, MixedIN115]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Mixed['r'] = np.abs(Mixed.loc[:,['r']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mixed = Mixed.loc[:,['r', 'señal', 'FFF', 'Area', 'In', 'Z']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ecuaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fluencia de los fotones se define como \n",
    "\n",
    "\\\\[ \\phi_\\gamma(x,y,z) = w_0\\phi_0 (x,y,z) \\phi^\\gamma_{horn}(x,y,z) + (1-w_0)\\phi_s(x,y,z)\\\\]\n",
    "\n",
    "donde \\\\(\\phi_o \\\\) es la fluencia producida por la fuente primaria y \\\\( \\phi_s \\\\) la producida por la fuente secundaria (\"scatter source\"). \n",
    "\n",
    "\n",
    "Todas las fluencias son tal que:\n",
    "\n",
    "\\\\[ \\phi_\\alpha (x,y,z) = Z(z; z_D^x, z_D^y, z_\\alpha) T_\\alpha(x^+_\\alpha x^-_\\alpha)T_\\alpha(y^+_\\alpha, y^-_\\alpha)\\\\]\n",
    "\n",
    "Donde\n",
    "\n",
    "\\\\[ Z(z; z_D^x, z_D^y, z_\\alpha) = \\frac{1}{4} \\frac{ (z^x_D - z_\\alpha) (z^y_D - z_\\alpha)}{ (z-z_\\alpha)^2} \\\\]\n",
    "\n",
    "y \\\\(T_0\\\\) es la fuente primaria y \\\\(T_\\alpha\\\\)\n",
    "\n",
    "\\\\[ T_0(t_0^+, t_0^-) = Q_0(\\frac{t_0^+}{\\delta_0}) + Q_0(\\frac{t^-_0}{\\delta_0}) \\\\]\n",
    "\n",
    "donde \\\\(Q_0(v) = \\frac{v}{\\sqrt{1+v^2}}\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para los casos 'free' usamos únicamente las fluencias primarias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\\[ \\phi_\\gamma(x,y,z) = w_0\\phi_0 (x,y,z) \\phi^\\gamma_{horn}(x,y,z)\\\\]\n",
    "\n",
    "Por tanto \n",
    "\n",
    "\\\\[ \\phi_\\gamma(x,y,z) = w_0\\phi_0 (x,y,z) ( 1 + \\rho^2 \\sum^4_{j=0} h^{\\gamma}_j \\rho^j )\\\\]\n",
    "\n",
    "donde \\\\[ \\rho = \\frac{\\sqrt{x^2 + y^2}}{z-z_0} \\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\\[ \\Phi_{horn}=0 \\\\]\n",
    "Por lo que nos queda \\\\[\\phi_\\gamma (x, y, z) = \\omega_0 \\phi_0(x,y,z) \\\\]\n",
    "\n",
    "Estamos suponiendo \\\\(\\omega_0\\\\) como 0\n",
    "\n",
    "\\\\[ \\phi_\\gamma (x,y,z) = \\phi_0(x,y,z) = \\frac{1}{4} \\frac{(z^x_D-z_0)(z^y_D - z_0)}{(z-z_0)^2}*Q_0(\\frac{t^+_0}{\\delta_0} + \\frac{t_0^-}{\\delta_0})  \\\\]\n",
    "\n",
    "Conocemos: \\\\( z^x_D = 50.9 ; z^y_D=42.6 \\\\), y sabiendo que hay componente x e y.\n",
    "\n",
    "\\\\[ \\phi_\\gamma (x,y,z) = \\phi_0(x,y,z) = \\frac{542.085}{z^2}*Q_0(\\frac{x^+_0}{\\delta_0} + \\frac{x_0^-}{\\delta_0}) * Q_0(\\frac{y^+_0}{\\delta_0} + \\frac{y_0^-}{\\delta_0})  \\\\]\n",
    "\n",
    "\\\\( Q_0 (v) = \\frac{v}{\\sqrt{1+v^2}} \\\\) por lo que, siendo fotones, van a la velocidad de la luz, siendo \\\\(Q_0 \\approx 1\\\\) \n",
    "\n",
    "\\\\[ \\phi_\\gamma (x,y,z) = \\phi_0(x,y,z) = \\frac{542.085}{z^2}*(\\frac{t^+_0 + t_0^-}{\\delta_0})  \\\\]\n",
    "\n",
    "t significa x o y, dependiendo del caso.\n",
    "\n",
    "\\\\[t_0^{\\pm} = min[\\frac{w_I^tz_U^t(z-z_0) \\pm 2tz_I(z_0 - z_U^t)}{2z_I(z-z_U^t)} , \\frac{w_I^tz_D^t(z-z_0) \\pm 2 t z_I(z_0-z_D^t)}{2z_I(z-z^t_D)}   ] \\\\]\n",
    "\n",
    "Conocemos: \\\\(  z_U^x = 43.1; z_U^y = 29.8 ; z_0 = 0 ; z^x_D = 50.9 ; z^y_D=42.6 \\\\)\n",
    "\n",
    "Para x:\n",
    "\n",
    "\\\\[x_0^{\\pm} = min[\\frac{w_I^x * 43.1*(z) \\pm 2xz_I*43.1}{2z_I(z-43.1)} , \\frac{w_I^x *50.9 *z \\pm 2 x z_I(-50.9)}{2z_I(z-50.9)}  ] \\\\]\n",
    "\n",
    "Para y:\n",
    "\n",
    "\\\\[y_0^{\\pm} = min[\\frac{w_I^y * 29.8*(z) \\pm 2yz_I*29.8}{2z_I(z-29.8)} , \\frac{w_I^y *42.6* z \\pm 2 y z_I(-42.6)}{2z_I(z-42.6)}  ] \\\\]\n",
    "\n",
    "\n",
    "\\\\( w_I \\\\) indican el área inicialmente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MixedF0 = Mixed[Mixed['FFF']==0].reset_index(drop=True)\n",
    "MixedF00 = MixedF0[MixedF0['señal']>0.5]\n",
    "MixedF00.reset_index(drop=True, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining T plus and T minus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z_I es lo mismo que Z?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\\\\[t_0^{\\pm} = min[\\frac{w_I^tz_U^t(z-z_0) \\pm 2tz_I(z_0 - z_U^t)}{2z_I(z-z_U^t)} , \\frac{w_I^tz_D^t(z-z_0) \\pm 2 t z_I(z_0-z_D^t)}{2z_I(z-z^t_D)}   ] \\\\]\n",
    "\n",
    "Suponemos \\\\(Z_I = Z; z_0=0\\\\)\n",
    "\n",
    "\\\\[t_0^{\\pm} = min[\\frac{w_I^tz_U^t \\pm 2t(z_0 - z_U^t)}{2(z-z_U^t)} , \\frac{w_I^tz_D^t \\pm 2 t (z_0-z_D^t)}{2z_I(z-z^t_D)}   ] \\\\]\n",
    "\n",
    "Separando en x, y, +, - y cada ima de ñas dos ecuaciones\n",
    "\n",
    "\n",
    "\n",
    "\\\\[x_0^{+} (1) = \\frac{ w_I^x * (43.1) + 2x*(-43.1)}{2(z-43.1)}   \\\\]\n",
    "\n",
    "\\\\[x_0^{+} (2) = \\frac{w_I^x *(50.9) + 2 x (-50.9)}{2(z-50.9)}  \\\\]\n",
    "\n",
    "\n",
    "\\\\[y_0^{+} (1) = \\frac{ w_I^y * (29.8) + 2y*(-29.8)}{2(z-29.8)}   \\\\]\n",
    "\n",
    "\\\\[y_0^{+} (2) = \\frac{w_I^y * (42.6) + 2 y (-42.6)}{2(z-42.6)}  \\\\]\n",
    "\n",
    "\n",
    "\\\\[x_0^{-} (1) = \\frac{ w_I^x * (43.1) - 2x*(-43.1)}{2(z-43.1)}   \\\\]\n",
    "\n",
    "\\\\[x_0^{-} (2) = \\frac{w_I^x *(50.9) - 2 x (-50.9)}{2(z-50.9)}  \\\\]\n",
    "\n",
    "\n",
    "\\\\[y_0^{-} (1) = \\frac{ w_I^y * (29.8) - 2y*(-29.8)}{2(z-29.8)}   \\\\]\n",
    "\n",
    "\\\\[y_0^{-} (2) = \\frac{w_I^y * (42.6) - 2 y (-42.6)}{2(z-42.6)}  \\\\]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tplus_1 = []\n",
    "tplus_2 = []\n",
    "\n",
    "tplus_ = []\n",
    "\n",
    "\n",
    "\n",
    "tminus_1 = []\n",
    "tminus_2 = []\n",
    "\n",
    "tminus_ = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-25-b658f6e40212>:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  MixedF00['tplus'] = tplus\n",
      "<ipython-input-25-b658f6e40212>:79: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  MixedF00['tminus'] = tminus\n",
      "<ipython-input-25-b658f6e40212>:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  MixedF00['tplus_'] = tplus_\n",
      "<ipython-input-25-b658f6e40212>:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  MixedF00['tminus_'] = tminus_\n"
     ]
    }
   ],
   "source": [
    "tplus1 = []\n",
    "tplus2 = []\n",
    "\n",
    "tplus = []\n",
    "\n",
    "\n",
    "\n",
    "tminus1 = []\n",
    "tminus2 = []\n",
    "\n",
    "tminus = []\n",
    "\n",
    "\n",
    "tplus_1 = []\n",
    "tplus_2 = []\n",
    "\n",
    "tplus_ = []\n",
    "\n",
    "\n",
    "\n",
    "tminus_1 = []\n",
    "tminus_2 = []\n",
    "\n",
    "tminus_ = []\n",
    "\n",
    "\n",
    "for k in np.arange(0, MixedF00.shape[0]):\n",
    "    \n",
    "    if MixedF00['In'][k]==1:\n",
    "        tplus1.append(((MixedF00['Area'][k] * 43.1 + 2*np.abs(MixedF00['r'][k]) * -43.1)/(2*((MixedF00['Z'][k]) - 43.1))))\n",
    "        tplus2.append(((MixedF00['Area'][k] * 50.9 + 2*np.abs(MixedF00['r'][k]) * -50.9)/(2*((MixedF00['Z'][k]) - 50.9))))\n",
    "        tminus1.append(((MixedF00['Area'][k] * 43.1 - 2*np.abs(MixedF00['r'][k]) * -43.1)/(2*((MixedF00['Z'][k]) - 43.1))))\n",
    "        tminus2.append(((MixedF00['Area'][k] * 50.9 - 2*np.abs(MixedF00['r'][k]) * -50.9)/(2*((MixedF00['Z'][k]) - 50.9))))\n",
    "        \n",
    "        tplus_1.append(((MixedF00['Area'][k] * 43.1)/(2*((MixedF00['Z'][k]) - 43.1))))\n",
    "        tplus_2.append(((MixedF00['Area'][k] * 50.9)/(2*((MixedF00['Z'][k]) - 50.9))))\n",
    "        tminus_1.append(((MixedF00['Area'][k] * 43.1)/(2*((MixedF00['Z'][k]) - 43.1))))\n",
    "        tminus_2.append(((MixedF00['Area'][k] * 50.9)/(2*((MixedF00['Z'][k]) - 50.9))))\n",
    "    else :\n",
    "        tplus1.append(((MixedF00['Area'][k] * 29.8 + 2*np.abs(MixedF00['r'][k]) * -29.8)/(2*((MixedF00['Z'][k]) - 29.8))))\n",
    "        tplus2.append(((MixedF00['Area'][k] * 42.6 + 2*np.abs(MixedF00['r'][k]) * -42.6)/(2*((MixedF00['Z'][k]) - 42.6))))\n",
    "        tminus1.append(((MixedF00['Area'][k] * 29.8 - 2*np.abs(MixedF00['r'][k]) * -29.8)/(2*((MixedF00['Z'][k]) - 29.8))))\n",
    "        tminus2.append(((MixedF00['Area'][k] * 42.6 - 2*np.abs(MixedF00['r'][k]) * -42.6)/(2*((MixedF00['Z'][k]) - 42.6))))\n",
    "\n",
    "        tplus_1.append(((MixedF00['Area'][k] * 43.1)/(2*((MixedF00['Z'][k]) - 43.1))))\n",
    "        tplus_2.append(((MixedF00['Area'][k] * 50.9)/(2*((MixedF00['Z'][k]) - 50.9))))\n",
    "        tminus_1.append(((MixedF00['Area'][k] * 43.1)/(2*((MixedF00['Z'][k]) - 43.1))))\n",
    "        tminus_2.append(((MixedF00['Area'][k] * 50.9)/(2*((MixedF00['Z'][k]) - 50.9))))\n",
    "\n",
    "        \n",
    "                \n",
    "                \n",
    "for i, j in zip(tplus1, tplus2):\n",
    "    if i <= j :\n",
    "        tplus.append(i)\n",
    "    else:\n",
    "        tplus.append(j)\n",
    "        \n",
    "for i, j in zip(tminus1, tminus2):\n",
    "    if i <= j :\n",
    "        tminus.append(i)\n",
    "    else:\n",
    "        tminus.append(j)\n",
    "        \n",
    "for i, j in zip(tplus_1, tplus_2):\n",
    "    if i <= j :\n",
    "        tplus_.append(i)\n",
    "    else:\n",
    "        tplus_.append(j)\n",
    "        \n",
    "for i, j in zip(tminus_1, tminus_2):\n",
    "    if i <= j :\n",
    "        tminus_.append(i)\n",
    "    else:\n",
    "        tminus_.append(j)\n",
    "\n",
    "        \n",
    "MixedF00['tplus'] = tplus \n",
    "MixedF00['tminus'] = tminus \n",
    "MixedF00['tplus_'] = tplus_\n",
    "MixedF00['tminus_'] = tminus_  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining Phi0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\\[ \\phi_\\gamma (x,y,z) = \\phi_0(x,y,z) = \\frac{542.085}{z^2}*(\\frac{x^+_0 + x_0^-}{\\delta_0})*(\\frac{y^+_0 + y_0^-}{\\delta_0})  \\\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-8b12198e1642>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  MixedF00['phi0 minus delta'] = ((542.085/(MixedF00['Z'].to_numpy())**2))*(MixedF00['tplus'].to_numpy()+MixedF00['tminus'].to_numpy())*(MixedF00['tplus_'].to_numpy()+MixedF00['tminus_'].to_numpy())\n"
     ]
    }
   ],
   "source": [
    "MixedF00['phi0 minus delta'] = ((542.085/(MixedF00['Z'].to_numpy())**2))*(MixedF00['tplus'].to_numpy()+MixedF00['tminus'].to_numpy())*(MixedF00['tplus_'].to_numpy()+MixedF00['tminus_'].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-38677b307b4c>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  MixedF00['Rho'] = np.ravel(Rho)\n"
     ]
    }
   ],
   "source": [
    "Rho = []\n",
    "Rho.append(MixedF00['r']/MixedF00['Z'])\n",
    "\n",
    "MixedF00['Rho'] = np.ravel(Rho)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate through possible values of phi_0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1441\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1520\n",
      "1521\n",
      "1522\n",
      "1523\n",
      "1524\n",
      "1525\n",
      "1526\n",
      "1527\n",
      "1528\n",
      "1529\n",
      "1530\n",
      "1531\n",
      "1532\n",
      "1533\n",
      "1534\n",
      "1535\n",
      "1536\n",
      "1537\n",
      "1538\n",
      "1539\n",
      "1540\n",
      "1541\n",
      "1542\n",
      "1543\n",
      "1544\n",
      "1545\n",
      "1546\n",
      "1547\n",
      "1548\n",
      "1549\n",
      "1550\n",
      "1551\n",
      "1552\n",
      "1553\n",
      "1554\n",
      "1555\n",
      "1556\n",
      "1557\n",
      "1558\n",
      "1559\n",
      "1560\n",
      "1561\n",
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1566\n",
      "1567\n",
      "1568\n",
      "1569\n",
      "1570\n",
      "1571\n",
      "1572\n",
      "1573\n",
      "1574\n",
      "1575\n",
      "1576\n",
      "1577\n",
      "1578\n",
      "1579\n",
      "1580\n",
      "1581\n",
      "1582\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1586\n",
      "1587\n",
      "1588\n",
      "1589\n",
      "1590\n",
      "1591\n",
      "1592\n",
      "1593\n",
      "1594\n",
      "1595\n",
      "1596\n",
      "1597\n",
      "1598\n",
      "1599\n",
      "1600\n",
      "1601\n",
      "1602\n",
      "1603\n",
      "1604\n",
      "1605\n",
      "1606\n",
      "1607\n",
      "1608\n",
      "1609\n",
      "1610\n",
      "1611\n",
      "1612\n",
      "1613\n",
      "1614\n",
      "1615\n",
      "1616\n",
      "1617\n",
      "1618\n",
      "1619\n",
      "1620\n",
      "1621\n",
      "1622\n",
      "1623\n",
      "1624\n",
      "1625\n",
      "1626\n",
      "1627\n",
      "1628\n",
      "1629\n",
      "1630\n",
      "1631\n",
      "1632\n",
      "1633\n",
      "1634\n",
      "1635\n",
      "1636\n",
      "1637\n",
      "1638\n",
      "1639\n",
      "1640\n",
      "1641\n",
      "1642\n",
      "1643\n",
      "1644\n",
      "1645\n",
      "1646\n",
      "1647\n",
      "1648\n",
      "1649\n",
      "1650\n",
      "1651\n",
      "1652\n",
      "1653\n",
      "1654\n",
      "1655\n",
      "1656\n",
      "1657\n",
      "1658\n",
      "1659\n",
      "1660\n",
      "1661\n",
      "1662\n",
      "1663\n",
      "1664\n",
      "1665\n",
      "1666\n",
      "1667\n",
      "1668\n",
      "1669\n",
      "1670\n",
      "1671\n",
      "1672\n",
      "1673\n",
      "1674\n",
      "1675\n",
      "1676\n",
      "1677\n",
      "1678\n",
      "1679\n",
      "1680\n",
      "1681\n",
      "1682\n",
      "1683\n",
      "1684\n",
      "1685\n",
      "1686\n",
      "1687\n",
      "1688\n",
      "1689\n",
      "1690\n",
      "1691\n",
      "1692\n",
      "1693\n",
      "1694\n",
      "1695\n",
      "1696\n",
      "1697\n",
      "1698\n",
      "1699\n",
      "1700\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1704\n",
      "1705\n",
      "1706\n",
      "1707\n",
      "1708\n",
      "1709\n",
      "1710\n",
      "1711\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n",
      "1719\n",
      "1720\n",
      "1721\n",
      "1722\n",
      "1723\n",
      "1724\n",
      "1725\n",
      "1726\n",
      "1727\n",
      "1728\n",
      "1729\n",
      "1730\n",
      "1731\n",
      "1732\n",
      "1733\n",
      "1734\n",
      "1735\n",
      "1736\n",
      "1737\n",
      "1738\n",
      "1739\n",
      "1740\n",
      "1741\n",
      "1742\n",
      "1743\n",
      "1744\n",
      "1745\n",
      "1746\n",
      "1747\n",
      "1748\n",
      "1749\n",
      "1750\n",
      "1751\n",
      "1752\n",
      "1753\n",
      "1754\n",
      "1755\n",
      "1756\n",
      "1757\n",
      "1758\n",
      "1759\n",
      "1760\n",
      "1761\n",
      "1762\n",
      "1763\n",
      "1764\n",
      "1765\n",
      "1766\n",
      "1767\n",
      "1768\n",
      "1769\n",
      "1770\n",
      "1771\n",
      "1772\n",
      "1773\n",
      "1774\n",
      "1775\n",
      "1776\n",
      "1777\n",
      "1778\n",
      "1779\n",
      "1780\n",
      "1781\n",
      "1782\n",
      "1783\n",
      "1784\n",
      "1785\n",
      "1786\n",
      "1787\n",
      "1788\n",
      "1789\n",
      "1790\n",
      "1791\n",
      "1792\n",
      "1793\n",
      "1794\n",
      "1795\n",
      "1796\n",
      "1797\n",
      "1798\n",
      "1799\n",
      "1800\n",
      "1801\n",
      "1802\n",
      "1803\n",
      "1804\n",
      "1805\n",
      "1806\n",
      "1807\n",
      "1808\n",
      "1809\n",
      "1810\n",
      "1811\n",
      "1812\n",
      "1813\n",
      "1814\n",
      "1815\n",
      "1816\n",
      "1817\n",
      "1818\n",
      "1819\n",
      "1820\n",
      "1821\n",
      "1822\n",
      "1823\n",
      "1824\n",
      "1825\n",
      "1826\n",
      "1827\n",
      "1828\n",
      "1829\n",
      "1830\n",
      "1831\n",
      "1832\n",
      "1833\n",
      "1834\n",
      "1835\n",
      "1836\n",
      "1837\n",
      "1838\n",
      "1839\n",
      "1840\n",
      "1841\n",
      "1842\n",
      "1843\n",
      "1844\n",
      "1845\n",
      "1846\n",
      "1847\n",
      "1848\n",
      "1849\n",
      "1850\n",
      "1851\n",
      "1852\n",
      "1853\n",
      "1854\n",
      "1855\n",
      "1856\n",
      "1857\n",
      "1858\n",
      "1859\n",
      "1860\n",
      "1861\n",
      "1862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1863\n",
      "1864\n",
      "1865\n",
      "1866\n",
      "1867\n",
      "1868\n",
      "1869\n",
      "1870\n",
      "1871\n",
      "1872\n",
      "1873\n",
      "1874\n",
      "1875\n",
      "1876\n",
      "1877\n",
      "1878\n",
      "1879\n",
      "1880\n",
      "1881\n",
      "1882\n",
      "1883\n",
      "1884\n",
      "1885\n",
      "1886\n",
      "1887\n",
      "1888\n",
      "1889\n",
      "1890\n",
      "1891\n",
      "1892\n",
      "1893\n",
      "1894\n",
      "1895\n",
      "1896\n",
      "1897\n",
      "1898\n",
      "1899\n",
      "1900\n",
      "1901\n",
      "1902\n",
      "1903\n",
      "1904\n",
      "1905\n",
      "1906\n",
      "1907\n",
      "1908\n",
      "1909\n",
      "1910\n",
      "1911\n",
      "1912\n",
      "1913\n",
      "1914\n",
      "1915\n",
      "1916\n",
      "1917\n",
      "1918\n",
      "1919\n",
      "1920\n",
      "1921\n",
      "1922\n",
      "1923\n",
      "1924\n",
      "1925\n",
      "1926\n",
      "1927\n",
      "1928\n",
      "1929\n",
      "1930\n",
      "1931\n",
      "1932\n",
      "1933\n",
      "1934\n",
      "1935\n",
      "1936\n",
      "1937\n",
      "1938\n",
      "1939\n",
      "1940\n",
      "1941\n",
      "1942\n",
      "1943\n",
      "1944\n",
      "1945\n",
      "1946\n",
      "1947\n",
      "1948\n",
      "1949\n",
      "1950\n",
      "1951\n",
      "1952\n",
      "1953\n",
      "1954\n",
      "1955\n",
      "1956\n",
      "1957\n",
      "1958\n",
      "1959\n",
      "1960\n",
      "1961\n",
      "1962\n",
      "1963\n",
      "1964\n",
      "1965\n",
      "1966\n",
      "1967\n",
      "1968\n",
      "1969\n",
      "1970\n",
      "1971\n",
      "1972\n",
      "1973\n",
      "1974\n",
      "1975\n",
      "1976\n",
      "1977\n",
      "1978\n",
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n",
      "2024\n",
      "2025\n",
      "2026\n",
      "2027\n",
      "2028\n",
      "2029\n",
      "2030\n",
      "2031\n",
      "2032\n",
      "2033\n",
      "2034\n",
      "2035\n",
      "2036\n",
      "2037\n",
      "2038\n",
      "2039\n",
      "2040\n",
      "2041\n",
      "2042\n",
      "2043\n",
      "2044\n",
      "2045\n",
      "2046\n",
      "2047\n",
      "2048\n",
      "2049\n",
      "2050\n",
      "2051\n",
      "2052\n",
      "2053\n",
      "2054\n",
      "2055\n",
      "2056\n",
      "2057\n",
      "2058\n",
      "2059\n",
      "2060\n",
      "2061\n",
      "2062\n",
      "2063\n",
      "2064\n",
      "2065\n",
      "2066\n",
      "2067\n",
      "2068\n",
      "2069\n",
      "2070\n",
      "2071\n",
      "2072\n",
      "2073\n",
      "2074\n",
      "2075\n",
      "2076\n",
      "2077\n",
      "2078\n",
      "2079\n",
      "2080\n",
      "2081\n",
      "2082\n",
      "2083\n",
      "2084\n",
      "2085\n",
      "2086\n",
      "2087\n",
      "2088\n",
      "2089\n",
      "2090\n",
      "2091\n",
      "2092\n",
      "2093\n",
      "2094\n",
      "2095\n",
      "2096\n",
      "2097\n",
      "2098\n",
      "2099\n",
      "2100\n",
      "2101\n",
      "2102\n",
      "2103\n",
      "2104\n",
      "2105\n",
      "2106\n",
      "2107\n",
      "2108\n",
      "2109\n",
      "2110\n",
      "2111\n",
      "2112\n",
      "2113\n",
      "2114\n",
      "2115\n",
      "2116\n",
      "2117\n",
      "2118\n",
      "2119\n",
      "2120\n",
      "2121\n",
      "2122\n",
      "2123\n",
      "2124\n",
      "2125\n",
      "2126\n",
      "2127\n",
      "2128\n",
      "2129\n",
      "2130\n",
      "2131\n",
      "2132\n",
      "2133\n",
      "2134\n",
      "2135\n",
      "2136\n",
      "2137\n",
      "2138\n",
      "2139\n",
      "2140\n",
      "2141\n",
      "2142\n",
      "2143\n",
      "2144\n",
      "2145\n",
      "2146\n",
      "2147\n",
      "2148\n",
      "2149\n",
      "2150\n",
      "2151\n",
      "2152\n",
      "2153\n",
      "2154\n",
      "2155\n",
      "2156\n",
      "2157\n",
      "2158\n",
      "2159\n",
      "2160\n",
      "2161\n",
      "2162\n",
      "2163\n",
      "2164\n",
      "2165\n",
      "2166\n",
      "2167\n",
      "2168\n",
      "2169\n",
      "2170\n",
      "2171\n",
      "2172\n",
      "2173\n",
      "2174\n",
      "2175\n",
      "2176\n",
      "2177\n",
      "2178\n",
      "2179\n",
      "2180\n",
      "2181\n",
      "2182\n",
      "2183\n",
      "2184\n",
      "2185\n",
      "2186\n",
      "2187\n",
      "2188\n",
      "2189\n",
      "2190\n",
      "2191\n",
      "2192\n",
      "2193\n",
      "2194\n",
      "2195\n",
      "2196\n",
      "2197\n",
      "2198\n",
      "2199\n",
      "2200\n",
      "2201\n",
      "2202\n",
      "2203\n",
      "2204\n",
      "2205\n",
      "2206\n",
      "2207\n",
      "2208\n",
      "2209\n",
      "2210\n",
      "2211\n",
      "2212\n",
      "2213\n",
      "2214\n",
      "2215\n",
      "2216\n",
      "2217\n",
      "2218\n",
      "2219\n",
      "2220\n",
      "2221\n",
      "2222\n",
      "2223\n",
      "2224\n",
      "2225\n",
      "2226\n",
      "2227\n",
      "2228\n",
      "2229\n",
      "2230\n",
      "2231\n",
      "2232\n",
      "2233\n",
      "2234\n",
      "2235\n",
      "2236\n",
      "2237\n",
      "2238\n",
      "2239\n",
      "2240\n",
      "2241\n",
      "2242\n",
      "2243\n",
      "2244\n",
      "2245\n",
      "2246\n",
      "2247\n",
      "2248\n",
      "2249\n",
      "2250\n",
      "2251\n",
      "2252\n",
      "2253\n",
      "2254\n",
      "2255\n",
      "2256\n",
      "2257\n",
      "2258\n",
      "2259\n",
      "2260\n",
      "2261\n",
      "2262\n",
      "2263\n",
      "2264\n",
      "2265\n",
      "2266\n",
      "2267\n",
      "2268\n",
      "2269\n",
      "2270\n",
      "2271\n",
      "2272\n",
      "2273\n",
      "2274\n",
      "2275\n",
      "2276\n",
      "2277\n",
      "2278\n",
      "2279\n",
      "2280\n",
      "2281\n",
      "2282\n",
      "2283\n",
      "2284\n",
      "2285\n",
      "2286\n",
      "2287\n",
      "2288\n",
      "2289\n",
      "2290\n",
      "2291\n",
      "2292\n",
      "2293\n",
      "2294\n",
      "2295\n",
      "2296\n",
      "2297\n",
      "2298\n",
      "2299\n",
      "2300\n",
      "2301\n",
      "2302\n",
      "2303\n",
      "2304\n",
      "2305\n",
      "2306\n",
      "2307\n",
      "2308\n",
      "2309\n",
      "2310\n",
      "2311\n",
      "2312\n",
      "2313\n",
      "2314\n",
      "2315\n",
      "2316\n",
      "2317\n",
      "2318\n",
      "2319\n",
      "2320\n",
      "2321\n",
      "2322\n",
      "2323\n",
      "2324\n",
      "2325\n",
      "2326\n",
      "2327\n",
      "2328\n",
      "2329\n",
      "2330\n",
      "2331\n",
      "2332\n",
      "2333\n",
      "2334\n",
      "2335\n",
      "2336\n",
      "2337\n",
      "2338\n",
      "2339\n",
      "2340\n",
      "2341\n",
      "2342\n",
      "2343\n",
      "2344\n",
      "2345\n",
      "2346\n",
      "2347\n",
      "2348\n",
      "2349\n",
      "2350\n",
      "2351\n",
      "2352\n",
      "2353\n",
      "2354\n",
      "2355\n",
      "2356\n",
      "2357\n",
      "2358\n",
      "2359\n",
      "2360\n",
      "2361\n",
      "2362\n",
      "2363\n",
      "2364\n",
      "2365\n",
      "2366\n",
      "2367\n",
      "2368\n",
      "2369\n",
      "2370\n",
      "2371\n",
      "2372\n",
      "2373\n",
      "2374\n",
      "2375\n",
      "2376\n",
      "2377\n",
      "2378\n",
      "2379\n",
      "2380\n",
      "2381\n",
      "2382\n",
      "2383\n",
      "2384\n",
      "2385\n",
      "2386\n",
      "2387\n",
      "2388\n",
      "2389\n",
      "2390\n",
      "2391\n",
      "2392\n",
      "2393\n",
      "2394\n",
      "2395\n",
      "2396\n",
      "2397\n",
      "2398\n",
      "2399\n",
      "2400\n",
      "2401\n",
      "2402\n",
      "2403\n",
      "2404\n",
      "2405\n",
      "2406\n",
      "2407\n",
      "2408\n",
      "2409\n",
      "2410\n",
      "2411\n",
      "2412\n",
      "2413\n",
      "2414\n",
      "2415\n",
      "2416\n",
      "2417\n",
      "2418\n",
      "2419\n",
      "2420\n",
      "2421\n",
      "2422\n",
      "2423\n",
      "2424\n",
      "2425\n",
      "2426\n",
      "2427\n",
      "2428\n",
      "2429\n",
      "2430\n",
      "2431\n",
      "2432\n",
      "2433\n",
      "2434\n",
      "2435\n",
      "2436\n",
      "2437\n",
      "2438\n",
      "2439\n",
      "2440\n",
      "2441\n",
      "2442\n",
      "2443\n",
      "2444\n",
      "2445\n",
      "2446\n",
      "2447\n",
      "2448\n",
      "2449\n",
      "2450\n",
      "2451\n",
      "2452\n",
      "2453\n",
      "2454\n",
      "2455\n",
      "2456\n",
      "2457\n",
      "2458\n",
      "2459\n",
      "2460\n",
      "2461\n",
      "2462\n",
      "2463\n",
      "2464\n",
      "2465\n",
      "2466\n",
      "2467\n",
      "2468\n",
      "2469\n",
      "2470\n",
      "2471\n",
      "2472\n",
      "2473\n",
      "2474\n",
      "2475\n",
      "2476\n",
      "2477\n",
      "2478\n",
      "2479\n",
      "2480\n",
      "2481\n",
      "2482\n",
      "2483\n",
      "2484\n",
      "2485\n",
      "2486\n",
      "2487\n",
      "2488\n",
      "2489\n",
      "2490\n",
      "2491\n",
      "2492\n",
      "2493\n",
      "2494\n",
      "2495\n",
      "2496\n",
      "2497\n",
      "2498\n",
      "2499\n",
      "2500\n",
      "2501\n",
      "2502\n",
      "2503\n",
      "2504\n",
      "2505\n",
      "2506\n",
      "2507\n",
      "2508\n",
      "2509\n",
      "2510\n",
      "2511\n",
      "2512\n",
      "2513\n",
      "2514\n",
      "2515\n",
      "2516\n",
      "2517\n",
      "2518\n",
      "2519\n",
      "2520\n",
      "2521\n",
      "2522\n",
      "2523\n",
      "2524\n",
      "2525\n",
      "2526\n",
      "2527\n",
      "2528\n",
      "2529\n",
      "2530\n",
      "2531\n",
      "2532\n",
      "2533\n",
      "2534\n",
      "2535\n",
      "2536\n",
      "2537\n",
      "2538\n",
      "2539\n",
      "2540\n",
      "2541\n",
      "2542\n",
      "2543\n",
      "2544\n",
      "2545\n",
      "2546\n",
      "2547\n",
      "2548\n",
      "2549\n",
      "2550\n",
      "2551\n",
      "2552\n",
      "2553\n",
      "2554\n",
      "2555\n",
      "2556\n",
      "2557\n",
      "2558\n",
      "2559\n",
      "2560\n",
      "2561\n",
      "2562\n",
      "2563\n",
      "2564\n",
      "2565\n",
      "2566\n",
      "2567\n",
      "2568\n",
      "2569\n",
      "2570\n",
      "2571\n",
      "2572\n",
      "2573\n",
      "2574\n",
      "2575\n",
      "2576\n",
      "2577\n",
      "2578\n",
      "2579\n",
      "2580\n",
      "2581\n",
      "2582\n",
      "2583\n",
      "2584\n",
      "2585\n",
      "2586\n",
      "2587\n",
      "2588\n",
      "2589\n",
      "2590\n",
      "2591\n",
      "2592\n",
      "2593\n",
      "2594\n",
      "2595\n",
      "2596\n",
      "2597\n",
      "2598\n",
      "2599\n",
      "2600\n",
      "2601\n",
      "2602\n",
      "2603\n",
      "2604\n",
      "2605\n",
      "2606\n",
      "2607\n",
      "2608\n",
      "2609\n",
      "2610\n",
      "2611\n",
      "2612\n",
      "2613\n",
      "2614\n",
      "2615\n",
      "2616\n",
      "2617\n",
      "2618\n",
      "2619\n",
      "2620\n",
      "2621\n",
      "2622\n",
      "2623\n",
      "2624\n",
      "2625\n",
      "2626\n",
      "2627\n",
      "2628\n",
      "2629\n",
      "2630\n",
      "2631\n",
      "2632\n",
      "2633\n",
      "2634\n",
      "2635\n",
      "2636\n",
      "2637\n",
      "2638\n",
      "2639\n",
      "2640\n",
      "2641\n",
      "2642\n",
      "2643\n",
      "2644\n",
      "2645\n",
      "2646\n",
      "2647\n",
      "2648\n",
      "2649\n",
      "2650\n",
      "2651\n",
      "2652\n",
      "2653\n",
      "2654\n",
      "2655\n",
      "2656\n",
      "2657\n",
      "2658\n",
      "2659\n",
      "2660\n",
      "2661\n",
      "2662\n",
      "2663\n",
      "2664\n",
      "2665\n",
      "2666\n",
      "2667\n",
      "2668\n",
      "2669\n",
      "2670\n",
      "2671\n",
      "2672\n",
      "2673\n",
      "2674\n",
      "2675\n",
      "2676\n",
      "2677\n",
      "2678\n",
      "2679\n",
      "2680\n",
      "2681\n",
      "2682\n",
      "2683\n",
      "2684\n",
      "2685\n",
      "2686\n",
      "2687\n",
      "2688\n",
      "2689\n",
      "2690\n",
      "2691\n",
      "2692\n",
      "2693\n",
      "2694\n",
      "2695\n",
      "2696\n",
      "2697\n",
      "2698\n",
      "2699\n",
      "2700\n",
      "2701\n",
      "2702\n",
      "2703\n",
      "2704\n",
      "2705\n",
      "2706\n",
      "2707\n",
      "2708\n",
      "2709\n",
      "2710\n",
      "2711\n",
      "2712\n",
      "2713\n",
      "2714\n",
      "2715\n",
      "2716\n",
      "2717\n",
      "2718\n",
      "2719\n",
      "2720\n",
      "2721\n",
      "2722\n",
      "2723\n",
      "2724\n",
      "2725\n",
      "2726\n",
      "2727\n",
      "2728\n",
      "2729\n",
      "2730\n",
      "2731\n",
      "2732\n",
      "2733\n",
      "2734\n",
      "2735\n",
      "2736\n",
      "2737\n",
      "2738\n",
      "2739\n",
      "2740\n",
      "2741\n",
      "2742\n",
      "2743\n",
      "2744\n",
      "2745\n",
      "2746\n",
      "2747\n",
      "2748\n",
      "2749\n",
      "2750\n",
      "2751\n",
      "2752\n",
      "2753\n",
      "2754\n",
      "2755\n",
      "2756\n",
      "2757\n",
      "2758\n",
      "2759\n",
      "2760\n",
      "2761\n",
      "2762\n",
      "2763\n",
      "2764\n",
      "2765\n",
      "2766\n",
      "2767\n",
      "2768\n",
      "2769\n",
      "2770\n",
      "2771\n",
      "2772\n",
      "2773\n",
      "2774\n",
      "2775\n",
      "2776\n",
      "2777\n",
      "2778\n",
      "2779\n",
      "2780\n",
      "2781\n",
      "2782\n",
      "2783\n",
      "2784\n",
      "2785\n",
      "2786\n",
      "2787\n",
      "2788\n",
      "2789\n",
      "2790\n",
      "2791\n",
      "2792\n",
      "2793\n",
      "2794\n",
      "2795\n",
      "2796\n",
      "2797\n",
      "2798\n",
      "2799\n",
      "2800\n",
      "2801\n",
      "2802\n",
      "2803\n",
      "2804\n",
      "2805\n",
      "2806\n",
      "2807\n",
      "2808\n",
      "2809\n",
      "2810\n",
      "2811\n",
      "2812\n",
      "2813\n",
      "2814\n",
      "2815\n",
      "2816\n",
      "2817\n",
      "2818\n",
      "2819\n",
      "2820\n",
      "2821\n",
      "2822\n",
      "2823\n",
      "2824\n",
      "2825\n",
      "2826\n",
      "2827\n",
      "2828\n",
      "2829\n",
      "2830\n",
      "2831\n",
      "2832\n",
      "2833\n",
      "2834\n",
      "2835\n",
      "2836\n",
      "2837\n",
      "2838\n",
      "2839\n",
      "2840\n",
      "2841\n",
      "2842\n",
      "2843\n",
      "2844\n",
      "2845\n",
      "2846\n",
      "2847\n",
      "2848\n",
      "2849\n",
      "2850\n",
      "2851\n",
      "2852\n",
      "2853\n",
      "2854\n",
      "2855\n",
      "2856\n",
      "2857\n",
      "2858\n",
      "2859\n",
      "2860\n",
      "2861\n",
      "2862\n",
      "2863\n",
      "2864\n",
      "2865\n",
      "2866\n",
      "2867\n",
      "2868\n",
      "2869\n",
      "2870\n",
      "2871\n",
      "2872\n",
      "2873\n",
      "2874\n",
      "2875\n",
      "2876\n",
      "2877\n",
      "2878\n",
      "2879\n",
      "2880\n",
      "2881\n",
      "2882\n",
      "2883\n",
      "2884\n",
      "2885\n",
      "2886\n",
      "2887\n",
      "2888\n",
      "2889\n",
      "2890\n",
      "2891\n",
      "2892\n",
      "2893\n",
      "2894\n",
      "2895\n",
      "2896\n",
      "2897\n",
      "2898\n",
      "2899\n",
      "2900\n",
      "2901\n",
      "2902\n",
      "2903\n",
      "2904\n",
      "2905\n",
      "2906\n",
      "2907\n",
      "2908\n",
      "2909\n",
      "2910\n",
      "2911\n",
      "2912\n",
      "2913\n",
      "2914\n",
      "2915\n",
      "2916\n",
      "2917\n",
      "2918\n",
      "2919\n",
      "2920\n",
      "2921\n",
      "2922\n",
      "2923\n",
      "2924\n",
      "2925\n",
      "2926\n",
      "2927\n",
      "2928\n",
      "2929\n",
      "2930\n",
      "2931\n",
      "2932\n",
      "2933\n",
      "2934\n",
      "2935\n",
      "2936\n",
      "2937\n",
      "2938\n",
      "2939\n",
      "2940\n",
      "2941\n",
      "2942\n",
      "2943\n",
      "2944\n",
      "2945\n",
      "2946\n",
      "2947\n",
      "2948\n",
      "2949\n",
      "2950\n",
      "2951\n",
      "2952\n",
      "2953\n",
      "2954\n",
      "2955\n",
      "2956\n",
      "2957\n",
      "2958\n",
      "2959\n",
      "2960\n",
      "2961\n",
      "2962\n",
      "2963\n",
      "2964\n",
      "2965\n",
      "2966\n",
      "2967\n",
      "2968\n",
      "2969\n",
      "2970\n",
      "2971\n",
      "2972\n",
      "2973\n",
      "2974\n",
      "2975\n",
      "2976\n",
      "2977\n",
      "2978\n",
      "2979\n",
      "2980\n",
      "2981\n",
      "2982\n",
      "2983\n",
      "2984\n",
      "2985\n",
      "2986\n",
      "2987\n",
      "2988\n",
      "2989\n",
      "2990\n",
      "2991\n",
      "2992\n",
      "2993\n",
      "2994\n",
      "2995\n",
      "2996\n",
      "2997\n",
      "2998\n",
      "2999\n",
      "3000\n",
      "3001\n",
      "3002\n",
      "3003\n",
      "3004\n",
      "3005\n",
      "3006\n",
      "3007\n",
      "3008\n",
      "3009\n",
      "3010\n",
      "3011\n",
      "3012\n",
      "3013\n",
      "3014\n",
      "3015\n",
      "3016\n",
      "3017\n",
      "3018\n",
      "3019\n",
      "3020\n",
      "3021\n",
      "3022\n",
      "3023\n",
      "3024\n",
      "3025\n",
      "3026\n",
      "3027\n",
      "3028\n",
      "3029\n",
      "3030\n",
      "3031\n",
      "3032\n",
      "3033\n",
      "3034\n",
      "3035\n",
      "3036\n",
      "3037\n",
      "3038\n",
      "3039\n",
      "3040\n",
      "3041\n",
      "3042\n",
      "3043\n",
      "3044\n",
      "3045\n",
      "3046\n",
      "3047\n",
      "3048\n",
      "3049\n",
      "3050\n",
      "3051\n",
      "3052\n",
      "3053\n",
      "3054\n",
      "3055\n",
      "3056\n",
      "3057\n",
      "3058\n",
      "3059\n",
      "3060\n",
      "3061\n",
      "3062\n",
      "3063\n",
      "3064\n",
      "3065\n",
      "3066\n",
      "3067\n",
      "3068\n",
      "3069\n",
      "3070\n",
      "3071\n",
      "3072\n",
      "3073\n",
      "3074\n",
      "3075\n",
      "3076\n",
      "3077\n",
      "3078\n",
      "3079\n",
      "3080\n",
      "3081\n",
      "3082\n",
      "3083\n",
      "3084\n",
      "3085\n",
      "3086\n",
      "3087\n",
      "3088\n",
      "3089\n",
      "3090\n",
      "3091\n",
      "3092\n",
      "3093\n",
      "3094\n",
      "3095\n",
      "3096\n",
      "3097\n",
      "3098\n",
      "3099\n",
      "3100\n",
      "3101\n",
      "3102\n",
      "3103\n",
      "3104\n",
      "3105\n",
      "3106\n",
      "3107\n",
      "3108\n",
      "3109\n",
      "3110\n",
      "3111\n",
      "3112\n",
      "3113\n",
      "3114\n",
      "3115\n",
      "3116\n",
      "3117\n",
      "3118\n",
      "3119\n",
      "3120\n",
      "3121\n",
      "3122\n",
      "3123\n",
      "3124\n",
      "3125\n",
      "3126\n",
      "3127\n",
      "3128\n",
      "3129\n",
      "3130\n",
      "3131\n",
      "3132\n",
      "3133\n",
      "3134\n",
      "3135\n",
      "3136\n",
      "3137\n",
      "3138\n",
      "3139\n",
      "3140\n",
      "3141\n",
      "3142\n",
      "3143\n",
      "3144\n",
      "3145\n",
      "3146\n",
      "3147\n",
      "3148\n",
      "3149\n",
      "3150\n",
      "3151\n",
      "3152\n",
      "3153\n",
      "3154\n",
      "3155\n",
      "3156\n",
      "3157\n",
      "3158\n",
      "3159\n",
      "3160\n",
      "3161\n",
      "3162\n",
      "3163\n",
      "3164\n",
      "3165\n",
      "3166\n",
      "3167\n",
      "3168\n",
      "3169\n",
      "3170\n",
      "3171\n",
      "3172\n",
      "3173\n",
      "3174\n",
      "3175\n",
      "3176\n",
      "3177\n",
      "3178\n",
      "3179\n",
      "3180\n",
      "3181\n",
      "3182\n",
      "3183\n",
      "3184\n",
      "3185\n",
      "3186\n",
      "3187\n",
      "3188\n",
      "3189\n",
      "3190\n",
      "3191\n",
      "3192\n",
      "3193\n",
      "3194\n",
      "3195\n",
      "3196\n",
      "3197\n",
      "3198\n",
      "3199\n",
      "3200\n",
      "3201\n",
      "3202\n",
      "3203\n",
      "3204\n",
      "3205\n",
      "3206\n",
      "3207\n",
      "3208\n",
      "3209\n",
      "3210\n",
      "3211\n",
      "3212\n",
      "3213\n",
      "3214\n",
      "3215\n",
      "3216\n",
      "3217\n",
      "3218\n",
      "3219\n",
      "3220\n",
      "3221\n",
      "3222\n",
      "3223\n",
      "3224\n",
      "3225\n",
      "3226\n",
      "3227\n",
      "3228\n",
      "3229\n",
      "3230\n",
      "3231\n",
      "3232\n",
      "3233\n",
      "3234\n",
      "3235\n",
      "3236\n",
      "3237\n",
      "3238\n",
      "3239\n",
      "3240\n",
      "3241\n",
      "3242\n",
      "3243\n",
      "3244\n",
      "3245\n",
      "3246\n",
      "3247\n",
      "3248\n",
      "3249\n",
      "3250\n",
      "3251\n",
      "3252\n",
      "3253\n",
      "3254\n",
      "3255\n",
      "3256\n",
      "3257\n",
      "3258\n",
      "3259\n",
      "3260\n",
      "3261\n",
      "3262\n",
      "3263\n",
      "3264\n",
      "3265\n",
      "3266\n",
      "3267\n",
      "3268\n",
      "3269\n",
      "3270\n",
      "3271\n",
      "3272\n",
      "3273\n",
      "3274\n",
      "3275\n",
      "3276\n",
      "3277\n",
      "3278\n",
      "3279\n",
      "3280\n",
      "3281\n",
      "3282\n",
      "3283\n",
      "3284\n",
      "3285\n",
      "3286\n",
      "3287\n",
      "3288\n",
      "3289\n",
      "3290\n",
      "3291\n",
      "3292\n",
      "3293\n",
      "3294\n",
      "3295\n",
      "3296\n",
      "3297\n",
      "3298\n",
      "3299\n",
      "3300\n",
      "3301\n",
      "3302\n",
      "3303\n",
      "3304\n",
      "3305\n",
      "3306\n",
      "3307\n",
      "3308\n",
      "3309\n",
      "3310\n",
      "3311\n",
      "3312\n",
      "3313\n",
      "3314\n",
      "3315\n",
      "3316\n",
      "3317\n",
      "3318\n",
      "3319\n",
      "3320\n",
      "3321\n",
      "3322\n",
      "3323\n",
      "3324\n",
      "3325\n",
      "3326\n",
      "3327\n",
      "3328\n",
      "3329\n",
      "3330\n",
      "3331\n",
      "3332\n",
      "3333\n",
      "3334\n",
      "3335\n",
      "3336\n",
      "3337\n",
      "3338\n",
      "3339\n",
      "3340\n",
      "3341\n",
      "3342\n",
      "3343\n",
      "3344\n",
      "3345\n",
      "3346\n",
      "3347\n",
      "3348\n",
      "3349\n",
      "3350\n",
      "3351\n",
      "3352\n",
      "3353\n",
      "3354\n",
      "3355\n",
      "3356\n",
      "3357\n",
      "3358\n",
      "3359\n",
      "3360\n",
      "3361\n",
      "3362\n",
      "3363\n",
      "3364\n",
      "3365\n",
      "3366\n",
      "3367\n",
      "3368\n",
      "3369\n",
      "3370\n",
      "3371\n",
      "3372\n",
      "3373\n",
      "3374\n",
      "3375\n",
      "3376\n",
      "3377\n",
      "3378\n",
      "3379\n",
      "3380\n",
      "3381\n",
      "3382\n",
      "3383\n",
      "3384\n",
      "3385\n",
      "3386\n",
      "3387\n",
      "3388\n",
      "3389\n",
      "3390\n",
      "3391\n",
      "3392\n",
      "3393\n",
      "3394\n",
      "3395\n",
      "3396\n",
      "3397\n",
      "3398\n",
      "3399\n",
      "3400\n",
      "3401\n",
      "3402\n",
      "3403\n",
      "3404\n",
      "3405\n",
      "3406\n",
      "3407\n",
      "3408\n",
      "3409\n",
      "3410\n",
      "3411\n",
      "3412\n",
      "3413\n",
      "3414\n",
      "3415\n",
      "3416\n",
      "3417\n",
      "3418\n",
      "3419\n",
      "3420\n",
      "3421\n",
      "3422\n",
      "3423\n",
      "3424\n",
      "3425\n",
      "3426\n",
      "3427\n",
      "3428\n",
      "3429\n",
      "3430\n",
      "3431\n",
      "3432\n",
      "3433\n",
      "3434\n",
      "3435\n",
      "3436\n",
      "3437\n",
      "3438\n",
      "3439\n",
      "3440\n",
      "3441\n",
      "3442\n",
      "3443\n",
      "3444\n",
      "3445\n",
      "3446\n",
      "3447\n",
      "3448\n",
      "3449\n",
      "3450\n",
      "3451\n",
      "3452\n",
      "3453\n",
      "3454\n",
      "3455\n",
      "3456\n",
      "3457\n",
      "3458\n",
      "3459\n",
      "3460\n",
      "3461\n",
      "3462\n",
      "3463\n",
      "3464\n",
      "3465\n",
      "3466\n",
      "3467\n",
      "3468\n",
      "3469\n",
      "3470\n",
      "3471\n",
      "3472\n",
      "3473\n",
      "3474\n",
      "3475\n",
      "3476\n",
      "3477\n",
      "3478\n",
      "3479\n",
      "3480\n",
      "3481\n",
      "3482\n",
      "3483\n",
      "3484\n",
      "3485\n",
      "3486\n",
      "3487\n",
      "3488\n",
      "3489\n",
      "3490\n",
      "3491\n",
      "3492\n",
      "3493\n",
      "3494\n",
      "3495\n",
      "3496\n",
      "3497\n",
      "3498\n",
      "3499\n",
      "3500\n",
      "3501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3502\n",
      "3503\n",
      "3504\n",
      "3505\n",
      "3506\n",
      "3507\n",
      "3508\n",
      "3509\n",
      "3510\n",
      "3511\n",
      "3512\n",
      "3513\n",
      "3514\n",
      "3515\n",
      "3516\n",
      "3517\n",
      "3518\n",
      "3519\n",
      "3520\n",
      "3521\n",
      "3522\n",
      "3523\n",
      "3524\n",
      "3525\n",
      "3526\n",
      "3527\n",
      "3528\n",
      "3529\n",
      "3530\n",
      "3531\n",
      "3532\n",
      "3533\n",
      "3534\n",
      "3535\n",
      "3536\n",
      "3537\n",
      "3538\n",
      "3539\n",
      "3540\n",
      "3541\n",
      "3542\n",
      "3543\n",
      "3544\n",
      "3545\n",
      "3546\n",
      "3547\n",
      "3548\n",
      "3549\n",
      "3550\n",
      "3551\n",
      "3552\n",
      "3553\n",
      "3554\n",
      "3555\n",
      "3556\n",
      "3557\n",
      "3558\n",
      "3559\n",
      "3560\n",
      "3561\n",
      "3562\n",
      "3563\n",
      "3564\n",
      "3565\n",
      "3566\n",
      "3567\n",
      "3568\n",
      "3569\n",
      "3570\n",
      "3571\n",
      "3572\n",
      "3573\n",
      "3574\n",
      "3575\n",
      "3576\n",
      "3577\n",
      "3578\n",
      "3579\n",
      "3580\n",
      "3581\n",
      "3582\n",
      "3583\n",
      "3584\n",
      "3585\n",
      "3586\n",
      "3587\n",
      "3588\n",
      "3589\n",
      "3590\n",
      "3591\n",
      "3592\n",
      "3593\n",
      "3594\n",
      "3595\n",
      "3596\n",
      "3597\n",
      "3598\n",
      "3599\n",
      "3600\n",
      "3601\n",
      "3602\n",
      "3603\n",
      "3604\n",
      "3605\n",
      "3606\n",
      "3607\n",
      "3608\n",
      "3609\n",
      "3610\n",
      "3611\n",
      "3612\n",
      "3613\n",
      "3614\n",
      "3615\n",
      "3616\n",
      "3617\n",
      "3618\n",
      "3619\n",
      "3620\n",
      "3621\n",
      "3622\n",
      "3623\n",
      "3624\n",
      "3625\n",
      "3626\n",
      "3627\n",
      "3628\n",
      "3629\n",
      "3630\n",
      "3631\n",
      "3632\n",
      "3633\n",
      "3634\n",
      "3635\n",
      "3636\n",
      "3637\n",
      "3638\n",
      "3639\n",
      "3640\n",
      "3641\n",
      "3642\n",
      "3643\n",
      "3644\n",
      "3645\n",
      "3646\n",
      "3647\n",
      "3648\n",
      "3649\n",
      "3650\n",
      "3651\n",
      "3652\n",
      "3653\n",
      "3654\n",
      "3655\n",
      "3656\n",
      "3657\n",
      "3658\n",
      "3659\n",
      "3660\n",
      "3661\n",
      "3662\n",
      "3663\n",
      "3664\n",
      "3665\n",
      "3666\n",
      "3667\n",
      "3668\n",
      "3669\n",
      "3670\n",
      "3671\n",
      "3672\n",
      "3673\n",
      "3674\n",
      "3675\n",
      "3676\n",
      "3677\n",
      "3678\n",
      "3679\n",
      "3680\n",
      "3681\n",
      "3682\n",
      "3683\n",
      "3684\n",
      "3685\n",
      "3686\n",
      "3687\n",
      "3688\n",
      "3689\n",
      "3690\n",
      "3691\n",
      "3692\n",
      "3693\n",
      "3694\n",
      "3695\n",
      "3696\n",
      "3697\n",
      "3698\n",
      "3699\n",
      "3700\n",
      "3701\n",
      "3702\n",
      "3703\n",
      "3704\n",
      "3705\n",
      "3706\n",
      "3707\n",
      "3708\n",
      "3709\n",
      "3710\n",
      "3711\n",
      "3712\n",
      "3713\n",
      "3714\n",
      "3715\n",
      "3716\n",
      "3717\n",
      "3718\n",
      "3719\n",
      "3720\n",
      "3721\n",
      "3722\n",
      "3723\n",
      "3724\n",
      "3725\n",
      "3726\n",
      "3727\n",
      "3728\n",
      "3729\n",
      "3730\n",
      "3731\n",
      "3732\n",
      "3733\n",
      "3734\n",
      "3735\n",
      "3736\n",
      "3737\n",
      "3738\n",
      "3739\n",
      "3740\n",
      "3741\n",
      "3742\n",
      "3743\n",
      "3744\n",
      "3745\n",
      "3746\n",
      "3747\n",
      "3748\n",
      "3749\n",
      "3750\n",
      "3751\n",
      "3752\n",
      "3753\n",
      "3754\n",
      "3755\n",
      "3756\n",
      "3757\n",
      "3758\n",
      "3759\n",
      "3760\n",
      "3761\n",
      "3762\n",
      "3763\n",
      "3764\n",
      "3765\n",
      "3766\n",
      "3767\n",
      "3768\n",
      "3769\n",
      "3770\n",
      "3771\n",
      "3772\n",
      "3773\n",
      "3774\n",
      "3775\n",
      "3776\n",
      "3777\n",
      "3778\n",
      "3779\n",
      "3780\n",
      "3781\n",
      "3782\n",
      "3783\n",
      "3784\n",
      "3785\n",
      "3786\n",
      "3787\n",
      "3788\n",
      "3789\n",
      "3790\n",
      "3791\n",
      "3792\n",
      "3793\n",
      "3794\n",
      "3795\n",
      "3796\n",
      "3797\n",
      "3798\n",
      "3799\n",
      "3800\n",
      "3801\n",
      "3802\n",
      "3803\n",
      "3804\n",
      "3805\n",
      "3806\n",
      "3807\n",
      "3808\n",
      "3809\n",
      "3810\n",
      "3811\n",
      "3812\n",
      "3813\n",
      "3814\n",
      "3815\n",
      "3816\n",
      "3817\n",
      "3818\n",
      "3819\n",
      "3820\n",
      "3821\n",
      "3822\n",
      "3823\n",
      "3824\n",
      "3825\n",
      "3826\n",
      "3827\n",
      "3828\n",
      "3829\n",
      "3830\n",
      "3831\n",
      "3832\n",
      "3833\n",
      "3834\n",
      "3835\n",
      "3836\n",
      "3837\n",
      "3838\n",
      "3839\n",
      "3840\n",
      "3841\n",
      "3842\n",
      "3843\n",
      "3844\n",
      "3845\n",
      "3846\n",
      "3847\n",
      "3848\n",
      "3849\n",
      "3850\n",
      "3851\n",
      "3852\n",
      "3853\n",
      "3854\n",
      "3855\n",
      "3856\n",
      "3857\n",
      "3858\n",
      "3859\n",
      "3860\n",
      "3861\n",
      "3862\n",
      "3863\n",
      "3864\n",
      "3865\n",
      "3866\n",
      "3867\n",
      "3868\n",
      "3869\n",
      "3870\n",
      "3871\n",
      "3872\n",
      "3873\n",
      "3874\n",
      "3875\n",
      "3876\n",
      "3877\n",
      "3878\n",
      "3879\n",
      "3880\n",
      "3881\n",
      "3882\n",
      "3883\n",
      "3884\n",
      "3885\n",
      "3886\n",
      "3887\n",
      "3888\n",
      "3889\n",
      "3890\n",
      "3891\n",
      "3892\n",
      "3893\n",
      "3894\n",
      "3895\n",
      "3896\n",
      "3897\n",
      "3898\n",
      "3899\n",
      "3900\n",
      "3901\n",
      "3902\n",
      "3903\n",
      "3904\n",
      "3905\n",
      "3906\n",
      "3907\n",
      "3908\n",
      "3909\n",
      "3910\n",
      "3911\n",
      "3912\n",
      "3913\n",
      "3914\n",
      "3915\n",
      "3916\n",
      "3917\n",
      "3918\n",
      "3919\n",
      "3920\n",
      "3921\n",
      "3922\n",
      "3923\n",
      "3924\n",
      "3925\n",
      "3926\n",
      "3927\n",
      "3928\n",
      "3929\n",
      "3930\n",
      "3931\n",
      "3932\n",
      "3933\n",
      "3934\n",
      "3935\n",
      "3936\n",
      "3937\n",
      "3938\n",
      "3939\n",
      "3940\n",
      "3941\n",
      "3942\n",
      "3943\n",
      "3944\n",
      "3945\n",
      "3946\n",
      "3947\n",
      "3948\n",
      "3949\n",
      "3950\n",
      "3951\n",
      "3952\n",
      "3953\n",
      "3954\n",
      "3955\n",
      "3956\n",
      "3957\n",
      "3958\n",
      "3959\n",
      "3960\n",
      "3961\n",
      "3962\n",
      "3963\n",
      "3964\n",
      "3965\n",
      "3966\n",
      "3967\n",
      "3968\n",
      "3969\n",
      "3970\n",
      "3971\n",
      "3972\n",
      "3973\n",
      "3974\n",
      "3975\n",
      "3976\n",
      "3977\n",
      "3978\n",
      "3979\n",
      "3980\n",
      "3981\n",
      "3982\n",
      "3983\n",
      "3984\n",
      "3985\n",
      "3986\n",
      "3987\n",
      "3988\n",
      "3989\n",
      "3990\n",
      "3991\n",
      "3992\n",
      "3993\n",
      "3994\n",
      "3995\n",
      "3996\n",
      "3997\n",
      "3998\n",
      "3999\n",
      "4000\n",
      "4001\n",
      "4002\n",
      "4003\n",
      "4004\n",
      "4005\n",
      "4006\n",
      "4007\n",
      "4008\n",
      "4009\n",
      "4010\n",
      "4011\n",
      "4012\n",
      "4013\n",
      "4014\n",
      "4015\n",
      "4016\n",
      "4017\n",
      "4018\n",
      "4019\n",
      "4020\n",
      "4021\n",
      "4022\n",
      "4023\n",
      "4024\n",
      "4025\n",
      "4026\n",
      "4027\n",
      "4028\n",
      "4029\n",
      "4030\n",
      "4031\n",
      "4032\n",
      "4033\n",
      "4034\n",
      "4035\n",
      "4036\n",
      "4037\n",
      "4038\n",
      "4039\n",
      "4040\n",
      "4041\n",
      "4042\n",
      "4043\n",
      "4044\n",
      "4045\n",
      "4046\n",
      "4047\n",
      "4048\n",
      "4049\n",
      "4050\n",
      "4051\n",
      "4052\n",
      "4053\n",
      "4054\n",
      "4055\n",
      "4056\n",
      "4057\n",
      "4058\n",
      "4059\n",
      "4060\n",
      "4061\n",
      "4062\n",
      "4063\n",
      "4064\n",
      "4065\n",
      "4066\n",
      "4067\n",
      "4068\n",
      "4069\n",
      "4070\n",
      "4071\n",
      "4072\n",
      "4073\n",
      "4074\n",
      "4075\n",
      "4076\n",
      "4077\n",
      "4078\n",
      "4079\n",
      "4080\n",
      "4081\n",
      "4082\n",
      "4083\n",
      "4084\n",
      "4085\n",
      "4086\n",
      "4087\n",
      "4088\n",
      "4089\n",
      "4090\n",
      "4091\n",
      "4092\n",
      "4093\n",
      "4094\n",
      "4095\n",
      "4096\n",
      "4097\n",
      "4098\n",
      "4099\n",
      "4100\n",
      "4101\n",
      "4102\n",
      "4103\n",
      "4104\n",
      "4105\n",
      "4106\n",
      "4107\n",
      "4108\n",
      "4109\n",
      "4110\n",
      "4111\n",
      "4112\n",
      "4113\n",
      "4114\n",
      "4115\n",
      "4116\n",
      "4117\n",
      "4118\n",
      "4119\n",
      "4120\n",
      "4121\n",
      "4122\n",
      "4123\n",
      "4124\n",
      "4125\n",
      "4126\n",
      "4127\n",
      "4128\n",
      "4129\n",
      "4130\n",
      "4131\n",
      "4132\n",
      "4133\n",
      "4134\n",
      "4135\n",
      "4136\n",
      "4137\n",
      "4138\n",
      "4139\n",
      "4140\n",
      "4141\n",
      "4142\n",
      "4143\n",
      "4144\n",
      "4145\n",
      "4146\n",
      "4147\n",
      "4148\n",
      "4149\n",
      "4150\n",
      "4151\n",
      "4152\n",
      "4153\n",
      "4154\n",
      "4155\n",
      "4156\n",
      "4157\n",
      "4158\n",
      "4159\n",
      "4160\n",
      "4161\n",
      "4162\n",
      "4163\n",
      "4164\n",
      "4165\n",
      "4166\n",
      "4167\n",
      "4168\n",
      "4169\n",
      "4170\n",
      "4171\n",
      "4172\n",
      "4173\n",
      "4174\n",
      "4175\n",
      "4176\n",
      "4177\n",
      "4178\n",
      "4179\n",
      "4180\n",
      "4181\n",
      "4182\n",
      "4183\n",
      "4184\n",
      "4185\n",
      "4186\n",
      "4187\n",
      "4188\n",
      "4189\n",
      "4190\n",
      "4191\n",
      "4192\n",
      "4193\n",
      "4194\n",
      "4195\n",
      "4196\n",
      "4197\n",
      "4198\n",
      "4199\n",
      "4200\n",
      "4201\n",
      "4202\n",
      "4203\n",
      "4204\n",
      "4205\n",
      "4206\n",
      "4207\n",
      "4208\n",
      "4209\n",
      "4210\n",
      "4211\n",
      "4212\n",
      "4213\n",
      "4214\n",
      "4215\n",
      "4216\n",
      "4217\n",
      "4218\n",
      "4219\n",
      "4220\n",
      "4221\n",
      "4222\n",
      "4223\n",
      "4224\n",
      "4225\n",
      "4226\n",
      "4227\n",
      "4228\n",
      "4229\n",
      "4230\n",
      "4231\n",
      "4232\n",
      "4233\n",
      "4234\n",
      "4235\n",
      "4236\n",
      "4237\n",
      "4238\n",
      "4239\n",
      "4240\n",
      "4241\n",
      "4242\n",
      "4243\n",
      "4244\n",
      "4245\n",
      "4246\n",
      "4247\n",
      "4248\n",
      "4249\n",
      "4250\n",
      "4251\n",
      "4252\n",
      "4253\n",
      "4254\n",
      "4255\n",
      "4256\n",
      "4257\n",
      "4258\n",
      "4259\n",
      "4260\n",
      "4261\n",
      "4262\n",
      "4263\n",
      "4264\n",
      "4265\n",
      "4266\n",
      "4267\n",
      "4268\n",
      "4269\n",
      "4270\n",
      "4271\n",
      "4272\n",
      "4273\n",
      "4274\n",
      "4275\n",
      "4276\n",
      "4277\n",
      "4278\n",
      "4279\n",
      "4280\n",
      "4281\n",
      "4282\n",
      "4283\n",
      "4284\n",
      "4285\n",
      "4286\n",
      "4287\n",
      "4288\n",
      "4289\n",
      "4290\n",
      "4291\n",
      "4292\n",
      "4293\n",
      "4294\n",
      "4295\n",
      "4296\n",
      "4297\n",
      "4298\n",
      "4299\n",
      "4300\n",
      "4301\n",
      "4302\n",
      "4303\n",
      "4304\n",
      "4305\n",
      "4306\n",
      "4307\n",
      "4308\n",
      "4309\n",
      "4310\n",
      "4311\n",
      "4312\n",
      "4313\n",
      "4314\n",
      "4315\n",
      "4316\n",
      "4317\n",
      "4318\n",
      "4319\n",
      "4320\n",
      "4321\n",
      "4322\n",
      "4323\n",
      "4324\n",
      "4325\n",
      "4326\n",
      "4327\n",
      "4328\n",
      "4329\n",
      "4330\n",
      "4331\n",
      "4332\n",
      "4333\n",
      "4334\n",
      "4335\n",
      "4336\n",
      "4337\n",
      "4338\n",
      "4339\n",
      "4340\n",
      "4341\n",
      "4342\n",
      "4343\n",
      "4344\n",
      "4345\n",
      "4346\n",
      "4347\n",
      "4348\n",
      "4349\n",
      "4350\n",
      "4351\n",
      "4352\n",
      "4353\n",
      "4354\n",
      "4355\n",
      "4356\n",
      "4357\n",
      "4358\n",
      "4359\n",
      "4360\n",
      "4361\n",
      "4362\n",
      "4363\n",
      "4364\n",
      "4365\n",
      "4366\n",
      "4367\n",
      "4368\n",
      "4369\n",
      "4370\n",
      "4371\n",
      "4372\n",
      "4373\n",
      "4374\n",
      "4375\n",
      "4376\n",
      "4377\n",
      "4378\n",
      "4379\n",
      "4380\n",
      "4381\n",
      "4382\n",
      "4383\n",
      "4384\n",
      "4385\n",
      "4386\n",
      "4387\n",
      "4388\n",
      "4389\n",
      "4390\n",
      "4391\n",
      "4392\n",
      "4393\n",
      "4394\n",
      "4395\n",
      "4396\n",
      "4397\n",
      "4398\n",
      "4399\n",
      "4400\n",
      "4401\n",
      "4402\n",
      "4403\n",
      "4404\n",
      "4405\n",
      "4406\n",
      "4407\n",
      "4408\n",
      "4409\n",
      "4410\n",
      "4411\n",
      "4412\n",
      "4413\n",
      "4414\n",
      "4415\n",
      "4416\n",
      "4417\n",
      "4418\n",
      "4419\n",
      "4420\n",
      "4421\n",
      "4422\n",
      "4423\n",
      "4424\n",
      "4425\n",
      "4426\n",
      "4427\n",
      "4428\n",
      "4429\n",
      "4430\n",
      "4431\n",
      "4432\n",
      "4433\n",
      "4434\n",
      "4435\n",
      "4436\n",
      "4437\n",
      "4438\n",
      "4439\n",
      "4440\n",
      "4441\n",
      "4442\n",
      "4443\n",
      "4444\n",
      "4445\n",
      "4446\n",
      "4447\n",
      "4448\n",
      "4449\n",
      "4450\n",
      "4451\n",
      "4452\n",
      "4453\n",
      "4454\n",
      "4455\n",
      "4456\n",
      "4457\n",
      "4458\n",
      "4459\n",
      "4460\n",
      "4461\n",
      "4462\n",
      "4463\n",
      "4464\n",
      "4465\n",
      "4466\n",
      "4467\n",
      "4468\n",
      "4469\n",
      "4470\n",
      "4471\n",
      "4472\n",
      "4473\n",
      "4474\n",
      "4475\n",
      "4476\n",
      "4477\n",
      "4478\n",
      "4479\n",
      "4480\n",
      "4481\n",
      "4482\n",
      "4483\n",
      "4484\n",
      "4485\n",
      "4486\n",
      "4487\n",
      "4488\n",
      "4489\n",
      "4490\n",
      "4491\n",
      "4492\n",
      "4493\n",
      "4494\n",
      "4495\n",
      "4496\n",
      "4497\n",
      "4498\n",
      "4499\n",
      "4500\n",
      "4501\n",
      "4502\n",
      "4503\n",
      "4504\n",
      "4505\n",
      "4506\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "RF = RandomForestRegressor()\n",
    "\n",
    "\n",
    "ScoresRF = []\n",
    "for i in np.arange(6, NormX.shape[1]):\n",
    "\n",
    "    x = NormX.iloc[:,[0, 1, 2, 3, 4, 5, 6, i]]\n",
    "    print(i)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "\n",
    "    RF.fit(x_train, y_train)\n",
    "    ScoresRF.append((mean_absolute_error(RF.predict(x_test), y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3657</th>\n",
       "      <td>0.026622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0.026714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>0.026846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4419</th>\n",
       "      <td>0.027190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>0.027391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>0.070016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.070107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3432</th>\n",
       "      <td>0.071196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>0.073103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>0.073406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4501 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0\n",
       "3657  0.026622\n",
       "165   0.026714\n",
       "616   0.026846\n",
       "4419  0.027190\n",
       "883   0.027391\n",
       "...        ...\n",
       "915   0.070016\n",
       "200   0.070107\n",
       "3432  0.071196\n",
       "1990  0.073103\n",
       "472   0.073406\n",
       "\n",
       "[4501 rows x 1 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(ScoresRF).sort_values(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>r</th>\n",
       "      <td>0.074224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Area</th>\n",
       "      <td>0.161847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>In</th>\n",
       "      <td>0.001201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z</th>\n",
       "      <td>0.095599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tplus</th>\n",
       "      <td>0.086647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tminus</th>\n",
       "      <td>0.037520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tminus_</th>\n",
       "      <td>0.227341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analytic 0.4999000000000129</th>\n",
       "      <td>0.315621</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    0\n",
       "r                            0.074224\n",
       "Area                         0.161847\n",
       "In                           0.001201\n",
       "Z                            0.095599\n",
       "tplus                        0.086647\n",
       "tminus                       0.037520\n",
       "tminus_                      0.227341\n",
       "analytic 0.4999000000000129  0.315621"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(RF.feature_importances_, index=x_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1441\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1520\n",
      "1521\n",
      "1522\n",
      "1523\n",
      "1524\n",
      "1525\n",
      "1526\n",
      "1527\n",
      "1528\n",
      "1529\n",
      "1530\n",
      "1531\n",
      "1532\n",
      "1533\n",
      "1534\n",
      "1535\n",
      "1536\n",
      "1537\n",
      "1538\n",
      "1539\n",
      "1540\n",
      "1541\n",
      "1542\n",
      "1543\n",
      "1544\n",
      "1545\n",
      "1546\n",
      "1547\n",
      "1548\n",
      "1549\n",
      "1550\n",
      "1551\n",
      "1552\n",
      "1553\n",
      "1554\n",
      "1555\n",
      "1556\n",
      "1557\n",
      "1558\n",
      "1559\n",
      "1560\n",
      "1561\n",
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1566\n",
      "1567\n",
      "1568\n",
      "1569\n",
      "1570\n",
      "1571\n",
      "1572\n",
      "1573\n",
      "1574\n",
      "1575\n",
      "1576\n",
      "1577\n",
      "1578\n",
      "1579\n",
      "1580\n",
      "1581\n",
      "1582\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1586\n",
      "1587\n",
      "1588\n",
      "1589\n",
      "1590\n",
      "1591\n",
      "1592\n",
      "1593\n",
      "1594\n",
      "1595\n",
      "1596\n",
      "1597\n",
      "1598\n",
      "1599\n",
      "1600\n",
      "1601\n",
      "1602\n",
      "1603\n",
      "1604\n",
      "1605\n",
      "1606\n",
      "1607\n",
      "1608\n",
      "1609\n",
      "1610\n",
      "1611\n",
      "1612\n",
      "1613\n",
      "1614\n",
      "1615\n",
      "1616\n",
      "1617\n",
      "1618\n",
      "1619\n",
      "1620\n",
      "1621\n",
      "1622\n",
      "1623\n",
      "1624\n",
      "1625\n",
      "1626\n",
      "1627\n",
      "1628\n",
      "1629\n",
      "1630\n",
      "1631\n",
      "1632\n",
      "1633\n",
      "1634\n",
      "1635\n",
      "1636\n",
      "1637\n",
      "1638\n",
      "1639\n",
      "1640\n",
      "1641\n",
      "1642\n",
      "1643\n",
      "1644\n",
      "1645\n",
      "1646\n",
      "1647\n",
      "1648\n",
      "1649\n",
      "1650\n",
      "1651\n",
      "1652\n",
      "1653\n",
      "1654\n",
      "1655\n",
      "1656\n",
      "1657\n",
      "1658\n",
      "1659\n",
      "1660\n",
      "1661\n",
      "1662\n",
      "1663\n",
      "1664\n",
      "1665\n",
      "1666\n",
      "1667\n",
      "1668\n",
      "1669\n",
      "1670\n",
      "1671\n",
      "1672\n",
      "1673\n",
      "1674\n",
      "1675\n",
      "1676\n",
      "1677\n",
      "1678\n",
      "1679\n",
      "1680\n",
      "1681\n",
      "1682\n",
      "1683\n",
      "1684\n",
      "1685\n",
      "1686\n",
      "1687\n",
      "1688\n",
      "1689\n",
      "1690\n",
      "1691\n",
      "1692\n",
      "1693\n",
      "1694\n",
      "1695\n",
      "1696\n",
      "1697\n",
      "1698\n",
      "1699\n",
      "1700\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1704\n",
      "1705\n",
      "1706\n",
      "1707\n",
      "1708\n",
      "1709\n",
      "1710\n",
      "1711\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n",
      "1719\n",
      "1720\n",
      "1721\n",
      "1722\n",
      "1723\n",
      "1724\n",
      "1725\n",
      "1726\n",
      "1727\n",
      "1728\n",
      "1729\n",
      "1730\n",
      "1731\n",
      "1732\n",
      "1733\n",
      "1734\n",
      "1735\n",
      "1736\n",
      "1737\n",
      "1738\n",
      "1739\n",
      "1740\n",
      "1741\n",
      "1742\n",
      "1743\n",
      "1744\n",
      "1745\n",
      "1746\n",
      "1747\n",
      "1748\n",
      "1749\n",
      "1750\n",
      "1751\n",
      "1752\n",
      "1753\n",
      "1754\n",
      "1755\n",
      "1756\n",
      "1757\n",
      "1758\n",
      "1759\n",
      "1760\n",
      "1761\n",
      "1762\n",
      "1763\n",
      "1764\n",
      "1765\n",
      "1766\n",
      "1767\n",
      "1768\n",
      "1769\n",
      "1770\n",
      "1771\n",
      "1772\n",
      "1773\n",
      "1774\n",
      "1775\n",
      "1776\n",
      "1777\n",
      "1778\n",
      "1779\n",
      "1780\n",
      "1781\n",
      "1782\n",
      "1783\n",
      "1784\n",
      "1785\n",
      "1786\n",
      "1787\n",
      "1788\n",
      "1789\n",
      "1790\n",
      "1791\n",
      "1792\n",
      "1793\n",
      "1794\n",
      "1795\n",
      "1796\n",
      "1797\n",
      "1798\n",
      "1799\n",
      "1800\n",
      "1801\n",
      "1802\n",
      "1803\n",
      "1804\n",
      "1805\n",
      "1806\n",
      "1807\n",
      "1808\n",
      "1809\n",
      "1810\n",
      "1811\n",
      "1812\n",
      "1813\n",
      "1814\n",
      "1815\n",
      "1816\n",
      "1817\n",
      "1818\n",
      "1819\n",
      "1820\n",
      "1821\n",
      "1822\n",
      "1823\n",
      "1824\n",
      "1825\n",
      "1826\n",
      "1827\n",
      "1828\n",
      "1829\n",
      "1830\n",
      "1831\n",
      "1832\n",
      "1833\n",
      "1834\n",
      "1835\n",
      "1836\n",
      "1837\n",
      "1838\n",
      "1839\n",
      "1840\n",
      "1841\n",
      "1842\n",
      "1843\n",
      "1844\n",
      "1845\n",
      "1846\n",
      "1847\n",
      "1848\n",
      "1849\n",
      "1850\n",
      "1851\n",
      "1852\n",
      "1853\n",
      "1854\n",
      "1855\n",
      "1856\n",
      "1857\n",
      "1858\n",
      "1859\n",
      "1860\n",
      "1861\n",
      "1862\n",
      "1863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1864\n",
      "1865\n",
      "1866\n",
      "1867\n",
      "1868\n",
      "1869\n",
      "1870\n",
      "1871\n",
      "1872\n",
      "1873\n",
      "1874\n",
      "1875\n",
      "1876\n",
      "1877\n",
      "1878\n",
      "1879\n",
      "1880\n",
      "1881\n",
      "1882\n",
      "1883\n",
      "1884\n",
      "1885\n",
      "1886\n",
      "1887\n",
      "1888\n",
      "1889\n",
      "1890\n",
      "1891\n",
      "1892\n",
      "1893\n",
      "1894\n",
      "1895\n",
      "1896\n",
      "1897\n",
      "1898\n",
      "1899\n",
      "1900\n",
      "1901\n",
      "1902\n",
      "1903\n",
      "1904\n",
      "1905\n",
      "1906\n",
      "1907\n",
      "1908\n",
      "1909\n",
      "1910\n",
      "1911\n",
      "1912\n",
      "1913\n",
      "1914\n",
      "1915\n",
      "1916\n",
      "1917\n",
      "1918\n",
      "1919\n",
      "1920\n",
      "1921\n",
      "1922\n",
      "1923\n",
      "1924\n",
      "1925\n",
      "1926\n",
      "1927\n",
      "1928\n",
      "1929\n",
      "1930\n",
      "1931\n",
      "1932\n",
      "1933\n",
      "1934\n",
      "1935\n",
      "1936\n",
      "1937\n",
      "1938\n",
      "1939\n",
      "1940\n",
      "1941\n",
      "1942\n",
      "1943\n",
      "1944\n",
      "1945\n",
      "1946\n",
      "1947\n",
      "1948\n",
      "1949\n",
      "1950\n",
      "1951\n",
      "1952\n",
      "1953\n",
      "1954\n",
      "1955\n",
      "1956\n",
      "1957\n",
      "1958\n",
      "1959\n",
      "1960\n",
      "1961\n",
      "1962\n",
      "1963\n",
      "1964\n",
      "1965\n",
      "1966\n",
      "1967\n",
      "1968\n",
      "1969\n",
      "1970\n",
      "1971\n",
      "1972\n",
      "1973\n",
      "1974\n",
      "1975\n",
      "1976\n",
      "1977\n",
      "1978\n",
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n",
      "2024\n",
      "2025\n",
      "2026\n",
      "2027\n",
      "2028\n",
      "2029\n",
      "2030\n",
      "2031\n",
      "2032\n",
      "2033\n",
      "2034\n",
      "2035\n",
      "2036\n",
      "2037\n",
      "2038\n",
      "2039\n",
      "2040\n",
      "2041\n",
      "2042\n",
      "2043\n",
      "2044\n",
      "2045\n",
      "2046\n",
      "2047\n",
      "2048\n",
      "2049\n",
      "2050\n",
      "2051\n",
      "2052\n",
      "2053\n",
      "2054\n",
      "2055\n",
      "2056\n",
      "2057\n",
      "2058\n",
      "2059\n",
      "2060\n",
      "2061\n",
      "2062\n",
      "2063\n",
      "2064\n",
      "2065\n",
      "2066\n",
      "2067\n",
      "2068\n",
      "2069\n",
      "2070\n",
      "2071\n",
      "2072\n",
      "2073\n",
      "2074\n",
      "2075\n",
      "2076\n",
      "2077\n",
      "2078\n",
      "2079\n",
      "2080\n",
      "2081\n",
      "2082\n",
      "2083\n",
      "2084\n",
      "2085\n",
      "2086\n",
      "2087\n",
      "2088\n",
      "2089\n",
      "2090\n",
      "2091\n",
      "2092\n",
      "2093\n",
      "2094\n",
      "2095\n",
      "2096\n",
      "2097\n",
      "2098\n",
      "2099\n",
      "2100\n",
      "2101\n",
      "2102\n",
      "2103\n",
      "2104\n",
      "2105\n",
      "2106\n",
      "2107\n",
      "2108\n",
      "2109\n",
      "2110\n",
      "2111\n",
      "2112\n",
      "2113\n",
      "2114\n",
      "2115\n",
      "2116\n",
      "2117\n",
      "2118\n",
      "2119\n",
      "2120\n",
      "2121\n",
      "2122\n",
      "2123\n",
      "2124\n",
      "2125\n",
      "2126\n",
      "2127\n",
      "2128\n",
      "2129\n",
      "2130\n",
      "2131\n",
      "2132\n",
      "2133\n",
      "2134\n",
      "2135\n",
      "2136\n",
      "2137\n",
      "2138\n",
      "2139\n",
      "2140\n",
      "2141\n",
      "2142\n",
      "2143\n",
      "2144\n",
      "2145\n",
      "2146\n",
      "2147\n",
      "2148\n",
      "2149\n",
      "2150\n",
      "2151\n",
      "2152\n",
      "2153\n",
      "2154\n",
      "2155\n",
      "2156\n",
      "2157\n",
      "2158\n",
      "2159\n",
      "2160\n",
      "2161\n",
      "2162\n",
      "2163\n",
      "2164\n",
      "2165\n",
      "2166\n",
      "2167\n",
      "2168\n",
      "2169\n",
      "2170\n",
      "2171\n",
      "2172\n",
      "2173\n",
      "2174\n",
      "2175\n",
      "2176\n",
      "2177\n",
      "2178\n",
      "2179\n",
      "2180\n",
      "2181\n",
      "2182\n",
      "2183\n",
      "2184\n",
      "2185\n",
      "2186\n",
      "2187\n",
      "2188\n",
      "2189\n",
      "2190\n",
      "2191\n",
      "2192\n",
      "2193\n",
      "2194\n",
      "2195\n",
      "2196\n",
      "2197\n",
      "2198\n",
      "2199\n",
      "2200\n",
      "2201\n",
      "2202\n",
      "2203\n",
      "2204\n",
      "2205\n",
      "2206\n",
      "2207\n",
      "2208\n",
      "2209\n",
      "2210\n",
      "2211\n",
      "2212\n",
      "2213\n",
      "2214\n",
      "2215\n",
      "2216\n",
      "2217\n",
      "2218\n",
      "2219\n",
      "2220\n",
      "2221\n",
      "2222\n",
      "2223\n",
      "2224\n",
      "2225\n",
      "2226\n",
      "2227\n",
      "2228\n",
      "2229\n",
      "2230\n",
      "2231\n",
      "2232\n",
      "2233\n",
      "2234\n",
      "2235\n",
      "2236\n",
      "2237\n",
      "2238\n",
      "2239\n",
      "2240\n",
      "2241\n",
      "2242\n",
      "2243\n",
      "2244\n",
      "2245\n",
      "2246\n",
      "2247\n",
      "2248\n",
      "2249\n",
      "2250\n",
      "2251\n",
      "2252\n",
      "2253\n",
      "2254\n",
      "2255\n",
      "2256\n",
      "2257\n",
      "2258\n",
      "2259\n",
      "2260\n",
      "2261\n",
      "2262\n",
      "2263\n",
      "2264\n",
      "2265\n",
      "2266\n",
      "2267\n",
      "2268\n",
      "2269\n",
      "2270\n",
      "2271\n",
      "2272\n",
      "2273\n",
      "2274\n",
      "2275\n",
      "2276\n",
      "2277\n",
      "2278\n",
      "2279\n",
      "2280\n",
      "2281\n",
      "2282\n",
      "2283\n",
      "2284\n",
      "2285\n",
      "2286\n",
      "2287\n",
      "2288\n",
      "2289\n",
      "2290\n",
      "2291\n",
      "2292\n",
      "2293\n",
      "2294\n",
      "2295\n",
      "2296\n",
      "2297\n",
      "2298\n",
      "2299\n",
      "2300\n",
      "2301\n",
      "2302\n",
      "2303\n",
      "2304\n",
      "2305\n",
      "2306\n",
      "2307\n",
      "2308\n",
      "2309\n",
      "2310\n",
      "2311\n",
      "2312\n",
      "2313\n",
      "2314\n",
      "2315\n",
      "2316\n",
      "2317\n",
      "2318\n",
      "2319\n",
      "2320\n",
      "2321\n",
      "2322\n",
      "2323\n",
      "2324\n",
      "2325\n",
      "2326\n",
      "2327\n",
      "2328\n",
      "2329\n",
      "2330\n",
      "2331\n",
      "2332\n",
      "2333\n",
      "2334\n",
      "2335\n",
      "2336\n",
      "2337\n",
      "2338\n",
      "2339\n",
      "2340\n",
      "2341\n",
      "2342\n",
      "2343\n",
      "2344\n",
      "2345\n",
      "2346\n",
      "2347\n",
      "2348\n",
      "2349\n",
      "2350\n",
      "2351\n",
      "2352\n",
      "2353\n",
      "2354\n",
      "2355\n",
      "2356\n",
      "2357\n",
      "2358\n",
      "2359\n",
      "2360\n",
      "2361\n",
      "2362\n",
      "2363\n",
      "2364\n",
      "2365\n",
      "2366\n",
      "2367\n",
      "2368\n",
      "2369\n",
      "2370\n",
      "2371\n",
      "2372\n",
      "2373\n",
      "2374\n",
      "2375\n",
      "2376\n",
      "2377\n",
      "2378\n",
      "2379\n",
      "2380\n",
      "2381\n",
      "2382\n",
      "2383\n",
      "2384\n",
      "2385\n",
      "2386\n",
      "2387\n",
      "2388\n",
      "2389\n",
      "2390\n",
      "2391\n",
      "2392\n",
      "2393\n",
      "2394\n",
      "2395\n",
      "2396\n",
      "2397\n",
      "2398\n",
      "2399\n",
      "2400\n",
      "2401\n",
      "2402\n",
      "2403\n",
      "2404\n",
      "2405\n",
      "2406\n",
      "2407\n",
      "2408\n",
      "2409\n",
      "2410\n",
      "2411\n",
      "2412\n",
      "2413\n",
      "2414\n",
      "2415\n",
      "2416\n",
      "2417\n",
      "2418\n",
      "2419\n",
      "2420\n",
      "2421\n",
      "2422\n",
      "2423\n",
      "2424\n",
      "2425\n",
      "2426\n",
      "2427\n",
      "2428\n",
      "2429\n",
      "2430\n",
      "2431\n",
      "2432\n",
      "2433\n",
      "2434\n",
      "2435\n",
      "2436\n",
      "2437\n",
      "2438\n",
      "2439\n",
      "2440\n",
      "2441\n",
      "2442\n",
      "2443\n",
      "2444\n",
      "2445\n",
      "2446\n",
      "2447\n",
      "2448\n",
      "2449\n",
      "2450\n",
      "2451\n",
      "2452\n",
      "2453\n",
      "2454\n",
      "2455\n",
      "2456\n",
      "2457\n",
      "2458\n",
      "2459\n",
      "2460\n",
      "2461\n",
      "2462\n",
      "2463\n",
      "2464\n",
      "2465\n",
      "2466\n",
      "2467\n",
      "2468\n",
      "2469\n",
      "2470\n",
      "2471\n",
      "2472\n",
      "2473\n",
      "2474\n",
      "2475\n",
      "2476\n",
      "2477\n",
      "2478\n",
      "2479\n",
      "2480\n",
      "2481\n",
      "2482\n",
      "2483\n",
      "2484\n",
      "2485\n",
      "2486\n",
      "2487\n",
      "2488\n",
      "2489\n",
      "2490\n",
      "2491\n",
      "2492\n",
      "2493\n",
      "2494\n",
      "2495\n",
      "2496\n",
      "2497\n",
      "2498\n",
      "2499\n",
      "2500\n",
      "2501\n",
      "2502\n",
      "2503\n",
      "2504\n",
      "2505\n",
      "2506\n",
      "2507\n",
      "2508\n",
      "2509\n",
      "2510\n",
      "2511\n",
      "2512\n",
      "2513\n",
      "2514\n",
      "2515\n",
      "2516\n",
      "2517\n",
      "2518\n",
      "2519\n",
      "2520\n",
      "2521\n",
      "2522\n",
      "2523\n",
      "2524\n",
      "2525\n",
      "2526\n",
      "2527\n",
      "2528\n",
      "2529\n",
      "2530\n",
      "2531\n",
      "2532\n",
      "2533\n",
      "2534\n",
      "2535\n",
      "2536\n",
      "2537\n",
      "2538\n",
      "2539\n",
      "2540\n",
      "2541\n",
      "2542\n",
      "2543\n",
      "2544\n",
      "2545\n",
      "2546\n",
      "2547\n",
      "2548\n",
      "2549\n",
      "2550\n",
      "2551\n",
      "2552\n",
      "2553\n",
      "2554\n",
      "2555\n",
      "2556\n",
      "2557\n",
      "2558\n",
      "2559\n",
      "2560\n",
      "2561\n",
      "2562\n",
      "2563\n",
      "2564\n",
      "2565\n",
      "2566\n",
      "2567\n",
      "2568\n",
      "2569\n",
      "2570\n",
      "2571\n",
      "2572\n",
      "2573\n",
      "2574\n",
      "2575\n",
      "2576\n",
      "2577\n",
      "2578\n",
      "2579\n",
      "2580\n",
      "2581\n",
      "2582\n",
      "2583\n",
      "2584\n",
      "2585\n",
      "2586\n",
      "2587\n",
      "2588\n",
      "2589\n",
      "2590\n",
      "2591\n",
      "2592\n",
      "2593\n",
      "2594\n",
      "2595\n",
      "2596\n",
      "2597\n",
      "2598\n",
      "2599\n",
      "2600\n",
      "2601\n",
      "2602\n",
      "2603\n",
      "2604\n",
      "2605\n",
      "2606\n",
      "2607\n",
      "2608\n",
      "2609\n",
      "2610\n",
      "2611\n",
      "2612\n",
      "2613\n",
      "2614\n",
      "2615\n",
      "2616\n",
      "2617\n",
      "2618\n",
      "2619\n",
      "2620\n",
      "2621\n",
      "2622\n",
      "2623\n",
      "2624\n",
      "2625\n",
      "2626\n",
      "2627\n",
      "2628\n",
      "2629\n",
      "2630\n",
      "2631\n",
      "2632\n",
      "2633\n",
      "2634\n",
      "2635\n",
      "2636\n",
      "2637\n",
      "2638\n",
      "2639\n",
      "2640\n",
      "2641\n",
      "2642\n",
      "2643\n",
      "2644\n",
      "2645\n",
      "2646\n",
      "2647\n",
      "2648\n",
      "2649\n",
      "2650\n",
      "2651\n",
      "2652\n",
      "2653\n",
      "2654\n",
      "2655\n",
      "2656\n",
      "2657\n",
      "2658\n",
      "2659\n",
      "2660\n",
      "2661\n",
      "2662\n",
      "2663\n",
      "2664\n",
      "2665\n",
      "2666\n",
      "2667\n",
      "2668\n",
      "2669\n",
      "2670\n",
      "2671\n",
      "2672\n",
      "2673\n",
      "2674\n",
      "2675\n",
      "2676\n",
      "2677\n",
      "2678\n",
      "2679\n",
      "2680\n",
      "2681\n",
      "2682\n",
      "2683\n",
      "2684\n",
      "2685\n",
      "2686\n",
      "2687\n",
      "2688\n",
      "2689\n",
      "2690\n",
      "2691\n",
      "2692\n",
      "2693\n",
      "2694\n",
      "2695\n",
      "2696\n",
      "2697\n",
      "2698\n",
      "2699\n",
      "2700\n",
      "2701\n",
      "2702\n",
      "2703\n",
      "2704\n",
      "2705\n",
      "2706\n",
      "2707\n",
      "2708\n",
      "2709\n",
      "2710\n",
      "2711\n",
      "2712\n",
      "2713\n",
      "2714\n",
      "2715\n",
      "2716\n",
      "2717\n",
      "2718\n",
      "2719\n",
      "2720\n",
      "2721\n",
      "2722\n",
      "2723\n",
      "2724\n",
      "2725\n",
      "2726\n",
      "2727\n",
      "2728\n",
      "2729\n",
      "2730\n",
      "2731\n",
      "2732\n",
      "2733\n",
      "2734\n",
      "2735\n",
      "2736\n",
      "2737\n",
      "2738\n",
      "2739\n",
      "2740\n",
      "2741\n",
      "2742\n",
      "2743\n",
      "2744\n",
      "2745\n",
      "2746\n",
      "2747\n",
      "2748\n",
      "2749\n",
      "2750\n",
      "2751\n",
      "2752\n",
      "2753\n",
      "2754\n",
      "2755\n",
      "2756\n",
      "2757\n",
      "2758\n",
      "2759\n",
      "2760\n",
      "2761\n",
      "2762\n",
      "2763\n",
      "2764\n",
      "2765\n",
      "2766\n",
      "2767\n",
      "2768\n",
      "2769\n",
      "2770\n",
      "2771\n",
      "2772\n",
      "2773\n",
      "2774\n",
      "2775\n",
      "2776\n",
      "2777\n",
      "2778\n",
      "2779\n",
      "2780\n",
      "2781\n",
      "2782\n",
      "2783\n",
      "2784\n",
      "2785\n",
      "2786\n",
      "2787\n",
      "2788\n",
      "2789\n",
      "2790\n",
      "2791\n",
      "2792\n",
      "2793\n",
      "2794\n",
      "2795\n",
      "2796\n",
      "2797\n",
      "2798\n",
      "2799\n",
      "2800\n",
      "2801\n",
      "2802\n",
      "2803\n",
      "2804\n",
      "2805\n",
      "2806\n",
      "2807\n",
      "2808\n",
      "2809\n",
      "2810\n",
      "2811\n",
      "2812\n",
      "2813\n",
      "2814\n",
      "2815\n",
      "2816\n",
      "2817\n",
      "2818\n",
      "2819\n",
      "2820\n",
      "2821\n",
      "2822\n",
      "2823\n",
      "2824\n",
      "2825\n",
      "2826\n",
      "2827\n",
      "2828\n",
      "2829\n",
      "2830\n",
      "2831\n",
      "2832\n",
      "2833\n",
      "2834\n",
      "2835\n",
      "2836\n",
      "2837\n",
      "2838\n",
      "2839\n",
      "2840\n",
      "2841\n",
      "2842\n",
      "2843\n",
      "2844\n",
      "2845\n",
      "2846\n",
      "2847\n",
      "2848\n",
      "2849\n",
      "2850\n",
      "2851\n",
      "2852\n",
      "2853\n",
      "2854\n",
      "2855\n",
      "2856\n",
      "2857\n",
      "2858\n",
      "2859\n",
      "2860\n",
      "2861\n",
      "2862\n",
      "2863\n",
      "2864\n",
      "2865\n",
      "2866\n",
      "2867\n",
      "2868\n",
      "2869\n",
      "2870\n",
      "2871\n",
      "2872\n",
      "2873\n",
      "2874\n",
      "2875\n",
      "2876\n",
      "2877\n",
      "2878\n",
      "2879\n",
      "2880\n",
      "2881\n",
      "2882\n",
      "2883\n",
      "2884\n",
      "2885\n",
      "2886\n",
      "2887\n",
      "2888\n",
      "2889\n",
      "2890\n",
      "2891\n",
      "2892\n",
      "2893\n",
      "2894\n",
      "2895\n",
      "2896\n",
      "2897\n",
      "2898\n",
      "2899\n",
      "2900\n",
      "2901\n",
      "2902\n",
      "2903\n",
      "2904\n",
      "2905\n",
      "2906\n",
      "2907\n",
      "2908\n",
      "2909\n",
      "2910\n",
      "2911\n",
      "2912\n",
      "2913\n",
      "2914\n",
      "2915\n",
      "2916\n",
      "2917\n",
      "2918\n",
      "2919\n",
      "2920\n",
      "2921\n",
      "2922\n",
      "2923\n",
      "2924\n",
      "2925\n",
      "2926\n",
      "2927\n",
      "2928\n",
      "2929\n",
      "2930\n",
      "2931\n",
      "2932\n",
      "2933\n",
      "2934\n",
      "2935\n",
      "2936\n",
      "2937\n",
      "2938\n",
      "2939\n",
      "2940\n",
      "2941\n",
      "2942\n",
      "2943\n",
      "2944\n",
      "2945\n",
      "2946\n",
      "2947\n",
      "2948\n",
      "2949\n",
      "2950\n",
      "2951\n",
      "2952\n",
      "2953\n",
      "2954\n",
      "2955\n",
      "2956\n",
      "2957\n",
      "2958\n",
      "2959\n",
      "2960\n",
      "2961\n",
      "2962\n",
      "2963\n",
      "2964\n",
      "2965\n",
      "2966\n",
      "2967\n",
      "2968\n",
      "2969\n",
      "2970\n",
      "2971\n",
      "2972\n",
      "2973\n",
      "2974\n",
      "2975\n",
      "2976\n",
      "2977\n",
      "2978\n",
      "2979\n",
      "2980\n",
      "2981\n",
      "2982\n",
      "2983\n",
      "2984\n",
      "2985\n",
      "2986\n",
      "2987\n",
      "2988\n",
      "2989\n",
      "2990\n",
      "2991\n",
      "2992\n",
      "2993\n",
      "2994\n",
      "2995\n",
      "2996\n",
      "2997\n",
      "2998\n",
      "2999\n",
      "3000\n",
      "3001\n",
      "3002\n",
      "3003\n",
      "3004\n",
      "3005\n",
      "3006\n",
      "3007\n",
      "3008\n",
      "3009\n",
      "3010\n",
      "3011\n",
      "3012\n",
      "3013\n",
      "3014\n",
      "3015\n",
      "3016\n",
      "3017\n",
      "3018\n",
      "3019\n",
      "3020\n",
      "3021\n",
      "3022\n",
      "3023\n",
      "3024\n",
      "3025\n",
      "3026\n",
      "3027\n",
      "3028\n",
      "3029\n",
      "3030\n",
      "3031\n",
      "3032\n",
      "3033\n",
      "3034\n",
      "3035\n",
      "3036\n",
      "3037\n",
      "3038\n",
      "3039\n",
      "3040\n",
      "3041\n",
      "3042\n",
      "3043\n",
      "3044\n",
      "3045\n",
      "3046\n",
      "3047\n",
      "3048\n",
      "3049\n",
      "3050\n",
      "3051\n",
      "3052\n",
      "3053\n",
      "3054\n",
      "3055\n",
      "3056\n",
      "3057\n",
      "3058\n",
      "3059\n",
      "3060\n",
      "3061\n",
      "3062\n",
      "3063\n",
      "3064\n",
      "3065\n",
      "3066\n",
      "3067\n",
      "3068\n",
      "3069\n",
      "3070\n",
      "3071\n",
      "3072\n",
      "3073\n",
      "3074\n",
      "3075\n",
      "3076\n",
      "3077\n",
      "3078\n",
      "3079\n",
      "3080\n",
      "3081\n",
      "3082\n",
      "3083\n",
      "3084\n",
      "3085\n",
      "3086\n",
      "3087\n",
      "3088\n",
      "3089\n",
      "3090\n",
      "3091\n",
      "3092\n",
      "3093\n",
      "3094\n",
      "3095\n",
      "3096\n",
      "3097\n",
      "3098\n",
      "3099\n",
      "3100\n",
      "3101\n",
      "3102\n",
      "3103\n",
      "3104\n",
      "3105\n",
      "3106\n",
      "3107\n",
      "3108\n",
      "3109\n",
      "3110\n",
      "3111\n",
      "3112\n",
      "3113\n",
      "3114\n",
      "3115\n",
      "3116\n",
      "3117\n",
      "3118\n",
      "3119\n",
      "3120\n",
      "3121\n",
      "3122\n",
      "3123\n",
      "3124\n",
      "3125\n",
      "3126\n",
      "3127\n",
      "3128\n",
      "3129\n",
      "3130\n",
      "3131\n",
      "3132\n",
      "3133\n",
      "3134\n",
      "3135\n",
      "3136\n",
      "3137\n",
      "3138\n",
      "3139\n",
      "3140\n",
      "3141\n",
      "3142\n",
      "3143\n",
      "3144\n",
      "3145\n",
      "3146\n",
      "3147\n",
      "3148\n",
      "3149\n",
      "3150\n",
      "3151\n",
      "3152\n",
      "3153\n",
      "3154\n",
      "3155\n",
      "3156\n",
      "3157\n",
      "3158\n",
      "3159\n",
      "3160\n",
      "3161\n",
      "3162\n",
      "3163\n",
      "3164\n",
      "3165\n",
      "3166\n",
      "3167\n",
      "3168\n",
      "3169\n",
      "3170\n",
      "3171\n",
      "3172\n",
      "3173\n",
      "3174\n",
      "3175\n",
      "3176\n",
      "3177\n",
      "3178\n",
      "3179\n",
      "3180\n",
      "3181\n",
      "3182\n",
      "3183\n",
      "3184\n",
      "3185\n",
      "3186\n",
      "3187\n",
      "3188\n",
      "3189\n",
      "3190\n",
      "3191\n",
      "3192\n",
      "3193\n",
      "3194\n",
      "3195\n",
      "3196\n",
      "3197\n",
      "3198\n",
      "3199\n",
      "3200\n",
      "3201\n",
      "3202\n",
      "3203\n",
      "3204\n",
      "3205\n",
      "3206\n",
      "3207\n",
      "3208\n",
      "3209\n",
      "3210\n",
      "3211\n",
      "3212\n",
      "3213\n",
      "3214\n",
      "3215\n",
      "3216\n",
      "3217\n",
      "3218\n",
      "3219\n",
      "3220\n",
      "3221\n",
      "3222\n",
      "3223\n",
      "3224\n",
      "3225\n",
      "3226\n",
      "3227\n",
      "3228\n",
      "3229\n",
      "3230\n",
      "3231\n",
      "3232\n",
      "3233\n",
      "3234\n",
      "3235\n",
      "3236\n",
      "3237\n",
      "3238\n",
      "3239\n",
      "3240\n",
      "3241\n",
      "3242\n",
      "3243\n",
      "3244\n",
      "3245\n",
      "3246\n",
      "3247\n",
      "3248\n",
      "3249\n",
      "3250\n",
      "3251\n",
      "3252\n",
      "3253\n",
      "3254\n",
      "3255\n",
      "3256\n",
      "3257\n",
      "3258\n",
      "3259\n",
      "3260\n",
      "3261\n",
      "3262\n",
      "3263\n",
      "3264\n",
      "3265\n",
      "3266\n",
      "3267\n",
      "3268\n",
      "3269\n",
      "3270\n",
      "3271\n",
      "3272\n",
      "3273\n",
      "3274\n",
      "3275\n",
      "3276\n",
      "3277\n",
      "3278\n",
      "3279\n",
      "3280\n",
      "3281\n",
      "3282\n",
      "3283\n",
      "3284\n",
      "3285\n",
      "3286\n",
      "3287\n",
      "3288\n",
      "3289\n",
      "3290\n",
      "3291\n",
      "3292\n",
      "3293\n",
      "3294\n",
      "3295\n",
      "3296\n",
      "3297\n",
      "3298\n",
      "3299\n",
      "3300\n",
      "3301\n",
      "3302\n",
      "3303\n",
      "3304\n",
      "3305\n",
      "3306\n",
      "3307\n",
      "3308\n",
      "3309\n",
      "3310\n",
      "3311\n",
      "3312\n",
      "3313\n",
      "3314\n",
      "3315\n",
      "3316\n",
      "3317\n",
      "3318\n",
      "3319\n",
      "3320\n",
      "3321\n",
      "3322\n",
      "3323\n",
      "3324\n",
      "3325\n",
      "3326\n",
      "3327\n",
      "3328\n",
      "3329\n",
      "3330\n",
      "3331\n",
      "3332\n",
      "3333\n",
      "3334\n",
      "3335\n",
      "3336\n",
      "3337\n",
      "3338\n",
      "3339\n",
      "3340\n",
      "3341\n",
      "3342\n",
      "3343\n",
      "3344\n",
      "3345\n",
      "3346\n",
      "3347\n",
      "3348\n",
      "3349\n",
      "3350\n",
      "3351\n",
      "3352\n",
      "3353\n",
      "3354\n",
      "3355\n",
      "3356\n",
      "3357\n",
      "3358\n",
      "3359\n",
      "3360\n",
      "3361\n",
      "3362\n",
      "3363\n",
      "3364\n",
      "3365\n",
      "3366\n",
      "3367\n",
      "3368\n",
      "3369\n",
      "3370\n",
      "3371\n",
      "3372\n",
      "3373\n",
      "3374\n",
      "3375\n",
      "3376\n",
      "3377\n",
      "3378\n",
      "3379\n",
      "3380\n",
      "3381\n",
      "3382\n",
      "3383\n",
      "3384\n",
      "3385\n",
      "3386\n",
      "3387\n",
      "3388\n",
      "3389\n",
      "3390\n",
      "3391\n",
      "3392\n",
      "3393\n",
      "3394\n",
      "3395\n",
      "3396\n",
      "3397\n",
      "3398\n",
      "3399\n",
      "3400\n",
      "3401\n",
      "3402\n",
      "3403\n",
      "3404\n",
      "3405\n",
      "3406\n",
      "3407\n",
      "3408\n",
      "3409\n",
      "3410\n",
      "3411\n",
      "3412\n",
      "3413\n",
      "3414\n",
      "3415\n",
      "3416\n",
      "3417\n",
      "3418\n",
      "3419\n",
      "3420\n",
      "3421\n",
      "3422\n",
      "3423\n",
      "3424\n",
      "3425\n",
      "3426\n",
      "3427\n",
      "3428\n",
      "3429\n",
      "3430\n",
      "3431\n",
      "3432\n",
      "3433\n",
      "3434\n",
      "3435\n",
      "3436\n",
      "3437\n",
      "3438\n",
      "3439\n",
      "3440\n",
      "3441\n",
      "3442\n",
      "3443\n",
      "3444\n",
      "3445\n",
      "3446\n",
      "3447\n",
      "3448\n",
      "3449\n",
      "3450\n",
      "3451\n",
      "3452\n",
      "3453\n",
      "3454\n",
      "3455\n",
      "3456\n",
      "3457\n",
      "3458\n",
      "3459\n",
      "3460\n",
      "3461\n",
      "3462\n",
      "3463\n",
      "3464\n",
      "3465\n",
      "3466\n",
      "3467\n",
      "3468\n",
      "3469\n",
      "3470\n",
      "3471\n",
      "3472\n",
      "3473\n",
      "3474\n",
      "3475\n",
      "3476\n",
      "3477\n",
      "3478\n",
      "3479\n",
      "3480\n",
      "3481\n",
      "3482\n",
      "3483\n",
      "3484\n",
      "3485\n",
      "3486\n",
      "3487\n",
      "3488\n",
      "3489\n",
      "3490\n",
      "3491\n",
      "3492\n",
      "3493\n",
      "3494\n",
      "3495\n",
      "3496\n",
      "3497\n",
      "3498\n",
      "3499\n",
      "3500\n",
      "3501\n",
      "3502\n",
      "3503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3504\n",
      "3505\n",
      "3506\n",
      "3507\n",
      "3508\n",
      "3509\n",
      "3510\n",
      "3511\n",
      "3512\n",
      "3513\n",
      "3514\n",
      "3515\n",
      "3516\n",
      "3517\n",
      "3518\n",
      "3519\n",
      "3520\n",
      "3521\n",
      "3522\n",
      "3523\n",
      "3524\n",
      "3525\n",
      "3526\n",
      "3527\n",
      "3528\n",
      "3529\n",
      "3530\n",
      "3531\n",
      "3532\n",
      "3533\n",
      "3534\n",
      "3535\n",
      "3536\n",
      "3537\n",
      "3538\n",
      "3539\n",
      "3540\n",
      "3541\n",
      "3542\n",
      "3543\n",
      "3544\n",
      "3545\n",
      "3546\n",
      "3547\n",
      "3548\n",
      "3549\n",
      "3550\n",
      "3551\n",
      "3552\n",
      "3553\n",
      "3554\n",
      "3555\n",
      "3556\n",
      "3557\n",
      "3558\n",
      "3559\n",
      "3560\n",
      "3561\n",
      "3562\n",
      "3563\n",
      "3564\n",
      "3565\n",
      "3566\n",
      "3567\n",
      "3568\n",
      "3569\n",
      "3570\n",
      "3571\n",
      "3572\n",
      "3573\n",
      "3574\n",
      "3575\n",
      "3576\n",
      "3577\n",
      "3578\n",
      "3579\n",
      "3580\n",
      "3581\n",
      "3582\n",
      "3583\n",
      "3584\n",
      "3585\n",
      "3586\n",
      "3587\n",
      "3588\n",
      "3589\n",
      "3590\n",
      "3591\n",
      "3592\n",
      "3593\n",
      "3594\n",
      "3595\n",
      "3596\n",
      "3597\n",
      "3598\n",
      "3599\n",
      "3600\n",
      "3601\n",
      "3602\n",
      "3603\n",
      "3604\n",
      "3605\n",
      "3606\n",
      "3607\n",
      "3608\n",
      "3609\n",
      "3610\n",
      "3611\n",
      "3612\n",
      "3613\n",
      "3614\n",
      "3615\n",
      "3616\n",
      "3617\n",
      "3618\n",
      "3619\n",
      "3620\n",
      "3621\n",
      "3622\n",
      "3623\n",
      "3624\n",
      "3625\n",
      "3626\n",
      "3627\n",
      "3628\n",
      "3629\n",
      "3630\n",
      "3631\n",
      "3632\n",
      "3633\n",
      "3634\n",
      "3635\n",
      "3636\n",
      "3637\n",
      "3638\n",
      "3639\n",
      "3640\n",
      "3641\n",
      "3642\n",
      "3643\n",
      "3644\n",
      "3645\n",
      "3646\n",
      "3647\n",
      "3648\n",
      "3649\n",
      "3650\n",
      "3651\n",
      "3652\n",
      "3653\n",
      "3654\n",
      "3655\n",
      "3656\n",
      "3657\n",
      "3658\n",
      "3659\n",
      "3660\n",
      "3661\n",
      "3662\n",
      "3663\n",
      "3664\n",
      "3665\n",
      "3666\n",
      "3667\n",
      "3668\n",
      "3669\n",
      "3670\n",
      "3671\n",
      "3672\n",
      "3673\n",
      "3674\n",
      "3675\n",
      "3676\n",
      "3677\n",
      "3678\n",
      "3679\n",
      "3680\n",
      "3681\n",
      "3682\n",
      "3683\n",
      "3684\n",
      "3685\n",
      "3686\n",
      "3687\n",
      "3688\n",
      "3689\n",
      "3690\n",
      "3691\n",
      "3692\n",
      "3693\n",
      "3694\n",
      "3695\n",
      "3696\n",
      "3697\n",
      "3698\n",
      "3699\n",
      "3700\n",
      "3701\n",
      "3702\n",
      "3703\n",
      "3704\n",
      "3705\n",
      "3706\n",
      "3707\n",
      "3708\n",
      "3709\n",
      "3710\n",
      "3711\n",
      "3712\n",
      "3713\n",
      "3714\n",
      "3715\n",
      "3716\n",
      "3717\n",
      "3718\n",
      "3719\n",
      "3720\n",
      "3721\n",
      "3722\n",
      "3723\n",
      "3724\n",
      "3725\n",
      "3726\n",
      "3727\n",
      "3728\n",
      "3729\n",
      "3730\n",
      "3731\n",
      "3732\n",
      "3733\n",
      "3734\n",
      "3735\n",
      "3736\n",
      "3737\n",
      "3738\n",
      "3739\n",
      "3740\n",
      "3741\n",
      "3742\n",
      "3743\n",
      "3744\n",
      "3745\n",
      "3746\n",
      "3747\n",
      "3748\n",
      "3749\n",
      "3750\n",
      "3751\n",
      "3752\n",
      "3753\n",
      "3754\n",
      "3755\n",
      "3756\n",
      "3757\n",
      "3758\n",
      "3759\n",
      "3760\n",
      "3761\n",
      "3762\n",
      "3763\n",
      "3764\n",
      "3765\n",
      "3766\n",
      "3767\n",
      "3768\n",
      "3769\n",
      "3770\n",
      "3771\n",
      "3772\n",
      "3773\n",
      "3774\n",
      "3775\n",
      "3776\n",
      "3777\n",
      "3778\n",
      "3779\n",
      "3780\n",
      "3781\n",
      "3782\n",
      "3783\n",
      "3784\n",
      "3785\n",
      "3786\n",
      "3787\n",
      "3788\n",
      "3789\n",
      "3790\n",
      "3791\n",
      "3792\n",
      "3793\n",
      "3794\n",
      "3795\n",
      "3796\n",
      "3797\n",
      "3798\n",
      "3799\n",
      "3800\n",
      "3801\n",
      "3802\n",
      "3803\n",
      "3804\n",
      "3805\n",
      "3806\n",
      "3807\n",
      "3808\n",
      "3809\n",
      "3810\n",
      "3811\n",
      "3812\n",
      "3813\n",
      "3814\n",
      "3815\n",
      "3816\n",
      "3817\n",
      "3818\n",
      "3819\n",
      "3820\n",
      "3821\n",
      "3822\n",
      "3823\n",
      "3824\n",
      "3825\n",
      "3826\n",
      "3827\n",
      "3828\n",
      "3829\n",
      "3830\n",
      "3831\n",
      "3832\n",
      "3833\n",
      "3834\n",
      "3835\n",
      "3836\n",
      "3837\n",
      "3838\n",
      "3839\n",
      "3840\n",
      "3841\n",
      "3842\n",
      "3843\n",
      "3844\n",
      "3845\n",
      "3846\n",
      "3847\n",
      "3848\n",
      "3849\n",
      "3850\n",
      "3851\n",
      "3852\n",
      "3853\n",
      "3854\n",
      "3855\n",
      "3856\n",
      "3857\n",
      "3858\n",
      "3859\n",
      "3860\n",
      "3861\n",
      "3862\n",
      "3863\n",
      "3864\n",
      "3865\n",
      "3866\n",
      "3867\n",
      "3868\n",
      "3869\n",
      "3870\n",
      "3871\n",
      "3872\n",
      "3873\n",
      "3874\n",
      "3875\n",
      "3876\n",
      "3877\n",
      "3878\n",
      "3879\n",
      "3880\n",
      "3881\n",
      "3882\n",
      "3883\n",
      "3884\n",
      "3885\n",
      "3886\n",
      "3887\n",
      "3888\n",
      "3889\n",
      "3890\n",
      "3891\n",
      "3892\n",
      "3893\n",
      "3894\n",
      "3895\n",
      "3896\n",
      "3897\n",
      "3898\n",
      "3899\n",
      "3900\n",
      "3901\n",
      "3902\n",
      "3903\n",
      "3904\n",
      "3905\n",
      "3906\n",
      "3907\n",
      "3908\n",
      "3909\n",
      "3910\n",
      "3911\n",
      "3912\n",
      "3913\n",
      "3914\n",
      "3915\n",
      "3916\n",
      "3917\n",
      "3918\n",
      "3919\n",
      "3920\n",
      "3921\n",
      "3922\n",
      "3923\n",
      "3924\n",
      "3925\n",
      "3926\n",
      "3927\n",
      "3928\n",
      "3929\n",
      "3930\n",
      "3931\n",
      "3932\n",
      "3933\n",
      "3934\n",
      "3935\n",
      "3936\n",
      "3937\n",
      "3938\n",
      "3939\n",
      "3940\n",
      "3941\n",
      "3942\n",
      "3943\n",
      "3944\n",
      "3945\n",
      "3946\n",
      "3947\n",
      "3948\n",
      "3949\n",
      "3950\n",
      "3951\n",
      "3952\n",
      "3953\n",
      "3954\n",
      "3955\n",
      "3956\n",
      "3957\n",
      "3958\n",
      "3959\n",
      "3960\n",
      "3961\n",
      "3962\n",
      "3963\n",
      "3964\n",
      "3965\n",
      "3966\n",
      "3967\n",
      "3968\n",
      "3969\n",
      "3970\n",
      "3971\n",
      "3972\n",
      "3973\n",
      "3974\n",
      "3975\n",
      "3976\n",
      "3977\n",
      "3978\n",
      "3979\n",
      "3980\n",
      "3981\n",
      "3982\n",
      "3983\n",
      "3984\n",
      "3985\n",
      "3986\n",
      "3987\n",
      "3988\n",
      "3989\n",
      "3990\n",
      "3991\n",
      "3992\n",
      "3993\n",
      "3994\n",
      "3995\n",
      "3996\n",
      "3997\n",
      "3998\n",
      "3999\n",
      "4000\n",
      "4001\n",
      "4002\n",
      "4003\n",
      "4004\n",
      "4005\n",
      "4006\n",
      "4007\n",
      "4008\n",
      "4009\n",
      "4010\n",
      "4011\n",
      "4012\n",
      "4013\n",
      "4014\n",
      "4015\n",
      "4016\n",
      "4017\n",
      "4018\n",
      "4019\n",
      "4020\n",
      "4021\n",
      "4022\n",
      "4023\n",
      "4024\n",
      "4025\n",
      "4026\n",
      "4027\n",
      "4028\n",
      "4029\n",
      "4030\n",
      "4031\n",
      "4032\n",
      "4033\n",
      "4034\n",
      "4035\n",
      "4036\n",
      "4037\n",
      "4038\n",
      "4039\n",
      "4040\n",
      "4041\n",
      "4042\n",
      "4043\n",
      "4044\n",
      "4045\n",
      "4046\n",
      "4047\n",
      "4048\n",
      "4049\n",
      "4050\n",
      "4051\n",
      "4052\n",
      "4053\n",
      "4054\n",
      "4055\n",
      "4056\n",
      "4057\n",
      "4058\n",
      "4059\n",
      "4060\n",
      "4061\n",
      "4062\n",
      "4063\n",
      "4064\n",
      "4065\n",
      "4066\n",
      "4067\n",
      "4068\n",
      "4069\n",
      "4070\n",
      "4071\n",
      "4072\n",
      "4073\n",
      "4074\n",
      "4075\n",
      "4076\n",
      "4077\n",
      "4078\n",
      "4079\n",
      "4080\n",
      "4081\n",
      "4082\n",
      "4083\n",
      "4084\n",
      "4085\n",
      "4086\n",
      "4087\n",
      "4088\n",
      "4089\n",
      "4090\n",
      "4091\n",
      "4092\n",
      "4093\n",
      "4094\n",
      "4095\n",
      "4096\n",
      "4097\n",
      "4098\n",
      "4099\n",
      "4100\n",
      "4101\n",
      "4102\n",
      "4103\n",
      "4104\n",
      "4105\n",
      "4106\n",
      "4107\n",
      "4108\n",
      "4109\n",
      "4110\n",
      "4111\n",
      "4112\n",
      "4113\n",
      "4114\n",
      "4115\n",
      "4116\n",
      "4117\n",
      "4118\n",
      "4119\n",
      "4120\n",
      "4121\n",
      "4122\n",
      "4123\n",
      "4124\n",
      "4125\n",
      "4126\n",
      "4127\n",
      "4128\n",
      "4129\n",
      "4130\n",
      "4131\n",
      "4132\n",
      "4133\n",
      "4134\n",
      "4135\n",
      "4136\n",
      "4137\n",
      "4138\n",
      "4139\n",
      "4140\n",
      "4141\n",
      "4142\n",
      "4143\n",
      "4144\n",
      "4145\n",
      "4146\n",
      "4147\n",
      "4148\n",
      "4149\n",
      "4150\n",
      "4151\n",
      "4152\n",
      "4153\n",
      "4154\n",
      "4155\n",
      "4156\n",
      "4157\n",
      "4158\n",
      "4159\n",
      "4160\n",
      "4161\n",
      "4162\n",
      "4163\n",
      "4164\n",
      "4165\n",
      "4166\n",
      "4167\n",
      "4168\n",
      "4169\n",
      "4170\n",
      "4171\n",
      "4172\n",
      "4173\n",
      "4174\n",
      "4175\n",
      "4176\n",
      "4177\n",
      "4178\n",
      "4179\n",
      "4180\n",
      "4181\n",
      "4182\n",
      "4183\n",
      "4184\n",
      "4185\n",
      "4186\n",
      "4187\n",
      "4188\n",
      "4189\n",
      "4190\n",
      "4191\n",
      "4192\n",
      "4193\n",
      "4194\n",
      "4195\n",
      "4196\n",
      "4197\n",
      "4198\n",
      "4199\n",
      "4200\n",
      "4201\n",
      "4202\n",
      "4203\n",
      "4204\n",
      "4205\n",
      "4206\n",
      "4207\n",
      "4208\n",
      "4209\n",
      "4210\n",
      "4211\n",
      "4212\n",
      "4213\n",
      "4214\n",
      "4215\n",
      "4216\n",
      "4217\n",
      "4218\n",
      "4219\n",
      "4220\n",
      "4221\n",
      "4222\n",
      "4223\n",
      "4224\n",
      "4225\n",
      "4226\n",
      "4227\n",
      "4228\n",
      "4229\n",
      "4230\n",
      "4231\n",
      "4232\n",
      "4233\n",
      "4234\n",
      "4235\n",
      "4236\n",
      "4237\n",
      "4238\n",
      "4239\n",
      "4240\n",
      "4241\n",
      "4242\n",
      "4243\n",
      "4244\n",
      "4245\n",
      "4246\n",
      "4247\n",
      "4248\n",
      "4249\n",
      "4250\n",
      "4251\n",
      "4252\n",
      "4253\n",
      "4254\n",
      "4255\n",
      "4256\n",
      "4257\n",
      "4258\n",
      "4259\n",
      "4260\n",
      "4261\n",
      "4262\n",
      "4263\n",
      "4264\n",
      "4265\n",
      "4266\n",
      "4267\n",
      "4268\n",
      "4269\n",
      "4270\n",
      "4271\n",
      "4272\n",
      "4273\n",
      "4274\n",
      "4275\n",
      "4276\n",
      "4277\n",
      "4278\n",
      "4279\n",
      "4280\n",
      "4281\n",
      "4282\n",
      "4283\n",
      "4284\n",
      "4285\n",
      "4286\n",
      "4287\n",
      "4288\n",
      "4289\n",
      "4290\n",
      "4291\n",
      "4292\n",
      "4293\n",
      "4294\n",
      "4295\n",
      "4296\n",
      "4297\n",
      "4298\n",
      "4299\n",
      "4300\n",
      "4301\n",
      "4302\n",
      "4303\n",
      "4304\n",
      "4305\n",
      "4306\n",
      "4307\n",
      "4308\n",
      "4309\n",
      "4310\n",
      "4311\n",
      "4312\n",
      "4313\n",
      "4314\n",
      "4315\n",
      "4316\n",
      "4317\n",
      "4318\n",
      "4319\n",
      "4320\n",
      "4321\n",
      "4322\n",
      "4323\n",
      "4324\n",
      "4325\n",
      "4326\n",
      "4327\n",
      "4328\n",
      "4329\n",
      "4330\n",
      "4331\n",
      "4332\n",
      "4333\n",
      "4334\n",
      "4335\n",
      "4336\n",
      "4337\n",
      "4338\n",
      "4339\n",
      "4340\n",
      "4341\n",
      "4342\n",
      "4343\n",
      "4344\n",
      "4345\n",
      "4346\n",
      "4347\n",
      "4348\n",
      "4349\n",
      "4350\n",
      "4351\n",
      "4352\n",
      "4353\n",
      "4354\n",
      "4355\n",
      "4356\n",
      "4357\n",
      "4358\n",
      "4359\n",
      "4360\n",
      "4361\n",
      "4362\n",
      "4363\n",
      "4364\n",
      "4365\n",
      "4366\n",
      "4367\n",
      "4368\n",
      "4369\n",
      "4370\n",
      "4371\n",
      "4372\n",
      "4373\n",
      "4374\n",
      "4375\n",
      "4376\n",
      "4377\n",
      "4378\n",
      "4379\n",
      "4380\n",
      "4381\n",
      "4382\n",
      "4383\n",
      "4384\n",
      "4385\n",
      "4386\n",
      "4387\n",
      "4388\n",
      "4389\n",
      "4390\n",
      "4391\n",
      "4392\n",
      "4393\n",
      "4394\n",
      "4395\n",
      "4396\n",
      "4397\n",
      "4398\n",
      "4399\n",
      "4400\n",
      "4401\n",
      "4402\n",
      "4403\n",
      "4404\n",
      "4405\n",
      "4406\n",
      "4407\n",
      "4408\n",
      "4409\n",
      "4410\n",
      "4411\n",
      "4412\n",
      "4413\n",
      "4414\n",
      "4415\n",
      "4416\n",
      "4417\n",
      "4418\n",
      "4419\n",
      "4420\n",
      "4421\n",
      "4422\n",
      "4423\n",
      "4424\n",
      "4425\n",
      "4426\n",
      "4427\n",
      "4428\n",
      "4429\n",
      "4430\n",
      "4431\n",
      "4432\n",
      "4433\n",
      "4434\n",
      "4435\n",
      "4436\n",
      "4437\n",
      "4438\n",
      "4439\n",
      "4440\n",
      "4441\n",
      "4442\n",
      "4443\n",
      "4444\n",
      "4445\n",
      "4446\n",
      "4447\n",
      "4448\n",
      "4449\n",
      "4450\n",
      "4451\n",
      "4452\n",
      "4453\n",
      "4454\n",
      "4455\n",
      "4456\n",
      "4457\n",
      "4458\n",
      "4459\n",
      "4460\n",
      "4461\n",
      "4462\n",
      "4463\n",
      "4464\n",
      "4465\n",
      "4466\n",
      "4467\n",
      "4468\n",
      "4469\n",
      "4470\n",
      "4471\n",
      "4472\n",
      "4473\n",
      "4474\n",
      "4475\n",
      "4476\n",
      "4477\n",
      "4478\n",
      "4479\n",
      "4480\n",
      "4481\n",
      "4482\n",
      "4483\n",
      "4484\n",
      "4485\n",
      "4486\n",
      "4487\n",
      "4488\n",
      "4489\n",
      "4490\n",
      "4491\n",
      "4492\n",
      "4493\n",
      "4494\n",
      "4495\n",
      "4496\n",
      "4497\n",
      "4498\n",
      "4499\n",
      "4500\n",
      "4501\n",
      "4502\n",
      "4503\n",
      "4504\n",
      "4505\n",
      "4506\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "GB = GradientBoostingRegressor()\n",
    "\n",
    "\n",
    "ScoresGB = []\n",
    "for i in np.arange(6, NormX.shape[1]):\n",
    "\n",
    "    x = NormX.iloc[:,[0, 1, 2, 3, 4, 5, 6, i]]\n",
    "    print(i)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "\n",
    "    GB.fit(x_train, y_train)\n",
    "    ScoresGB.append((mean_absolute_error(GB.predict(x_test), y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3392</th>\n",
       "      <td>0.042524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3074</th>\n",
       "      <td>0.043666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2789</th>\n",
       "      <td>0.044511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2226</th>\n",
       "      <td>0.044931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1273</th>\n",
       "      <td>0.044987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2502</th>\n",
       "      <td>0.086434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3039</th>\n",
       "      <td>0.086521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2356</th>\n",
       "      <td>0.088221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>0.088257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3919</th>\n",
       "      <td>0.090205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4501 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0\n",
       "3392  0.042524\n",
       "3074  0.043666\n",
       "2789  0.044511\n",
       "2226  0.044931\n",
       "1273  0.044987\n",
       "...        ...\n",
       "2502  0.086434\n",
       "3039  0.086521\n",
       "2356  0.088221\n",
       "1007  0.088257\n",
       "3919  0.090205\n",
       "\n",
       "[4501 rows x 1 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(ScoresGB).sort_values(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>r</th>\n",
       "      <td>0.110375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Area</th>\n",
       "      <td>0.109346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>In</th>\n",
       "      <td>0.002067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z</th>\n",
       "      <td>0.092571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tplus</th>\n",
       "      <td>0.051950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tminus</th>\n",
       "      <td>0.032352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tminus_</th>\n",
       "      <td>0.241119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analytic 0.4999000000000129</th>\n",
       "      <td>0.360220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    0\n",
       "r                            0.110375\n",
       "Area                         0.109346\n",
       "In                           0.002067\n",
       "Z                            0.092571\n",
       "tplus                        0.051950\n",
       "tminus                       0.032352\n",
       "tminus_                      0.241119\n",
       "analytic 0.4999000000000129  0.360220"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(GB.feature_importances_, index=x_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3392"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(pd.DataFrame(ScoresGB).sort_values(0).iloc[[0],[0]].index.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x1 = NormX.iloc[:,[0, 1, 2, 3, 4, 5, 6, int(pd.DataFrame(ScoresRF).sort_values(0).iloc[[0],[0]].index.to_numpy())]]\n",
    "x2 = NormX.iloc[:,[0, 1, 2, 3, 4, 5, 6, int(pd.DataFrame(ScoresGB).sort_values(0).iloc[[0],[0]].index.to_numpy())]]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x1, y, random_state=15)\n",
    "\n",
    "RF.fit(x_train, y_train)\n",
    "RFC = RF.predict(x_test)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x2, y, random_state=15)\n",
    "\n",
    "GB.fit(x_train, y_train)\n",
    "GBC = GB.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "variable=Analytic Function<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "Analytic Function",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "mode": "lines",
         "name": "Analytic Function",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          2,
          8,
          10,
          15,
          23,
          24,
          28,
          38,
          42,
          43,
          44,
          47,
          51,
          64,
          80,
          84,
          87,
          90,
          92,
          94,
          99,
          100,
          103,
          109,
          117,
          120,
          128,
          136,
          146,
          148,
          159,
          164,
          170,
          175,
          177,
          179,
          184,
          187,
          189,
          194,
          200,
          204,
          208,
          209,
          218,
          222,
          225,
          226,
          227,
          228,
          230,
          233,
          238,
          239,
          242,
          243,
          249,
          251,
          252,
          256,
          257,
          262,
          263,
          267,
          270,
          272,
          273,
          274,
          276,
          277,
          285,
          289,
          290,
          291,
          293,
          295,
          302,
          303,
          306,
          307,
          309,
          310,
          313,
          316,
          328,
          335,
          346,
          349,
          350,
          361,
          367,
          376,
          380,
          396,
          400,
          405,
          408,
          411,
          412,
          415,
          423,
          424,
          430,
          431,
          432,
          443,
          446,
          449,
          451,
          452,
          453,
          456,
          465,
          471,
          482,
          490,
          498,
          512,
          513,
          514,
          516,
          518,
          520,
          532,
          550,
          552,
          557,
          574,
          579,
          583,
          589,
          591,
          607,
          612,
          616,
          620,
          626,
          629,
          642,
          643,
          644,
          652,
          657,
          659,
          665,
          669,
          670,
          680,
          683,
          686,
          688,
          696,
          700,
          702,
          708,
          709,
          710,
          712,
          714,
          720,
          722,
          725,
          727,
          733,
          740,
          741,
          744,
          745,
          746,
          756,
          765,
          766,
          767,
          770,
          771,
          773,
          777,
          780,
          782,
          783,
          785,
          789,
          791,
          793,
          794,
          801,
          805,
          808,
          811,
          817,
          827,
          831,
          832,
          834,
          842,
          843,
          849,
          858,
          862,
          865,
          867,
          870,
          871,
          879,
          883,
          888,
          889,
          890,
          893,
          896,
          897,
          899,
          904,
          906,
          911,
          912,
          913,
          914,
          917,
          919,
          922,
          925,
          926,
          942,
          944,
          947,
          957,
          958,
          959,
          963,
          969,
          971,
          972,
          979,
          983,
          986,
          995,
          996,
          998,
          1001,
          1005,
          1007,
          1009,
          1016,
          1019,
          1023,
          1027,
          1029,
          1033,
          1036,
          1041,
          1042,
          1046,
          1049,
          1052,
          1054,
          1061,
          1065,
          1067,
          1074,
          1078,
          1082,
          1084,
          1087,
          1088,
          1104,
          1106,
          1107,
          1108,
          1114,
          1116,
          1119,
          1123,
          1126,
          1127,
          1138,
          1140,
          1149,
          1160,
          1161,
          1162,
          1171,
          1173,
          1179,
          1180,
          1182,
          1183,
          1185,
          1189,
          1196,
          1199,
          1204,
          1209,
          1215,
          1220,
          1224,
          1228,
          1229,
          1234,
          1235,
          1243,
          1254,
          1255,
          1258,
          1262,
          1264,
          1273,
          1274,
          1278,
          1290,
          1295,
          1298,
          1302,
          1304,
          1316,
          1318,
          1327,
          1328,
          1331,
          1334,
          1339,
          1344,
          1345,
          1346,
          1347,
          1353,
          1354,
          1358,
          1360,
          1362,
          1363,
          1367,
          1376,
          1385,
          1394,
          1396,
          1404,
          1405,
          1407,
          1415,
          1423,
          1424,
          1426,
          1432,
          1434,
          1438,
          1439,
          1441,
          1446,
          1452,
          1468,
          1470,
          1479,
          1482,
          1491,
          1492,
          1493,
          1502,
          1508,
          1511,
          1520,
          1522,
          1524,
          1528,
          1536,
          1537,
          1544,
          1545,
          1546,
          1551,
          1552,
          1557,
          1560,
          1561,
          1562,
          1563,
          1565,
          1567,
          1572,
          1576,
          1583,
          1588,
          1594,
          1597,
          1606,
          1609,
          1611,
          1616,
          1617,
          1629,
          1630,
          1631,
          1633,
          1635,
          1650,
          1652,
          1656,
          1657,
          1660,
          1668,
          1677,
          1678,
          1681,
          1692,
          1695,
          1697,
          1701,
          1704,
          1705,
          1706,
          1707,
          1710,
          1711,
          1712,
          1713,
          1714,
          1716,
          1717,
          1720,
          1721,
          1722,
          1723,
          1733,
          1737,
          1740,
          1741,
          1744,
          1745,
          1750,
          1752,
          1753,
          1756,
          1760,
          1769,
          1771,
          1772,
          1774,
          1780,
          1784,
          1785,
          1786,
          1792,
          1795,
          1797,
          1799,
          1800,
          1807,
          1808,
          1810,
          1814,
          1820,
          1821,
          1822,
          1827,
          1830,
          1831,
          1834,
          1835,
          1836,
          1839,
          1844,
          1846,
          1851,
          1852,
          1857,
          1861,
          1864
         ],
         "xaxis": "x",
         "y": [
          1.1646338169234316e-05,
          2.1273419643883117e-05,
          2.4482446802099374e-05,
          3.250501469764006e-05,
          3.9725325803626675e-05,
          3.972532580362666e-05,
          3.41095282767482e-05,
          0.0008181212434996286,
          0.0008437934607653589,
          0.0008437934607653587,
          0.0008437934607653589,
          0.0008437934607653589,
          0.0008437934607653589,
          0.0008437934607653589,
          0.0008437934607653589,
          0.0008437934607653587,
          0.013708883620153071,
          0.013708883620153075,
          0.013708883620153075,
          0.013708883620153071,
          0.013708883620153075,
          0.013708883620153075,
          0.013708883620153071,
          0.013708883620153075,
          0.013708883620153075,
          0.013708883620153075,
          0.013708883620153075,
          0.013708883620153075,
          0.013708883620153075,
          0.013708883620153075,
          0.013708883620153075,
          0.013708883620153075,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          0.21955032617035652,
          6.132330664548191e-05,
          6.555393163809458e-05,
          8.177132744310972e-05,
          7.824580661593252e-05,
          6.273351497635282e-05,
          0.0015165294869970872,
          0.0015165294869970876,
          0.0015165294869970876,
          0.0015165294869970876,
          0.0015165294869970876,
          0.0015165294869970876,
          0.0015165294869970876,
          0.024472660039860733,
          0.024472660039860733,
          0.024472660039860733,
          0.024472660039860733,
          0.024472660039860733,
          0.024472660039860733,
          0.024472660039860733,
          0.024472660039860733,
          0.024472660039860733,
          0.024472660039860733,
          0.024472660039860733,
          0.024472660039860733,
          0.024472660039860733,
          0.024472660039860733,
          0.024472660039860733,
          0.024472660039860733,
          0.024472660039860733,
          0.024472660039860733,
          0.024472660039860733,
          0.024472660039860733,
          0.39177074888567903,
          0.39177074888567903,
          0.39177074888567903,
          0.39177074888567903,
          0.39177074888567903,
          0.39177074888567903,
          0.39177074888567903,
          0.39177074888567903,
          0.39177074888567903,
          0.39177074888567903,
          0.39177074888567903,
          0.39177074888567903,
          0.39177074888567903,
          0.39177074888567903,
          0.39177074888567903,
          0.39177074888567903,
          0.39177074888567903,
          0.39177074888567903,
          0.39177074888567903,
          0.39177074888567903,
          0.39177074888567903,
          0.39177074888567903,
          5.68801550890094e-05,
          6.813415649991788e-05,
          0.00011425349561520962,
          0.00011425349561520962,
          0.00011425349561520962,
          0.00010939882833991571,
          9.06421593217349e-05,
          8.313949171446255e-05,
          0.002036244177750685,
          0.002036244177750685,
          0.0020362441777506857,
          0.0020362441777506857,
          0.0020362441777506857,
          0.002036244177750685,
          0.0020362441777506857,
          0.0020362441777506857,
          0.002036244177750685,
          0.002036244177750685,
          0.002036244177750685,
          0.002036244177750685,
          0.002036244177750685,
          0.0020362441777506857,
          0.0020362441777506857,
          0.0327880950919183,
          0.03278809509191829,
          0.0327880950919183,
          0.0327880950919183,
          0.03278809509191829,
          0.03278809509191829,
          0.0327880950919183,
          0.0327880950919183,
          0.0327880950919183,
          0.0327880950919183,
          0.0327880950919183,
          0.0327880950919183,
          0.0327880950919183,
          0.0327880950919183,
          0.0327880950919183,
          0.0327880950919183,
          0.0327880950919183,
          0.0327880950919183,
          0.03278809509191829,
          0.0327880950919183,
          0.0327880950919183,
          0.0327880950919183,
          0.0327880950919183,
          0.0327880950919183,
          0.0327880950919183,
          0.03278809509191829,
          0.5248177097186,
          0.5248177097186002,
          0.5248177097186,
          0.5248177097186002,
          0.5248177097186002,
          0.5248177097186002,
          0.5248177097186002,
          0.5248177097186002,
          0.5248177097186002,
          0.5248177097186002,
          0.5248177097186002,
          0.5248177097186002,
          0.5248177097186002,
          0.5248177097186002,
          0.5248177097186002,
          0.5248177097186002,
          0.5248177097186002,
          0.5248177097186002,
          0.5248177097186002,
          0.5248177097186002,
          0.5248177097186002,
          0.5248177097186002,
          0.5248177097186002,
          0.5248177097186002,
          0.5248177097186002,
          0.5248177097186002,
          0.5248177097186002,
          0.5248177097186002,
          0.5248177097186002,
          0.5248177097186002,
          0.5248177097186002,
          0.5248177097186002,
          0.5248177097186002,
          0.5248177097186002,
          0.5248177097186002,
          0.5248177097186002,
          0.5248177097186002,
          0.5248177097186002,
          0.5248177097186002,
          0.5248177097186,
          0.5248177097186002,
          0.0002141849905855906,
          0.000217929603027419,
          0.0002216742154692474,
          0.00023026479695344192,
          0.00023026479695344192,
          0.0002291634403529042,
          0.00022541882791107578,
          0.00019920654081827697,
          0.0001842280910509633,
          0.003892424999162403,
          0.003892424999162403,
          0.003892424999162403,
          0.003892424999162403,
          0.003892424999162403,
          0.003892424999162403,
          0.003892424999162403,
          0.003892424999162403,
          0.003892424999162403,
          0.003892424999162403,
          0.003892424999162403,
          0.003892424999162403,
          0.003892424999162403,
          0.003892424999162403,
          0.062486988234505776,
          0.062486988234505776,
          0.062486988234505776,
          0.062486988234505776,
          0.062486988234505776,
          0.062486988234505776,
          0.062486988234505776,
          0.062486988234505776,
          0.062486988234505776,
          0.062486988234505776,
          0.062486988234505776,
          0.062486988234505776,
          0.062486988234505776,
          0.062486988234505776,
          0.062486988234505776,
          0.062486988234505776,
          0.062486988234505776,
          0.062486988234505776,
          0.062486988234505776,
          0.062486988234505776,
          0.062486988234505776,
          0.062486988234505776,
          0.9999999999999998,
          0.9999999999999998,
          0.9999999999999998,
          0.9999999999999998,
          0.9999999999999998,
          0.9999999999999998,
          0.9999999999999998,
          0.9999999999999998,
          0.9999999999999998,
          0.9999999999999998,
          0.9999999999999998,
          0.9999999999999998,
          0.9999999999999998,
          0.9999999999999998,
          0.9999999999999998,
          0.9999999999999998,
          0.9999999999999998,
          0.9999999999999998,
          0.9999999999999998,
          0.9999999999999998,
          0.9999999999999998,
          0.9999999999999998,
          0.9999999999999998,
          0.9999999999999998,
          0.9999999999999998,
          0.9999999999999998,
          0.9999999999999998,
          0.9999999999999998,
          0.9999999999999998,
          0.9999999999999998,
          0.9999999999999998,
          0.9999999999999998,
          0.9999999999999998,
          0.9999999999999998,
          0.9999999999999998,
          0,
          8.294641427834098e-07,
          4.147320713917071e-06,
          1.2550153116896873e-05,
          8.29464142783414e-06,
          5.806248999483897e-06,
          2.488392428350241e-06,
          0.0003807167896062822,
          0.000408990697777682,
          0.000408990697777682,
          0.000408990697777682,
          0.000408990697777682,
          0.000408990697777682,
          0.000408990697777682,
          0.000408990697777682,
          0.000408990697777682,
          0.000408990697777682,
          0.000408990697777682,
          0.000408990697777682,
          0.006752039412350245,
          0.006752039412350245,
          0.006752039412350245,
          0.006752039412350245,
          0.006752039412350245,
          0.006752039412350245,
          0.006752039412350245,
          0.006752039412350245,
          0.006752039412350245,
          0.006752039412350245,
          0.006752039412350245,
          0.006752039412350245,
          0.006752039412350245,
          0.006752039412350245,
          0.006752039412350245,
          0.006752039412350245,
          0.006752039412350245,
          0.006752039412350245,
          0.10824081884551125,
          0.10824081884551125,
          0.10824081884551125,
          0.10824081884551125,
          0.10824081884551125,
          0.10824081884551125,
          0.10824081884551125,
          0.10824081884551125,
          0.10824081884551125,
          0.10824081884551125,
          0.10824081884551125,
          0.10824081884551125,
          0.10824081884551125,
          0.10824081884551125,
          0.10824081884551125,
          0.10824081884551125,
          0.10824081884551125,
          0.10824081884551125,
          0.10824081884551125,
          0.10824081884551125,
          0.10824081884551125,
          0.10824081884551125,
          0.10824081884551125,
          0.10824081884551125,
          0.10824081884551125,
          0.10824081884551125,
          0.10824081884551125,
          0.10824081884551125,
          0.10824081884551125,
          0.10824081884551125,
          0.10824081884551125,
          0.10824081884551125,
          0.10824081884551125,
          0.10824081884551125,
          2.185727971539698e-05,
          2.5239824029556524e-05,
          2.7945859480884166e-05,
          3.1416644081500054e-05,
          2.7945859480884166e-05,
          2.3886806303892714e-05,
          2.185727971539698e-05,
          0.0007108545532113329,
          0.0007108545532113329,
          0.0007108545532113329,
          0.000710854553211333,
          0.0007108545532113329,
          0.000710854553211333,
          0.0007108545532113329,
          0.0007108545532113329,
          0.0007108545532113329,
          0.0007108545532113329,
          0.011581861099288658,
          0.01158186109928866,
          0.011581861099288658,
          0.01158186109928866,
          0.01158186109928866,
          0.011581861099288658,
          0.01158186109928866,
          0.01158186109928866,
          0.011581861099288658,
          0.01158186109928866,
          0.01158186109928866,
          0.01158186109928866,
          0.01158186109928866,
          0.01158186109928866,
          0.01158186109928866,
          0.01158186109928866,
          0.01158186109928866,
          0.011581861099288658,
          0.011581861099288658,
          0.011581861099288658,
          0.011581861099288658,
          0.01158186109928866,
          0.011581861099288658,
          0.01158186109928866,
          0.011581861099288658,
          0.011581861099288658,
          0.18551796583652588,
          0.18551796583652586,
          0.18551796583652586,
          0.18551796583652586,
          0.18551796583652586,
          0.18551796583652586,
          0.18551796583652588,
          0.18551796583652586,
          0.18551796583652588,
          0.18551796583652586,
          0.18551796583652586,
          0.18551796583652586,
          0.18551796583652586,
          0.18551796583652586,
          0.18551796583652588,
          0.18551796583652586,
          0.18551796583652586,
          0.18551796583652586,
          0.18551796583652586,
          0.18551796583652586,
          0.18551796583652586,
          0.18551796583652586,
          0.18551796583652586,
          0.18551796583652586,
          0.18551796583652586,
          0.18551796583652586,
          0.18551796583652586,
          0.18551796583652586,
          0.18551796583652586,
          0.18551796583652586,
          0.18551796583652588,
          0.18551796583652586,
          0.18551796583652586,
          0.18551796583652586,
          0.18551796583652586,
          0.18551796583652586,
          0.18551796583652586,
          0.18551796583652586,
          0.18551796583652588,
          0.18551796583652586,
          0.18551796583652588,
          0.18551796583652586,
          0.18551796583652586,
          0.18551796583652588,
          0.18551796583652588,
          0.18551796583652586,
          0.18551796583652588
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=Signal<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "Signal",
         "line": {
          "color": "#EF553B",
          "dash": "solid"
         },
         "mode": "lines",
         "name": "Signal",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          2,
          8,
          10,
          15,
          23,
          24,
          28,
          38,
          42,
          43,
          44,
          47,
          51,
          64,
          80,
          84,
          87,
          90,
          92,
          94,
          99,
          100,
          103,
          109,
          117,
          120,
          128,
          136,
          146,
          148,
          159,
          164,
          170,
          175,
          177,
          179,
          184,
          187,
          189,
          194,
          200,
          204,
          208,
          209,
          218,
          222,
          225,
          226,
          227,
          228,
          230,
          233,
          238,
          239,
          242,
          243,
          249,
          251,
          252,
          256,
          257,
          262,
          263,
          267,
          270,
          272,
          273,
          274,
          276,
          277,
          285,
          289,
          290,
          291,
          293,
          295,
          302,
          303,
          306,
          307,
          309,
          310,
          313,
          316,
          328,
          335,
          346,
          349,
          350,
          361,
          367,
          376,
          380,
          396,
          400,
          405,
          408,
          411,
          412,
          415,
          423,
          424,
          430,
          431,
          432,
          443,
          446,
          449,
          451,
          452,
          453,
          456,
          465,
          471,
          482,
          490,
          498,
          512,
          513,
          514,
          516,
          518,
          520,
          532,
          550,
          552,
          557,
          574,
          579,
          583,
          589,
          591,
          607,
          612,
          616,
          620,
          626,
          629,
          642,
          643,
          644,
          652,
          657,
          659,
          665,
          669,
          670,
          680,
          683,
          686,
          688,
          696,
          700,
          702,
          708,
          709,
          710,
          712,
          714,
          720,
          722,
          725,
          727,
          733,
          740,
          741,
          744,
          745,
          746,
          756,
          765,
          766,
          767,
          770,
          771,
          773,
          777,
          780,
          782,
          783,
          785,
          789,
          791,
          793,
          794,
          801,
          805,
          808,
          811,
          817,
          827,
          831,
          832,
          834,
          842,
          843,
          849,
          858,
          862,
          865,
          867,
          870,
          871,
          879,
          883,
          888,
          889,
          890,
          893,
          896,
          897,
          899,
          904,
          906,
          911,
          912,
          913,
          914,
          917,
          919,
          922,
          925,
          926,
          942,
          944,
          947,
          957,
          958,
          959,
          963,
          969,
          971,
          972,
          979,
          983,
          986,
          995,
          996,
          998,
          1001,
          1005,
          1007,
          1009,
          1016,
          1019,
          1023,
          1027,
          1029,
          1033,
          1036,
          1041,
          1042,
          1046,
          1049,
          1052,
          1054,
          1061,
          1065,
          1067,
          1074,
          1078,
          1082,
          1084,
          1087,
          1088,
          1104,
          1106,
          1107,
          1108,
          1114,
          1116,
          1119,
          1123,
          1126,
          1127,
          1138,
          1140,
          1149,
          1160,
          1161,
          1162,
          1171,
          1173,
          1179,
          1180,
          1182,
          1183,
          1185,
          1189,
          1196,
          1199,
          1204,
          1209,
          1215,
          1220,
          1224,
          1228,
          1229,
          1234,
          1235,
          1243,
          1254,
          1255,
          1258,
          1262,
          1264,
          1273,
          1274,
          1278,
          1290,
          1295,
          1298,
          1302,
          1304,
          1316,
          1318,
          1327,
          1328,
          1331,
          1334,
          1339,
          1344,
          1345,
          1346,
          1347,
          1353,
          1354,
          1358,
          1360,
          1362,
          1363,
          1367,
          1376,
          1385,
          1394,
          1396,
          1404,
          1405,
          1407,
          1415,
          1423,
          1424,
          1426,
          1432,
          1434,
          1438,
          1439,
          1441,
          1446,
          1452,
          1468,
          1470,
          1479,
          1482,
          1491,
          1492,
          1493,
          1502,
          1508,
          1511,
          1520,
          1522,
          1524,
          1528,
          1536,
          1537,
          1544,
          1545,
          1546,
          1551,
          1552,
          1557,
          1560,
          1561,
          1562,
          1563,
          1565,
          1567,
          1572,
          1576,
          1583,
          1588,
          1594,
          1597,
          1606,
          1609,
          1611,
          1616,
          1617,
          1629,
          1630,
          1631,
          1633,
          1635,
          1650,
          1652,
          1656,
          1657,
          1660,
          1668,
          1677,
          1678,
          1681,
          1692,
          1695,
          1697,
          1701,
          1704,
          1705,
          1706,
          1707,
          1710,
          1711,
          1712,
          1713,
          1714,
          1716,
          1717,
          1720,
          1721,
          1722,
          1723,
          1733,
          1737,
          1740,
          1741,
          1744,
          1745,
          1750,
          1752,
          1753,
          1756,
          1760,
          1769,
          1771,
          1772,
          1774,
          1780,
          1784,
          1785,
          1786,
          1792,
          1795,
          1797,
          1799,
          1800,
          1807,
          1808,
          1810,
          1814,
          1820,
          1821,
          1822,
          1827,
          1830,
          1831,
          1834,
          1835,
          1836,
          1839,
          1844,
          1846,
          1851,
          1852,
          1857,
          1861,
          1864
         ],
         "xaxis": "x",
         "y": [
          0.26878966550807026,
          0.6905562669743748,
          0.7176308346115418,
          0.7191224687952942,
          0.7210362635970898,
          0.7181655713943964,
          0.707330115531289,
          0,
          0.1357358963173522,
          0.1575081265918973,
          0.16595133895276024,
          0.1690471834850766,
          0.1695256321855255,
          0.16792142183696154,
          0.16814657416658454,
          0.10203622138102808,
          0.009808198359202408,
          0.09036770189831556,
          0.11032745591939544,
          0.1127084418051588,
          0.11396085163868677,
          0.113659710397816,
          0.11473199836764561,
          0.11432390977020387,
          0.11138004306038304,
          0.11040344483064321,
          0.1082447968703826,
          0.11075243094155893,
          0.1141437879065055,
          0.11444211474325597,
          0.11292233651830061,
          0.0634394832754035,
          0.07168850175196653,
          0.07995440665325132,
          0.08030902157240755,
          0.08014578613343087,
          0.08039345369601622,
          0.08039345369601622,
          0.08036530965481331,
          0.07978554240603405,
          0.07886804666282032,
          0.07878361453921168,
          0.07831360905112364,
          0.07808845672150064,
          0.0768416756962132,
          0.07659400813362788,
          0.07602831290545006,
          0.07528249581357385,
          0.07481249032548581,
          0.07450290587225417,
          0.07329834090877108,
          0.07293528277725397,
          0.07184329397858236,
          0.07134233004517115,
          0.07090609740652656,
          0.07108059046198442,
          0.07295498360609598,
          0.07296905562669742,
          0.07318013593571901,
          0.07498416897682339,
          0.07512207477871746,
          0.07659400813362788,
          0.07673191393552198,
          0.0773426396296244,
          0.07770288335702119,
          0.0778436035630356,
          0.07787456200835877,
          0.07811941516682377,
          0.07826013537283819,
          0.07867385277852046,
          0.07931553691794602,
          0.08003320996861937,
          0.07997692188621364,
          0.08014578613343087,
          0.08033998001773074,
          0.08044974177842196,
          0.08045255618254224,
          0.08058764758031603,
          0.07997692188621364,
          0.07912134303364618,
          0.07182640755386066,
          0.058750686011004305,
          0.04329116417826434,
          0.3138482754738753,
          0.680255547894122,
          0.6823944950255406,
          0.13227980805763898,
          0.04190366294696252,
          0.07775917143942698,
          0.18390723724019528,
          0.18401981340500675,
          0.18396352532260105,
          0.18475155847628158,
          0.00633803807888772,
          0.09822270379803832,
          0.1257560193068122,
          0.12663692779646227,
          0.12739400250481966,
          0.12768670053332956,
          0.12768670053332956,
          0.12522691133219818,
          0.12483008035123758,
          0.12348479518174013,
          0.12268269000745816,
          0.12273053487750299,
          0.12338347663340979,
          0.12372401953196457,
          0.12523254014043875,
          0.12634422976795237,
          0.12657782530993625,
          0.12698309950325762,
          0.12750939307375145,
          0.12593332676639038,
          0.1117768740413436,
          0.08979637786189718,
          0.09148783473819005,
          0.09137244416925827,
          0.08963595682704076,
          0.08971476014240884,
          0.08942206211389894,
          0.08905337517414125,
          0.08881977963215734,
          0.08878037797447333,
          0.08702700420753415,
          0.0818710158591672,
          0.08230161968957123,
          0.08374540900327879,
          0.08837228937703165,
          0.08874097631678934,
          0.08913499289362958,
          0.08969787371768712,
          0.09002997340388105,
          0.09122328075088298,
          0.09045494842604448,
          0.08829067165754328,
          0.03766798474592964,
          0.4101008963877123,
          0.6579091791790382,
          0.8839339740793378,
          0.8847220072330185,
          0.883033364760846,
          0.854326442733912,
          0.5307825450656458,
          0.3474522606701096,
          0.11846108382702672,
          0.27045016393904,
          0.2832838467275516,
          0.28992584045143044,
          0.2896444000394016,
          0.2888000788033154,
          0.2884623503088808,
          0.29017913682225627,
          0.29082644976992245,
          0.2892222394213585,
          0.24495166260923404,
          0.20608474170806182,
          0.16583876278794865,
          0.08573800712044241,
          0.00029551243263017946,
          0.1772370994751136,
          0.20740751164459706,
          0.21095366083615943,
          0.2119949903606659,
          0.21328961625599815,
          0.2127830235143464,
          0.21255787118472338,
          0.21067222042413067,
          0.2103344919296962,
          0.20957460281721849,
          0.20597216554325035,
          0.20791410438624877,
          0.2080266805510603,
          0.20808296863346606,
          0.20903986603436384,
          0.20974346706443578,
          0.2110099489185652,
          0.21289559967915786,
          0.21354291262682404,
          0.2138524970800557,
          0.2140495053684758,
          0.21343033646201257,
          0.2112351012481882,
          0.20025892517906643,
          0.15044397224997533,
          0.11747322798080576,
          0.10094704698647677,
          0.16254590996721222,
          0.16400940010976175,
          0.1641501203157761,
          0.16448784881021064,
          0.16350280736810993,
          0.1628836384616467,
          0.16282735037924093,
          0.1624614778436035,
          0.16127942811308274,
          0.1611668519482712,
          0.15998480221775044,
          0.15764884679791172,
          0.15511588308965277,
          0.1545248582243924,
          0.15339909657627734,
          0.1528643597934227,
          0.15244219917537957,
          0.15249848725778534,
          0.15387754527672623,
          0.1560446364493477,
          0.15694524576783983,
          0.1572829742622743,
          0.15894347269324396,
          0.15970336180572167,
          0.1596470737233159,
          0.16018181050617056,
          0.16136386023669133,
          0.16150458044270574,
          0.16220818147277768,
          0.1624614778436035,
          0.16260219804961792,
          0.16251776592600928,
          0.1630806467500668,
          0.16336208716209558,
          0.16384053586254452,
          0.16434712860419629,
          0.16448784881021064,
          0.1399152864359793,
          0.08602226193659146,
          0.00044186144688512785,
          0.9730942966100501,
          0.9922885327104117,
          0.9965382829320463,
          0.9957502497783656,
          0.9980862051982043,
          0.9999999999999999,
          0.9992401108875223,
          0.5788244233989558,
          0.13602015113350127,
          0.07726946512249694,
          0.3097111014170525,
          0.3096829573758496,
          0.3095703812110381,
          0.3104428464883272,
          0.3099925418290812,
          0.30948594908742943,
          0.30858533976893743,
          0.31100572731238474,
          0.31038655840592144,
          0.3100206858702841,
          0.3054050631130123,
          0.2667070064590574,
          0.10306066448081277,
          0.011744508393960296,
          0.18247189113884862,
          0.20819554479827754,
          0.22806523788750824,
          0.22935986378284054,
          0.2308796420077959,
          0.23107665029621596,
          0.22997903268930378,
          0.22699576432179894,
          0.226067010962104,
          0.22356219129504804,
          0.22297116642978762,
          0.22502568143759763,
          0.225841858632481,
          0.22623587520932129,
          0.22657360370375576,
          0.2302604731013326,
          0.22899399124720318,
          0.22823410213472548,
          0.22792451768149388,
          0.1293866006219833,
          0.06266270773820412,
          0.030544727917481668,
          0.14650380648157266,
          0.1767586507746647,
          0.17794070050518554,
          0.17979820722457537,
          0.1797419191421696,
          0.1783065730408229,
          0.17656164248624454,
          0.1763927782390273,
          0.1763083461154187,
          0.17490114405527488,
          0.17442269535482594,
          0.17152385911092968,
          0.17076396999845203,
          0.16946934410311978,
          0.1695256321855255,
          0.16918790369109096,
          0.16730225293049825,
          0.16724596484809254,
          0.1686531669082363,
          0.1694974881443226,
          0.17293106117107357,
          0.17473227980805764,
          0.1757173212501583,
          0.17636463419782447,
          0.17729338755751942,
          0.17729338755751942,
          0.17782812434037396,
          0.1778562683815769,
          0.1790664621533006,
          0.17878502174127178,
          0.17858801345285166,
          0.1772370994751136,
          0.14689782305841295,
          0.09308923068263372,
          0.4791382294583678,
          0.5330059243206732,
          0.594922814967001,
          0.5957389921618844,
          0.5908137849513811,
          0.5067475338783896,
          0.20225715210447068,
          0.01735924461393412,
          0.09361270984900724,
          0.09335097026582043,
          0.09264736923574854,
          0.09273461576347747,
          0.09289503679833389,
          0.09403487046705036,
          0.09412211699477924,
          0.09357893699956374,
          0.09196628343863897,
          0.08855804004897064,
          0.07874421288152764,
          0.045759396591756596,
          0.04963483106539268,
          0.05123341260571604,
          0.05167527405260122,
          0.051897611978103914,
          0.05211713549948635,
          0.05284325176252058,
          0.05134317436640726,
          0.048855241124072996,
          0.048469667759593604,
          0.04885805552819328,
          0.05085065364535693,
          0.05123341260571604,
          0.05217905239013268,
          0.05256744015873238,
          0.05002040442987207,
          0.047251030775509045,
          0.02487933242334267,
          0.02653138764195148,
          0.027158999760775654,
          0.027158999760775654,
          0.02711678369897133,
          0.02719840141845964,
          0.02678186960865711,
          0.026227431996960426,
          0.025101670348845362,
          0.02483993076565863,
          0.02350871761676257,
          0.02220283410494911,
          0.02057610852342287,
          0.020120175055936246,
          0.020258080857830374,
          0.02131066799881795,
          0.02234073990684321,
          0.023331410157184457,
          0.02496376454695129,
          0.025020052629357026,
          0.025059454287041066,
          0.02557449024105371,
          0.026128927852750367,
          0.026089526195066326,
          0.026880373752867198,
          0.026961991472355507,
          0.027060495616565594,
          0.02719840141845964,
          0.027308163179150885,
          0.027240617480263962,
          0.026961991472355507,
          0.02653138764195148,
          0.02543658443915961,
          0.023058412957516566,
          0.0019728972883216456,
          0.117231189226461,
          0.45634155608403804,
          0.4797573983648311,
          0.4774777310273981,
          0.479785542406034,
          0.36751896204776036,
          0.1370277078085642,
          0.10487032633015775,
          0.10442846488327256,
          0.10472397731590277,
          0.10438343441734796,
          0.10444816571211463,
          0.10489002715899973,
          0.10484499669307512,
          0.1047436781447448,
          0.10444816571211463,
          0.10480840943951142,
          0.06000872465277288,
          0.06054909024386809,
          0.061798685673275816,
          0.06195910670813223,
          0.06215048618831179,
          0.06140748350055583,
          0.059108115334280814,
          0.05875912922336518,
          0.05767558363705444,
          0.05919254745788943,
          0.0598511180220368,
          0.06094029241658808,
          0.06179024246091494,
          0.06203228121525972,
          0.06203228121525972,
          0.06195066349577133,
          0.06203228121525972,
          0.06164107904253971,
          0.0612076608080154,
          0.060974065266031524,
          0.06047591573674063,
          0.06055753345622894,
          0.05976668589842818,
          0.05872535637392173,
          0.02542814122679876,
          0.0029917115798657468,
          0.007548231850611403,
          0.02183133276107116,
          0.035478378340345895,
          0.035514965593909625,
          0.035388317408496656,
          0.03538268860025609,
          0.03515753627063309,
          0.03508154735938532,
          0.03460028425481612,
          0.03438076073343371,
          0.03434417347986998,
          0.03411902115024695,
          0.033663087682760384,
          0.033001702714492764,
          0.0328075088301929,
          0.03271744789834369,
          0.03234876095858602,
          0.030404007711467285,
          0.029818611654447436,
          0.0296244177701476,
          0.02948932637237378,
          0.028180628456440066,
          0.028315719854213883,
          0.0288054261711439,
          0.029323276529276826,
          0.029458367927050644,
          0.03092185806960021,
          0.03138060594120709,
          0.03192941474466318,
          0.03255139805524676,
          0.033288771934762124,
          0.033325359188325826,
          0.03341542012017504,
          0.03387416799178192,
          0.03400925938955571,
          0.03400363058131517,
          0.034324472651027976,
          0.034332915863388797,
          0.0344286056034786,
          0.034693159590785616,
          0.03517442269535481,
          0.03514346425003165,
          0.03524196839424171,
          0.03517442269535481,
          0.034369503116952554,
          0.03208702137539929,
          0.001398758847782955
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=GB<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "GB",
         "line": {
          "color": "#00cc96",
          "dash": "solid"
         },
         "mode": "lines",
         "name": "GB",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          2,
          8,
          10,
          15,
          23,
          24,
          28,
          38,
          42,
          43,
          44,
          47,
          51,
          64,
          80,
          84,
          87,
          90,
          92,
          94,
          99,
          100,
          103,
          109,
          117,
          120,
          128,
          136,
          146,
          148,
          159,
          164,
          170,
          175,
          177,
          179,
          184,
          187,
          189,
          194,
          200,
          204,
          208,
          209,
          218,
          222,
          225,
          226,
          227,
          228,
          230,
          233,
          238,
          239,
          242,
          243,
          249,
          251,
          252,
          256,
          257,
          262,
          263,
          267,
          270,
          272,
          273,
          274,
          276,
          277,
          285,
          289,
          290,
          291,
          293,
          295,
          302,
          303,
          306,
          307,
          309,
          310,
          313,
          316,
          328,
          335,
          346,
          349,
          350,
          361,
          367,
          376,
          380,
          396,
          400,
          405,
          408,
          411,
          412,
          415,
          423,
          424,
          430,
          431,
          432,
          443,
          446,
          449,
          451,
          452,
          453,
          456,
          465,
          471,
          482,
          490,
          498,
          512,
          513,
          514,
          516,
          518,
          520,
          532,
          550,
          552,
          557,
          574,
          579,
          583,
          589,
          591,
          607,
          612,
          616,
          620,
          626,
          629,
          642,
          643,
          644,
          652,
          657,
          659,
          665,
          669,
          670,
          680,
          683,
          686,
          688,
          696,
          700,
          702,
          708,
          709,
          710,
          712,
          714,
          720,
          722,
          725,
          727,
          733,
          740,
          741,
          744,
          745,
          746,
          756,
          765,
          766,
          767,
          770,
          771,
          773,
          777,
          780,
          782,
          783,
          785,
          789,
          791,
          793,
          794,
          801,
          805,
          808,
          811,
          817,
          827,
          831,
          832,
          834,
          842,
          843,
          849,
          858,
          862,
          865,
          867,
          870,
          871,
          879,
          883,
          888,
          889,
          890,
          893,
          896,
          897,
          899,
          904,
          906,
          911,
          912,
          913,
          914,
          917,
          919,
          922,
          925,
          926,
          942,
          944,
          947,
          957,
          958,
          959,
          963,
          969,
          971,
          972,
          979,
          983,
          986,
          995,
          996,
          998,
          1001,
          1005,
          1007,
          1009,
          1016,
          1019,
          1023,
          1027,
          1029,
          1033,
          1036,
          1041,
          1042,
          1046,
          1049,
          1052,
          1054,
          1061,
          1065,
          1067,
          1074,
          1078,
          1082,
          1084,
          1087,
          1088,
          1104,
          1106,
          1107,
          1108,
          1114,
          1116,
          1119,
          1123,
          1126,
          1127,
          1138,
          1140,
          1149,
          1160,
          1161,
          1162,
          1171,
          1173,
          1179,
          1180,
          1182,
          1183,
          1185,
          1189,
          1196,
          1199,
          1204,
          1209,
          1215,
          1220,
          1224,
          1228,
          1229,
          1234,
          1235,
          1243,
          1254,
          1255,
          1258,
          1262,
          1264,
          1273,
          1274,
          1278,
          1290,
          1295,
          1298,
          1302,
          1304,
          1316,
          1318,
          1327,
          1328,
          1331,
          1334,
          1339,
          1344,
          1345,
          1346,
          1347,
          1353,
          1354,
          1358,
          1360,
          1362,
          1363,
          1367,
          1376,
          1385,
          1394,
          1396,
          1404,
          1405,
          1407,
          1415,
          1423,
          1424,
          1426,
          1432,
          1434,
          1438,
          1439,
          1441,
          1446,
          1452,
          1468,
          1470,
          1479,
          1482,
          1491,
          1492,
          1493,
          1502,
          1508,
          1511,
          1520,
          1522,
          1524,
          1528,
          1536,
          1537,
          1544,
          1545,
          1546,
          1551,
          1552,
          1557,
          1560,
          1561,
          1562,
          1563,
          1565,
          1567,
          1572,
          1576,
          1583,
          1588,
          1594,
          1597,
          1606,
          1609,
          1611,
          1616,
          1617,
          1629,
          1630,
          1631,
          1633,
          1635,
          1650,
          1652,
          1656,
          1657,
          1660,
          1668,
          1677,
          1678,
          1681,
          1692,
          1695,
          1697,
          1701,
          1704,
          1705,
          1706,
          1707,
          1710,
          1711,
          1712,
          1713,
          1714,
          1716,
          1717,
          1720,
          1721,
          1722,
          1723,
          1733,
          1737,
          1740,
          1741,
          1744,
          1745,
          1750,
          1752,
          1753,
          1756,
          1760,
          1769,
          1771,
          1772,
          1774,
          1780,
          1784,
          1785,
          1786,
          1792,
          1795,
          1797,
          1799,
          1800,
          1807,
          1808,
          1810,
          1814,
          1820,
          1821,
          1822,
          1827,
          1830,
          1831,
          1834,
          1835,
          1836,
          1839,
          1844,
          1846,
          1851,
          1852,
          1857,
          1861,
          1864
         ],
         "xaxis": "x",
         "y": [
          0.2585097465903058,
          0.33053474732593535,
          0.4480726660347786,
          0.7205746010478079,
          0.7296437002929632,
          0.7303322343250236,
          0.7290718353298782,
          0.09885304606556863,
          0.10128346934173585,
          0.10128346934173585,
          0.10128346934173585,
          0.10128346934173585,
          0.1414635152584159,
          0.16448069796976192,
          0.1414635152584159,
          0.13851980747689943,
          0.07465947220933883,
          0.07272057822899333,
          0.07272057822899333,
          0.07438632367913803,
          0.0781264563797768,
          0.0781264563797768,
          0.0781264563797768,
          0.08809674037990042,
          0.09502927443600442,
          0.09414752413429975,
          0.11463770155063299,
          0.093486062148911,
          0.09459613641347994,
          0.0877622510779276,
          0.0781264563797768,
          0.0781264563797768,
          0.04442017040797108,
          0.05022143258021258,
          0.05669952999475439,
          0.05669952999475439,
          0.05669952999475439,
          0.05669952999475439,
          0.05831532731918718,
          0.05831532731918718,
          0.05831532731918718,
          0.05831532731918718,
          0.05831532731918718,
          0.05637643333884165,
          0.0604463703875337,
          0.0604463703875337,
          0.0604463703875337,
          0.0604463703875337,
          0.0604463703875337,
          0.0604463703875337,
          0.06020979979260113,
          0.059598152970495866,
          0.05893669098510709,
          0.05893669098510709,
          0.05833676305131921,
          0.05833676305131921,
          0.05893669098510709,
          0.05893669098510709,
          0.059598152970495866,
          0.06020979979260113,
          0.059877910034736526,
          0.0604463703875337,
          0.0604463703875337,
          0.0604463703875337,
          0.0604463703875337,
          0.058042178788986376,
          0.058042178788986376,
          0.058042178788986376,
          0.058042178788986376,
          0.05637643333884165,
          0.05831532731918718,
          0.05831532731918718,
          0.05831532731918718,
          0.05831532731918718,
          0.05831532731918718,
          0.05831532731918718,
          0.05669952999475439,
          0.05669952999475439,
          0.05669952999475439,
          0.05669952999475439,
          0.05669952999475439,
          0.05669952999475439,
          0.027480245089547772,
          0.3068124631294773,
          0.7081782605225132,
          0.6995354143815429,
          0.13868874237239723,
          0.05763757423021143,
          0.08410094191810943,
          0.16329255714077903,
          0.16630167327478337,
          0.16630167327478337,
          0.16630167327478337,
          0.05763757423021143,
          0.0829413958134618,
          0.09280868246334587,
          0.09280868246334587,
          0.10511939575529997,
          0.10511939575529997,
          0.10511939575529997,
          0.10555253377782445,
          0.10528243029822507,
          0.10400932149073103,
          0.10400932149073103,
          0.10400932149073103,
          0.10400932149073103,
          0.10467078347611977,
          0.10555253377782445,
          0.10511939575529997,
          0.10511939575529997,
          0.10511939575529997,
          0.10511939575529997,
          0.09280868246334587,
          0.09071118912314644,
          0.0674015475201711,
          0.068833605726628,
          0.0702706448674641,
          0.0702706448674641,
          0.0702706448674641,
          0.0702706448674641,
          0.06999749633726329,
          0.06999749633726329,
          0.06999749633726329,
          0.07240168793581062,
          0.06886002239313924,
          0.06945995032692712,
          0.06945995032692712,
          0.07240168793581062,
          0.06999749633726329,
          0.06833175088711857,
          0.0702706448674641,
          0.0702706448674641,
          0.068833605726628,
          0.068833605726628,
          0.0674015475201711,
          0.044851493734591186,
          0.364093238416479,
          0.517714849856557,
          0.9026589993998115,
          0.9039193983949569,
          0.9039193983949569,
          0.8888589155536502,
          0.8101818355520768,
          0.7181227395657762,
          0.19988924793350196,
          0.22281267245317168,
          0.22281267245317168,
          0.2696737030302099,
          0.2726828191642142,
          0.2726828191642142,
          0.28407714388999705,
          0.28407714388999705,
          0.2726828191642142,
          0.2726828191642142,
          0.26798202634131796,
          0.24848753259358328,
          0.2453628096767815,
          0.2287155167949569,
          0.22281267245317168,
          0.14575857113660665,
          0.14575857113660665,
          0.14575857113660665,
          0.18701767411419892,
          0.18701767411419892,
          0.18701767411419892,
          0.18701767411419892,
          0.18596424392990044,
          0.1941594797003581,
          0.19609937779249906,
          0.19482626898500502,
          0.19482626898500502,
          0.19482626898500502,
          0.19482626898500502,
          0.19548773097039382,
          0.19548773097039382,
          0.19609937779249906,
          0.18701767411419892,
          0.18701767411419892,
          0.18701767411419892,
          0.18701767411419892,
          0.18701767411419892,
          0.18701767411419892,
          0.18701767411419892,
          0.18701767411419892,
          0.18701767411419892,
          0.09448419285978374,
          0.10317156006066144,
          0.13697943962875744,
          0.14350465141668542,
          0.14350465141668542,
          0.14350465141668542,
          0.14350465141668542,
          0.14350465141668542,
          0.14350465141668542,
          0.14323150288648465,
          0.14323150288648465,
          0.14563569448503197,
          0.14563569448503197,
          0.15431779302547446,
          0.15370614620336917,
          0.15304468421798037,
          0.15304468421798037,
          0.15304468421798037,
          0.15304468421798037,
          0.15304468421798037,
          0.15370614620336917,
          0.15370614620336917,
          0.15431779302547446,
          0.1450672341322348,
          0.14563569448503197,
          0.14563569448503197,
          0.14563569448503197,
          0.14563569448503197,
          0.14563569448503197,
          0.14323150288648465,
          0.14323150288648465,
          0.14323150288648465,
          0.14323150288648465,
          0.14350465141668542,
          0.14350465141668542,
          0.14350465141668542,
          0.14350465141668542,
          0.14350465141668542,
          0.14350465141668542,
          0.13697943962875744,
          0.10317156006066144,
          0.986149308595025,
          0.993628319319442,
          0.993628319319442,
          1,
          1,
          0.9964587727234804,
          0.9964587727234804,
          0.6832486176640491,
          0.17303805807489045,
          0.12527446104728643,
          0.28287503118622803,
          0.28287503118622803,
          0.28287503118622803,
          0.2836193677133174,
          0.2902341035570244,
          0.2902341035570244,
          0.2911087176601682,
          0.2902341035570244,
          0.2836193677133174,
          0.28287503118622803,
          0.28287503118622803,
          0.26531055832537864,
          0.1613934331522722,
          0.08761806919064927,
          0.16075557629405587,
          0.1816272508614946,
          0.20183954741814938,
          0.20183954741814938,
          0.20183954741814938,
          0.20183954741814938,
          0.2007861172338509,
          0.21030960427434428,
          0.21030960427434428,
          0.20964814228895548,
          0.2090482143551676,
          0.20964814228895548,
          0.20964814228895548,
          0.21030960427434428,
          0.21030960427434428,
          0.20183954741814938,
          0.20183954741814938,
          0.20183954741814938,
          0.20183954741814938,
          0.1569691853639599,
          0.09411320837073964,
          0.0001809629650350586,
          0.11074157160010542,
          0.1591557540115974,
          0.16230568262903364,
          0.16257792079174258,
          0.16257792079174258,
          0.16257792079174258,
          0.16230477226154175,
          0.16302607083731915,
          0.16302607083731915,
          0.16302607083731915,
          0.16302607083731915,
          0.16492844006638022,
          0.16391698666392315,
          0.16391698666392315,
          0.16391698666392315,
          0.16311135523644507,
          0.16255061828598857,
          0.16218988740785753,
          0.16301609188360563,
          0.16391698666392315,
          0.1644485925454526,
          0.16302607083731915,
          0.16302607083731915,
          0.16302607083731915,
          0.16230477226154175,
          0.16230477226154175,
          0.16257792079174258,
          0.16257792079174258,
          0.16257792079174258,
          0.16257792079174258,
          0.16257792079174258,
          0.16257792079174258,
          0.14343570309029988,
          0.10885266279575204,
          0.3926203821049883,
          0.3530882652157251,
          0.45688944562380684,
          0.6125620810438458,
          0.6119902160807609,
          0.5337867840063347,
          0.5190107439833129,
          0.04932367159708323,
          0.055910069597358975,
          0.07582832970410153,
          0.07790805810057147,
          0.07492298993721275,
          0.07492298993721275,
          0.07790805810057147,
          0.08011734604277651,
          0.055910069597358975,
          0.055910069597358975,
          0.055910069597358975,
          0.055910069597358975,
          0.019355872420970283,
          0.019355872420970283,
          0.019355872420970283,
          0.019355872420970283,
          0.019355872420970283,
          0.01741697844062476,
          0.021180217230968884,
          0.03108673892437569,
          0.03221319242179718,
          0.04835031604634529,
          0.03198682845888132,
          0.03251065219708049,
          0.03108673892437569,
          0.03108673892437569,
          0.03108673892437569,
          0.019355872420970283,
          0.019355872420970283,
          0.019355872420970283,
          0.0023381096933970946,
          0.0023381096933970946,
          0.0023381096933970946,
          0.002931168489124031,
          0.0032105829439158518,
          0.0032105829439158518,
          0.004100520847243216,
          0.0038273723170424157,
          0.0038273723170424157,
          0.0038273723170424157,
          0.0038273723170424157,
          0.0072718231504592945,
          0.0072718231504592945,
          0.0072718231504592945,
          0.0072718231504592945,
          0.0032589119642452424,
          0.0038273723170424157,
          0.0038273723170424157,
          0.0038273723170424157,
          0.0038273723170424157,
          0.004100520847243216,
          0.004100520847243216,
          0.004100520847243216,
          0.004100520847243216,
          0.0032105829439158518,
          0.0032105829439158518,
          0.0032105829439158518,
          0.0032105829439158518,
          0.0023381096933970946,
          0.0023381096933970946,
          0.0023381096933970946,
          0.0023381096933970946,
          0.0023381096933970946,
          0,
          0.10504410226421709,
          0.43013141829003465,
          0.5121804648279,
          0.4992116218277194,
          0.5121804648279,
          0.3514635901040063,
          0.10504410226421709,
          0.0711673388363323,
          0.08709512116187437,
          0.09151495422122755,
          0.08917484955834429,
          0.08917484955834429,
          0.0905814208212852,
          0.09151495422122755,
          0.09151495422122755,
          0.08709512116187437,
          0.07975899470816658,
          0.02734256998424539,
          0.02734256998424539,
          0.02813463556088,
          0.034968520896432376,
          0.044875042589839154,
          0.044875042589839154,
          0.0422784001184473,
          0.04087182885550639,
          0.04087182885550639,
          0.042939862103836074,
          0.04382161240554075,
          0.044875042589839154,
          0.044875042589839154,
          0.044875042589839154,
          0.044875042589839154,
          0.044875042589839154,
          0.034968520896432376,
          0.02813463556088,
          0.02603714222068057,
          0.024371396770535875,
          0.02734256998424539,
          0.02734256998424539,
          0.02734256998424539,
          0.02734256998424539,
          0.02330146791776977,
          0.02330146791776977,
          0.011086465655300082,
          0.011086465655300082,
          0.013040127218813813,
          0.01391260046933257,
          0.01391260046933257,
          0.01391260046933257,
          0.01391260046933257,
          0.01372985777095767,
          0.01372985777095767,
          0.01372985777095767,
          0.01372985777095767,
          0.01372985777095767,
          0.01372985777095767,
          0.01345670924075687,
          0.01345670924075687,
          0.01345670924075687,
          0.01345670924075687,
          0.01345670924075687,
          0.011121923616896151,
          0.011121923616896151,
          0.010460461631507378,
          0.008453962434778556,
          0.008453962434778556,
          0.009053890368566464,
          0.009053890368566464,
          0.010460461631507378,
          0.01345670924075687,
          0.01345670924075687,
          0.01345670924075687,
          0.01345670924075687,
          0.01345670924075687,
          0.01345670924075687,
          0.01345670924075687,
          0.01372985777095767,
          0.01372985777095767,
          0.01372985777095767,
          0.01372985777095767,
          0.01372985777095767,
          0.01372985777095767,
          0.01372985777095767,
          0.01391260046933257,
          0.01391260046933257,
          0.013040127218813813,
          0.013040127218813813,
          0.013040127218813813,
          0.012159146257002346,
          0.011086465655300082
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=RF<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "RF",
         "line": {
          "color": "#ab63fa",
          "dash": "solid"
         },
         "mode": "lines",
         "name": "RF",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          2,
          8,
          10,
          15,
          23,
          24,
          28,
          38,
          42,
          43,
          44,
          47,
          51,
          64,
          80,
          84,
          87,
          90,
          92,
          94,
          99,
          100,
          103,
          109,
          117,
          120,
          128,
          136,
          146,
          148,
          159,
          164,
          170,
          175,
          177,
          179,
          184,
          187,
          189,
          194,
          200,
          204,
          208,
          209,
          218,
          222,
          225,
          226,
          227,
          228,
          230,
          233,
          238,
          239,
          242,
          243,
          249,
          251,
          252,
          256,
          257,
          262,
          263,
          267,
          270,
          272,
          273,
          274,
          276,
          277,
          285,
          289,
          290,
          291,
          293,
          295,
          302,
          303,
          306,
          307,
          309,
          310,
          313,
          316,
          328,
          335,
          346,
          349,
          350,
          361,
          367,
          376,
          380,
          396,
          400,
          405,
          408,
          411,
          412,
          415,
          423,
          424,
          430,
          431,
          432,
          443,
          446,
          449,
          451,
          452,
          453,
          456,
          465,
          471,
          482,
          490,
          498,
          512,
          513,
          514,
          516,
          518,
          520,
          532,
          550,
          552,
          557,
          574,
          579,
          583,
          589,
          591,
          607,
          612,
          616,
          620,
          626,
          629,
          642,
          643,
          644,
          652,
          657,
          659,
          665,
          669,
          670,
          680,
          683,
          686,
          688,
          696,
          700,
          702,
          708,
          709,
          710,
          712,
          714,
          720,
          722,
          725,
          727,
          733,
          740,
          741,
          744,
          745,
          746,
          756,
          765,
          766,
          767,
          770,
          771,
          773,
          777,
          780,
          782,
          783,
          785,
          789,
          791,
          793,
          794,
          801,
          805,
          808,
          811,
          817,
          827,
          831,
          832,
          834,
          842,
          843,
          849,
          858,
          862,
          865,
          867,
          870,
          871,
          879,
          883,
          888,
          889,
          890,
          893,
          896,
          897,
          899,
          904,
          906,
          911,
          912,
          913,
          914,
          917,
          919,
          922,
          925,
          926,
          942,
          944,
          947,
          957,
          958,
          959,
          963,
          969,
          971,
          972,
          979,
          983,
          986,
          995,
          996,
          998,
          1001,
          1005,
          1007,
          1009,
          1016,
          1019,
          1023,
          1027,
          1029,
          1033,
          1036,
          1041,
          1042,
          1046,
          1049,
          1052,
          1054,
          1061,
          1065,
          1067,
          1074,
          1078,
          1082,
          1084,
          1087,
          1088,
          1104,
          1106,
          1107,
          1108,
          1114,
          1116,
          1119,
          1123,
          1126,
          1127,
          1138,
          1140,
          1149,
          1160,
          1161,
          1162,
          1171,
          1173,
          1179,
          1180,
          1182,
          1183,
          1185,
          1189,
          1196,
          1199,
          1204,
          1209,
          1215,
          1220,
          1224,
          1228,
          1229,
          1234,
          1235,
          1243,
          1254,
          1255,
          1258,
          1262,
          1264,
          1273,
          1274,
          1278,
          1290,
          1295,
          1298,
          1302,
          1304,
          1316,
          1318,
          1327,
          1328,
          1331,
          1334,
          1339,
          1344,
          1345,
          1346,
          1347,
          1353,
          1354,
          1358,
          1360,
          1362,
          1363,
          1367,
          1376,
          1385,
          1394,
          1396,
          1404,
          1405,
          1407,
          1415,
          1423,
          1424,
          1426,
          1432,
          1434,
          1438,
          1439,
          1441,
          1446,
          1452,
          1468,
          1470,
          1479,
          1482,
          1491,
          1492,
          1493,
          1502,
          1508,
          1511,
          1520,
          1522,
          1524,
          1528,
          1536,
          1537,
          1544,
          1545,
          1546,
          1551,
          1552,
          1557,
          1560,
          1561,
          1562,
          1563,
          1565,
          1567,
          1572,
          1576,
          1583,
          1588,
          1594,
          1597,
          1606,
          1609,
          1611,
          1616,
          1617,
          1629,
          1630,
          1631,
          1633,
          1635,
          1650,
          1652,
          1656,
          1657,
          1660,
          1668,
          1677,
          1678,
          1681,
          1692,
          1695,
          1697,
          1701,
          1704,
          1705,
          1706,
          1707,
          1710,
          1711,
          1712,
          1713,
          1714,
          1716,
          1717,
          1720,
          1721,
          1722,
          1723,
          1733,
          1737,
          1740,
          1741,
          1744,
          1745,
          1750,
          1752,
          1753,
          1756,
          1760,
          1769,
          1771,
          1772,
          1774,
          1780,
          1784,
          1785,
          1786,
          1792,
          1795,
          1797,
          1799,
          1800,
          1807,
          1808,
          1810,
          1814,
          1820,
          1821,
          1822,
          1827,
          1830,
          1831,
          1834,
          1835,
          1836,
          1839,
          1844,
          1846,
          1851,
          1852,
          1857,
          1861,
          1864
         ],
         "xaxis": "x",
         "y": [
          0.24508064284414013,
          0.26588278999159864,
          0.4066088845821516,
          0.6888093608395779,
          0.7178321208888766,
          0.718299979683495,
          0.7108757392662671,
          0.050953673014288714,
          0.11742230897829528,
          0.1405298306914257,
          0.14447232837816723,
          0.08078073437899208,
          0.16042529979370004,
          0.16262479022779683,
          0.1625061561271273,
          0.13747598056092383,
          0.04447923472982468,
          0.07185663215273089,
          0.09012099840015389,
          0.028590349408115223,
          0.10326642396892269,
          0.10493899429624745,
          0.10814981225999656,
          0.10839279242096964,
          0.10656644483100128,
          0.10488219910853139,
          0.10333947700111737,
          0.10396638884233647,
          0.10740173877241796,
          0.1077147395737825,
          0.10753490259048723,
          0.09834497236471917,
          0.0551131479720581,
          0.034481962272244204,
          0.0713724657867659,
          0.07163517283328522,
          0.07389619542983009,
          0.07409467241140222,
          0.07388208260625925,
          0.07371843452638205,
          0.0726725553476979,
          0.07237026713458466,
          0.07216546185083145,
          0.07164898269506403,
          0.07075359075468204,
          0.07015303936327322,
          0.06984303496092947,
          0.06960011210425252,
          0.06932493782140547,
          0.0684984488508461,
          0.06702677412526178,
          0.06651127983211325,
          0.06553385844938098,
          0.06523120651974393,
          0.06470796525406558,
          0.06470796525406558,
          0.06552880572594758,
          0.06597719005807609,
          0.06611997010534315,
          0.06702677412526178,
          0.0676996648499043,
          0.06976830258134573,
          0.06976775321784584,
          0.07048246565970195,
          0.07088476739236652,
          0.07103985412921474,
          0.07120326798539234,
          0.0712699943391405,
          0.07150198296780882,
          0.07164898269506403,
          0.0723351467048943,
          0.0729264133795621,
          0.07297915227557261,
          0.07305283328707854,
          0.07361668624950207,
          0.0736510986633026,
          0.07389619542983009,
          0.07397959767203041,
          0.07373352307906622,
          0.07163517283328522,
          0.0713724657867659,
          0.06037400676559157,
          0.07282722864115326,
          0.3180542778052502,
          0.6796510497983654,
          0.6816618054548227,
          0.13207363603947278,
          0.04658595850667943,
          0.07069038080092421,
          0.17842131144310444,
          0.17857293103324204,
          0.17837305270115564,
          0.17928236295524794,
          0.022779054434881124,
          0.08709261316223882,
          0.11912159068504577,
          0.12044325270872994,
          0.12128812072990333,
          0.12130254388972714,
          0.12138465005353224,
          0.11929071768482993,
          0.11837331781640345,
          0.11718278649224792,
          0.11689681437508817,
          0.11611274952723596,
          0.11749689928111198,
          0.11772243430942161,
          0.11929071768482993,
          0.12030740979641921,
          0.12080449851795708,
          0.1209680861813803,
          0.12140973133884778,
          0.11999294655153012,
          0.10849907484108254,
          0.08152221699299281,
          0.08471239871222933,
          0.08448608226169993,
          0.08320857296139009,
          0.08302861142412846,
          0.08297143973297913,
          0.08256742076578805,
          0.08240445018912429,
          0.08229150294785884,
          0.08035590637487683,
          0.07540617296938076,
          0.07576608467776288,
          0.07733227834071951,
          0.08197314206365375,
          0.08238272333628424,
          0.08284229195153167,
          0.08347877881968649,
          0.08367498304657128,
          0.08491960773191795,
          0.08444149620418478,
          0.08152221699299281,
          0.04185053985359172,
          0.31238350149073757,
          0.46476642273995805,
          0.8863251267787696,
          0.8869958711970461,
          0.8861497566890345,
          0.8759666648690806,
          0.8320306532167926,
          0.7856721623207356,
          0.1406151667337404,
          0.2101445672309226,
          0.16762015300786967,
          0.28328801234427337,
          0.2854771501179282,
          0.2856806135204426,
          0.28466949105492445,
          0.28466949105492445,
          0.28573632182088915,
          0.2857098718627151,
          0.2812384223714305,
          0.2826088001506658,
          0.27790617281546415,
          0.24574946875247136,
          0.16762015300786967,
          0.14892047254372867,
          0.13223254985084235,
          0.10309616769563287,
          0.19397191671783773,
          0.20750670840320054,
          0.20799723317809035,
          0.20799958218064232,
          0.20730850184107522,
          0.20701056686011152,
          0.2063192213105793,
          0.20158109372712982,
          0.2011287307719282,
          0.20130484913263522,
          0.20171275153146515,
          0.2026891362273854,
          0.20296057389125358,
          0.20413065184399284,
          0.20697533182182845,
          0.20799723317809035,
          0.20797410308037925,
          0.20812693505893107,
          0.20831277620641084,
          0.2066484125037866,
          0.2037076270646769,
          0.19397191671783773,
          0.17046191662326213,
          0.089753985694944,
          0.11385489125092105,
          0.13431909541161988,
          0.15159725566978974,
          0.15888988564160436,
          0.15804295656012765,
          0.15781236596685008,
          0.15756003762817952,
          0.15711547942339346,
          0.1560885485495707,
          0.15599657752222465,
          0.1548591582823887,
          0.15303981791858293,
          0.14986801565589383,
          0.14939255575422647,
          0.1486030493661876,
          0.14788063162765305,
          0.1476099280270831,
          0.1460315850414792,
          0.14756707293818158,
          0.14894033487579308,
          0.14901532772945023,
          0.14986801565589383,
          0.1520085300238965,
          0.15309954699154235,
          0.15331239219658885,
          0.153834661657179,
          0.154869823511719,
          0.15506869783464985,
          0.1558343732129258,
          0.15584943335025667,
          0.15607154669642112,
          0.15630102909091456,
          0.15663469164295438,
          0.15710766520119332,
          0.1575111632202391,
          0.15804295656012765,
          0.1583060827331104,
          0.1516408846034827,
          0.13431909541161988,
          0.11385489125092105,
          0.9402020955821152,
          0.9805414169570775,
          0.9960786480721199,
          0.9996099898020611,
          0.9996099898020611,
          0.9999999999999999,
          0.9994416761917694,
          0.6272020661674904,
          0.1584544558691162,
          0.054275243133248774,
          0.3052291331867134,
          0.30552380040202964,
          0.30539472162929737,
          0.30600298542122295,
          0.30580101382476466,
          0.30477159660751574,
          0.3040953159312051,
          0.306044467101373,
          0.30600298542122295,
          0.30540199595978135,
          0.3018456723982632,
          0.2685148890271758,
          0.11865304423887427,
          0.041862132235311156,
          0.1626358722156435,
          0.18943194833759144,
          0.22254673239939868,
          0.22461288378741237,
          0.22589286522347343,
          0.22604912835595445,
          0.2249070963570741,
          0.2219983776443383,
          0.22097315169172252,
          0.21843185770682125,
          0.2179984856795115,
          0.21971976939444135,
          0.22040590546250746,
          0.22097315169172252,
          0.22147906811640009,
          0.2251999805089613,
          0.22382776757148234,
          0.22279692958656003,
          0.22254673239939868,
          0.1338878470260472,
          0.07109731376261214,
          0.012620182982684663,
          0.12212076196276386,
          0.16683365343781534,
          0.17090099871584827,
          0.17384338015064363,
          0.17367958931744307,
          0.17251938096413674,
          0.17106132287592765,
          0.1708138393550214,
          0.17067298444795034,
          0.16886877999575003,
          0.16872324873336828,
          0.16579262854867974,
          0.16500690613854194,
          0.16426895940934447,
          0.163923277162794,
          0.16344133923709517,
          0.16181544992304667,
          0.1612471665334305,
          0.16287179068769417,
          0.16426895940934447,
          0.16731537892491857,
          0.16886877999575003,
          0.170162010090358,
          0.17067298444795034,
          0.1714284160913058,
          0.171527225747049,
          0.172279815855059,
          0.17238529838297204,
          0.17394410310685213,
          0.17400400267193314,
          0.1738730457796491,
          0.17245600525415059,
          0.15130274270924907,
          0.10994032951002738,
          0.40147525316576216,
          0.37486395640425496,
          0.40088250889271493,
          0.5934007340149898,
          0.5914434939408737,
          0.5632071003830567,
          0.5258295207703743,
          0.047712447307219824,
          0.08711148095693255,
          0.08718686215375324,
          0.08673905039099719,
          0.08657143769275716,
          0.08627767976875084,
          0.08668709291720655,
          0.08754159469037887,
          0.08711148095693255,
          0.08678762170183427,
          0.0865815251432333,
          0.0865439979331051,
          0.02458533320788278,
          0.0406181849178667,
          0.027645280326057675,
          0.04022144894107607,
          0.043259590928877684,
          0.04475965164538426,
          0.04577152332905468,
          0.045131696236101804,
          0.04270437386595821,
          0.04097820555177023,
          0.04135057455110647,
          0.043141752458104504,
          0.04330360441702105,
          0.04408653928098452,
          0.04567039308611265,
          0.042815601031160805,
          0.04022144894107607,
          0.027645280326057675,
          0.016972234879815307,
          0.02007416852425359,
          0.0202628029009784,
          0.02026241171627896,
          0.019999514286891673,
          0.019938253152791297,
          0.019451173265918498,
          0.01794685214075109,
          0.017802177262997654,
          0.01685662227839621,
          0.015720346756536524,
          0.013818547404952669,
          0.013746544793657611,
          0.01345178049255033,
          0.013814199855874315,
          0.014778562489338065,
          0.0157114361752827,
          0.01767379574891431,
          0.017802177262997654,
          0.01794685214075109,
          0.018094894237888598,
          0.018747796655733645,
          0.018844332609611908,
          0.019664808417790697,
          0.019655742972860463,
          0.019938253152791297,
          0.019999514286891673,
          0.02007183846526997,
          0.020006651750090032,
          0.01990765455301391,
          0.016972234879815307,
          0.01639548845076208,
          0.015483071451358188,
          0,
          0.16920212808454524,
          0.4499063531771833,
          0.47679298361814004,
          0.4779120566879861,
          0.47679298361814004,
          0.36616992477030386,
          0.16920212808454524,
          0.09868993712359486,
          0.09829338381693425,
          0.09846794785780755,
          0.09842187615089582,
          0.09792764080443411,
          0.09876093288419938,
          0.09866608953820896,
          0.09846794785780755,
          0.09829338381693425,
          0.09861728853659826,
          0.05223272005880958,
          0.05371571025172875,
          0.05486089777560957,
          0.055109640096674156,
          0.055494290211773295,
          0.05458086446732188,
          0.052342239867203544,
          0.051941103164030555,
          0.05093655939408201,
          0.05255276455270691,
          0.05340226664443892,
          0.05394710973540187,
          0.05522327025360074,
          0.055506580325731136,
          0.05551216631063108,
          0.055494290211773295,
          0.055413232574510723,
          0.05486089777560957,
          0.0546652864824331,
          0.054454188821624394,
          0.053938528298658944,
          0.05371571025172875,
          0.05223272005880958,
          0.04963513478512718,
          0.024137006180050297,
          0.0068438662947180595,
          0.01491772052621873,
          0.01491772052621873,
          0.028209535365732313,
          0.028472079023969726,
          0.028290614314509854,
          0.02827635028066522,
          0.028079952829376476,
          0.028033311394628058,
          0.027575199539834366,
          0.027325416269654512,
          0.02725966550970846,
          0.027083372394578115,
          0.026770338915556163,
          0.02594733603527649,
          0.02580049279543256,
          0.025621381350828504,
          0.02539924838453872,
          0.023446455313235914,
          0.022715809435520068,
          0.022710037330054944,
          0.022496634236902485,
          0.021091679708406058,
          0.02113673935514554,
          0.021558778392319483,
          0.022243271579366453,
          0.022496634236902485,
          0.02378132078190534,
          0.024436635663258682,
          0.02495496296714611,
          0.025621381350828504,
          0.026311265201047818,
          0.026386752955444126,
          0.026400136586920808,
          0.02680821543156514,
          0.027083372394578115,
          0.02714762613898647,
          0.027325416269654512,
          0.027476463290420278,
          0.027575199539834366,
          0.027918264732332027,
          0.028354127838960658,
          0.028290614314509854,
          0.028505898977650296,
          0.028436764422696825,
          0.027716492043348473,
          0.025201350128998212,
          0.01491772052621873
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "variable"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "index"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "value"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"4e3e523e-306e-478f-b14b-0927d9afac36\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"4e3e523e-306e-478f-b14b-0927d9afac36\")) {                    Plotly.newPlot(                        \"4e3e523e-306e-478f-b14b-0927d9afac36\",                        [{\"hovertemplate\": \"variable=Analytic Function<br>index=%{x}<br>value=%{y}<extra></extra>\", \"legendgroup\": \"Analytic Function\", \"line\": {\"color\": \"#636efa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"Analytic Function\", \"showlegend\": true, \"type\": \"scattergl\", \"x\": [2, 8, 10, 15, 23, 24, 28, 38, 42, 43, 44, 47, 51, 64, 80, 84, 87, 90, 92, 94, 99, 100, 103, 109, 117, 120, 128, 136, 146, 148, 159, 164, 170, 175, 177, 179, 184, 187, 189, 194, 200, 204, 208, 209, 218, 222, 225, 226, 227, 228, 230, 233, 238, 239, 242, 243, 249, 251, 252, 256, 257, 262, 263, 267, 270, 272, 273, 274, 276, 277, 285, 289, 290, 291, 293, 295, 302, 303, 306, 307, 309, 310, 313, 316, 328, 335, 346, 349, 350, 361, 367, 376, 380, 396, 400, 405, 408, 411, 412, 415, 423, 424, 430, 431, 432, 443, 446, 449, 451, 452, 453, 456, 465, 471, 482, 490, 498, 512, 513, 514, 516, 518, 520, 532, 550, 552, 557, 574, 579, 583, 589, 591, 607, 612, 616, 620, 626, 629, 642, 643, 644, 652, 657, 659, 665, 669, 670, 680, 683, 686, 688, 696, 700, 702, 708, 709, 710, 712, 714, 720, 722, 725, 727, 733, 740, 741, 744, 745, 746, 756, 765, 766, 767, 770, 771, 773, 777, 780, 782, 783, 785, 789, 791, 793, 794, 801, 805, 808, 811, 817, 827, 831, 832, 834, 842, 843, 849, 858, 862, 865, 867, 870, 871, 879, 883, 888, 889, 890, 893, 896, 897, 899, 904, 906, 911, 912, 913, 914, 917, 919, 922, 925, 926, 942, 944, 947, 957, 958, 959, 963, 969, 971, 972, 979, 983, 986, 995, 996, 998, 1001, 1005, 1007, 1009, 1016, 1019, 1023, 1027, 1029, 1033, 1036, 1041, 1042, 1046, 1049, 1052, 1054, 1061, 1065, 1067, 1074, 1078, 1082, 1084, 1087, 1088, 1104, 1106, 1107, 1108, 1114, 1116, 1119, 1123, 1126, 1127, 1138, 1140, 1149, 1160, 1161, 1162, 1171, 1173, 1179, 1180, 1182, 1183, 1185, 1189, 1196, 1199, 1204, 1209, 1215, 1220, 1224, 1228, 1229, 1234, 1235, 1243, 1254, 1255, 1258, 1262, 1264, 1273, 1274, 1278, 1290, 1295, 1298, 1302, 1304, 1316, 1318, 1327, 1328, 1331, 1334, 1339, 1344, 1345, 1346, 1347, 1353, 1354, 1358, 1360, 1362, 1363, 1367, 1376, 1385, 1394, 1396, 1404, 1405, 1407, 1415, 1423, 1424, 1426, 1432, 1434, 1438, 1439, 1441, 1446, 1452, 1468, 1470, 1479, 1482, 1491, 1492, 1493, 1502, 1508, 1511, 1520, 1522, 1524, 1528, 1536, 1537, 1544, 1545, 1546, 1551, 1552, 1557, 1560, 1561, 1562, 1563, 1565, 1567, 1572, 1576, 1583, 1588, 1594, 1597, 1606, 1609, 1611, 1616, 1617, 1629, 1630, 1631, 1633, 1635, 1650, 1652, 1656, 1657, 1660, 1668, 1677, 1678, 1681, 1692, 1695, 1697, 1701, 1704, 1705, 1706, 1707, 1710, 1711, 1712, 1713, 1714, 1716, 1717, 1720, 1721, 1722, 1723, 1733, 1737, 1740, 1741, 1744, 1745, 1750, 1752, 1753, 1756, 1760, 1769, 1771, 1772, 1774, 1780, 1784, 1785, 1786, 1792, 1795, 1797, 1799, 1800, 1807, 1808, 1810, 1814, 1820, 1821, 1822, 1827, 1830, 1831, 1834, 1835, 1836, 1839, 1844, 1846, 1851, 1852, 1857, 1861, 1864], \"xaxis\": \"x\", \"y\": [1.1646338169234316e-05, 2.1273419643883117e-05, 2.4482446802099374e-05, 3.250501469764006e-05, 3.9725325803626675e-05, 3.972532580362666e-05, 3.41095282767482e-05, 0.0008181212434996286, 0.0008437934607653589, 0.0008437934607653587, 0.0008437934607653589, 0.0008437934607653589, 0.0008437934607653589, 0.0008437934607653589, 0.0008437934607653589, 0.0008437934607653587, 0.013708883620153071, 0.013708883620153075, 0.013708883620153075, 0.013708883620153071, 0.013708883620153075, 0.013708883620153075, 0.013708883620153071, 0.013708883620153075, 0.013708883620153075, 0.013708883620153075, 0.013708883620153075, 0.013708883620153075, 0.013708883620153075, 0.013708883620153075, 0.013708883620153075, 0.013708883620153075, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 0.21955032617035652, 6.132330664548191e-05, 6.555393163809458e-05, 8.177132744310972e-05, 7.824580661593252e-05, 6.273351497635282e-05, 0.0015165294869970872, 0.0015165294869970876, 0.0015165294869970876, 0.0015165294869970876, 0.0015165294869970876, 0.0015165294869970876, 0.0015165294869970876, 0.024472660039860733, 0.024472660039860733, 0.024472660039860733, 0.024472660039860733, 0.024472660039860733, 0.024472660039860733, 0.024472660039860733, 0.024472660039860733, 0.024472660039860733, 0.024472660039860733, 0.024472660039860733, 0.024472660039860733, 0.024472660039860733, 0.024472660039860733, 0.024472660039860733, 0.024472660039860733, 0.024472660039860733, 0.024472660039860733, 0.024472660039860733, 0.024472660039860733, 0.39177074888567903, 0.39177074888567903, 0.39177074888567903, 0.39177074888567903, 0.39177074888567903, 0.39177074888567903, 0.39177074888567903, 0.39177074888567903, 0.39177074888567903, 0.39177074888567903, 0.39177074888567903, 0.39177074888567903, 0.39177074888567903, 0.39177074888567903, 0.39177074888567903, 0.39177074888567903, 0.39177074888567903, 0.39177074888567903, 0.39177074888567903, 0.39177074888567903, 0.39177074888567903, 0.39177074888567903, 5.68801550890094e-05, 6.813415649991788e-05, 0.00011425349561520962, 0.00011425349561520962, 0.00011425349561520962, 0.00010939882833991571, 9.06421593217349e-05, 8.313949171446255e-05, 0.002036244177750685, 0.002036244177750685, 0.0020362441777506857, 0.0020362441777506857, 0.0020362441777506857, 0.002036244177750685, 0.0020362441777506857, 0.0020362441777506857, 0.002036244177750685, 0.002036244177750685, 0.002036244177750685, 0.002036244177750685, 0.002036244177750685, 0.0020362441777506857, 0.0020362441777506857, 0.0327880950919183, 0.03278809509191829, 0.0327880950919183, 0.0327880950919183, 0.03278809509191829, 0.03278809509191829, 0.0327880950919183, 0.0327880950919183, 0.0327880950919183, 0.0327880950919183, 0.0327880950919183, 0.0327880950919183, 0.0327880950919183, 0.0327880950919183, 0.0327880950919183, 0.0327880950919183, 0.0327880950919183, 0.0327880950919183, 0.03278809509191829, 0.0327880950919183, 0.0327880950919183, 0.0327880950919183, 0.0327880950919183, 0.0327880950919183, 0.0327880950919183, 0.03278809509191829, 0.5248177097186, 0.5248177097186002, 0.5248177097186, 0.5248177097186002, 0.5248177097186002, 0.5248177097186002, 0.5248177097186002, 0.5248177097186002, 0.5248177097186002, 0.5248177097186002, 0.5248177097186002, 0.5248177097186002, 0.5248177097186002, 0.5248177097186002, 0.5248177097186002, 0.5248177097186002, 0.5248177097186002, 0.5248177097186002, 0.5248177097186002, 0.5248177097186002, 0.5248177097186002, 0.5248177097186002, 0.5248177097186002, 0.5248177097186002, 0.5248177097186002, 0.5248177097186002, 0.5248177097186002, 0.5248177097186002, 0.5248177097186002, 0.5248177097186002, 0.5248177097186002, 0.5248177097186002, 0.5248177097186002, 0.5248177097186002, 0.5248177097186002, 0.5248177097186002, 0.5248177097186002, 0.5248177097186002, 0.5248177097186002, 0.5248177097186, 0.5248177097186002, 0.0002141849905855906, 0.000217929603027419, 0.0002216742154692474, 0.00023026479695344192, 0.00023026479695344192, 0.0002291634403529042, 0.00022541882791107578, 0.00019920654081827697, 0.0001842280910509633, 0.003892424999162403, 0.003892424999162403, 0.003892424999162403, 0.003892424999162403, 0.003892424999162403, 0.003892424999162403, 0.003892424999162403, 0.003892424999162403, 0.003892424999162403, 0.003892424999162403, 0.003892424999162403, 0.003892424999162403, 0.003892424999162403, 0.003892424999162403, 0.062486988234505776, 0.062486988234505776, 0.062486988234505776, 0.062486988234505776, 0.062486988234505776, 0.062486988234505776, 0.062486988234505776, 0.062486988234505776, 0.062486988234505776, 0.062486988234505776, 0.062486988234505776, 0.062486988234505776, 0.062486988234505776, 0.062486988234505776, 0.062486988234505776, 0.062486988234505776, 0.062486988234505776, 0.062486988234505776, 0.062486988234505776, 0.062486988234505776, 0.062486988234505776, 0.062486988234505776, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.0, 8.294641427834098e-07, 4.147320713917071e-06, 1.2550153116896873e-05, 8.29464142783414e-06, 5.806248999483897e-06, 2.488392428350241e-06, 0.0003807167896062822, 0.000408990697777682, 0.000408990697777682, 0.000408990697777682, 0.000408990697777682, 0.000408990697777682, 0.000408990697777682, 0.000408990697777682, 0.000408990697777682, 0.000408990697777682, 0.000408990697777682, 0.000408990697777682, 0.006752039412350245, 0.006752039412350245, 0.006752039412350245, 0.006752039412350245, 0.006752039412350245, 0.006752039412350245, 0.006752039412350245, 0.006752039412350245, 0.006752039412350245, 0.006752039412350245, 0.006752039412350245, 0.006752039412350245, 0.006752039412350245, 0.006752039412350245, 0.006752039412350245, 0.006752039412350245, 0.006752039412350245, 0.006752039412350245, 0.10824081884551125, 0.10824081884551125, 0.10824081884551125, 0.10824081884551125, 0.10824081884551125, 0.10824081884551125, 0.10824081884551125, 0.10824081884551125, 0.10824081884551125, 0.10824081884551125, 0.10824081884551125, 0.10824081884551125, 0.10824081884551125, 0.10824081884551125, 0.10824081884551125, 0.10824081884551125, 0.10824081884551125, 0.10824081884551125, 0.10824081884551125, 0.10824081884551125, 0.10824081884551125, 0.10824081884551125, 0.10824081884551125, 0.10824081884551125, 0.10824081884551125, 0.10824081884551125, 0.10824081884551125, 0.10824081884551125, 0.10824081884551125, 0.10824081884551125, 0.10824081884551125, 0.10824081884551125, 0.10824081884551125, 0.10824081884551125, 2.185727971539698e-05, 2.5239824029556524e-05, 2.7945859480884166e-05, 3.1416644081500054e-05, 2.7945859480884166e-05, 2.3886806303892714e-05, 2.185727971539698e-05, 0.0007108545532113329, 0.0007108545532113329, 0.0007108545532113329, 0.000710854553211333, 0.0007108545532113329, 0.000710854553211333, 0.0007108545532113329, 0.0007108545532113329, 0.0007108545532113329, 0.0007108545532113329, 0.011581861099288658, 0.01158186109928866, 0.011581861099288658, 0.01158186109928866, 0.01158186109928866, 0.011581861099288658, 0.01158186109928866, 0.01158186109928866, 0.011581861099288658, 0.01158186109928866, 0.01158186109928866, 0.01158186109928866, 0.01158186109928866, 0.01158186109928866, 0.01158186109928866, 0.01158186109928866, 0.01158186109928866, 0.011581861099288658, 0.011581861099288658, 0.011581861099288658, 0.011581861099288658, 0.01158186109928866, 0.011581861099288658, 0.01158186109928866, 0.011581861099288658, 0.011581861099288658, 0.18551796583652588, 0.18551796583652586, 0.18551796583652586, 0.18551796583652586, 0.18551796583652586, 0.18551796583652586, 0.18551796583652588, 0.18551796583652586, 0.18551796583652588, 0.18551796583652586, 0.18551796583652586, 0.18551796583652586, 0.18551796583652586, 0.18551796583652586, 0.18551796583652588, 0.18551796583652586, 0.18551796583652586, 0.18551796583652586, 0.18551796583652586, 0.18551796583652586, 0.18551796583652586, 0.18551796583652586, 0.18551796583652586, 0.18551796583652586, 0.18551796583652586, 0.18551796583652586, 0.18551796583652586, 0.18551796583652586, 0.18551796583652586, 0.18551796583652586, 0.18551796583652588, 0.18551796583652586, 0.18551796583652586, 0.18551796583652586, 0.18551796583652586, 0.18551796583652586, 0.18551796583652586, 0.18551796583652586, 0.18551796583652588, 0.18551796583652586, 0.18551796583652588, 0.18551796583652586, 0.18551796583652586, 0.18551796583652588, 0.18551796583652588, 0.18551796583652586, 0.18551796583652588], \"yaxis\": \"y\"}, {\"hovertemplate\": \"variable=Signal<br>index=%{x}<br>value=%{y}<extra></extra>\", \"legendgroup\": \"Signal\", \"line\": {\"color\": \"#EF553B\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"Signal\", \"showlegend\": true, \"type\": \"scattergl\", \"x\": [2, 8, 10, 15, 23, 24, 28, 38, 42, 43, 44, 47, 51, 64, 80, 84, 87, 90, 92, 94, 99, 100, 103, 109, 117, 120, 128, 136, 146, 148, 159, 164, 170, 175, 177, 179, 184, 187, 189, 194, 200, 204, 208, 209, 218, 222, 225, 226, 227, 228, 230, 233, 238, 239, 242, 243, 249, 251, 252, 256, 257, 262, 263, 267, 270, 272, 273, 274, 276, 277, 285, 289, 290, 291, 293, 295, 302, 303, 306, 307, 309, 310, 313, 316, 328, 335, 346, 349, 350, 361, 367, 376, 380, 396, 400, 405, 408, 411, 412, 415, 423, 424, 430, 431, 432, 443, 446, 449, 451, 452, 453, 456, 465, 471, 482, 490, 498, 512, 513, 514, 516, 518, 520, 532, 550, 552, 557, 574, 579, 583, 589, 591, 607, 612, 616, 620, 626, 629, 642, 643, 644, 652, 657, 659, 665, 669, 670, 680, 683, 686, 688, 696, 700, 702, 708, 709, 710, 712, 714, 720, 722, 725, 727, 733, 740, 741, 744, 745, 746, 756, 765, 766, 767, 770, 771, 773, 777, 780, 782, 783, 785, 789, 791, 793, 794, 801, 805, 808, 811, 817, 827, 831, 832, 834, 842, 843, 849, 858, 862, 865, 867, 870, 871, 879, 883, 888, 889, 890, 893, 896, 897, 899, 904, 906, 911, 912, 913, 914, 917, 919, 922, 925, 926, 942, 944, 947, 957, 958, 959, 963, 969, 971, 972, 979, 983, 986, 995, 996, 998, 1001, 1005, 1007, 1009, 1016, 1019, 1023, 1027, 1029, 1033, 1036, 1041, 1042, 1046, 1049, 1052, 1054, 1061, 1065, 1067, 1074, 1078, 1082, 1084, 1087, 1088, 1104, 1106, 1107, 1108, 1114, 1116, 1119, 1123, 1126, 1127, 1138, 1140, 1149, 1160, 1161, 1162, 1171, 1173, 1179, 1180, 1182, 1183, 1185, 1189, 1196, 1199, 1204, 1209, 1215, 1220, 1224, 1228, 1229, 1234, 1235, 1243, 1254, 1255, 1258, 1262, 1264, 1273, 1274, 1278, 1290, 1295, 1298, 1302, 1304, 1316, 1318, 1327, 1328, 1331, 1334, 1339, 1344, 1345, 1346, 1347, 1353, 1354, 1358, 1360, 1362, 1363, 1367, 1376, 1385, 1394, 1396, 1404, 1405, 1407, 1415, 1423, 1424, 1426, 1432, 1434, 1438, 1439, 1441, 1446, 1452, 1468, 1470, 1479, 1482, 1491, 1492, 1493, 1502, 1508, 1511, 1520, 1522, 1524, 1528, 1536, 1537, 1544, 1545, 1546, 1551, 1552, 1557, 1560, 1561, 1562, 1563, 1565, 1567, 1572, 1576, 1583, 1588, 1594, 1597, 1606, 1609, 1611, 1616, 1617, 1629, 1630, 1631, 1633, 1635, 1650, 1652, 1656, 1657, 1660, 1668, 1677, 1678, 1681, 1692, 1695, 1697, 1701, 1704, 1705, 1706, 1707, 1710, 1711, 1712, 1713, 1714, 1716, 1717, 1720, 1721, 1722, 1723, 1733, 1737, 1740, 1741, 1744, 1745, 1750, 1752, 1753, 1756, 1760, 1769, 1771, 1772, 1774, 1780, 1784, 1785, 1786, 1792, 1795, 1797, 1799, 1800, 1807, 1808, 1810, 1814, 1820, 1821, 1822, 1827, 1830, 1831, 1834, 1835, 1836, 1839, 1844, 1846, 1851, 1852, 1857, 1861, 1864], \"xaxis\": \"x\", \"y\": [0.26878966550807026, 0.6905562669743748, 0.7176308346115418, 0.7191224687952942, 0.7210362635970898, 0.7181655713943964, 0.707330115531289, 0.0, 0.1357358963173522, 0.1575081265918973, 0.16595133895276024, 0.1690471834850766, 0.1695256321855255, 0.16792142183696154, 0.16814657416658454, 0.10203622138102808, 0.009808198359202408, 0.09036770189831556, 0.11032745591939544, 0.1127084418051588, 0.11396085163868677, 0.113659710397816, 0.11473199836764561, 0.11432390977020387, 0.11138004306038304, 0.11040344483064321, 0.1082447968703826, 0.11075243094155893, 0.1141437879065055, 0.11444211474325597, 0.11292233651830061, 0.0634394832754035, 0.07168850175196653, 0.07995440665325132, 0.08030902157240755, 0.08014578613343087, 0.08039345369601622, 0.08039345369601622, 0.08036530965481331, 0.07978554240603405, 0.07886804666282032, 0.07878361453921168, 0.07831360905112364, 0.07808845672150064, 0.0768416756962132, 0.07659400813362788, 0.07602831290545006, 0.07528249581357385, 0.07481249032548581, 0.07450290587225417, 0.07329834090877108, 0.07293528277725397, 0.07184329397858236, 0.07134233004517115, 0.07090609740652656, 0.07108059046198442, 0.07295498360609598, 0.07296905562669742, 0.07318013593571901, 0.07498416897682339, 0.07512207477871746, 0.07659400813362788, 0.07673191393552198, 0.0773426396296244, 0.07770288335702119, 0.0778436035630356, 0.07787456200835877, 0.07811941516682377, 0.07826013537283819, 0.07867385277852046, 0.07931553691794602, 0.08003320996861937, 0.07997692188621364, 0.08014578613343087, 0.08033998001773074, 0.08044974177842196, 0.08045255618254224, 0.08058764758031603, 0.07997692188621364, 0.07912134303364618, 0.07182640755386066, 0.058750686011004305, 0.04329116417826434, 0.3138482754738753, 0.680255547894122, 0.6823944950255406, 0.13227980805763898, 0.04190366294696252, 0.07775917143942698, 0.18390723724019528, 0.18401981340500675, 0.18396352532260105, 0.18475155847628158, 0.00633803807888772, 0.09822270379803832, 0.1257560193068122, 0.12663692779646227, 0.12739400250481966, 0.12768670053332956, 0.12768670053332956, 0.12522691133219818, 0.12483008035123758, 0.12348479518174013, 0.12268269000745816, 0.12273053487750299, 0.12338347663340979, 0.12372401953196457, 0.12523254014043875, 0.12634422976795237, 0.12657782530993625, 0.12698309950325762, 0.12750939307375145, 0.12593332676639038, 0.1117768740413436, 0.08979637786189718, 0.09148783473819005, 0.09137244416925827, 0.08963595682704076, 0.08971476014240884, 0.08942206211389894, 0.08905337517414125, 0.08881977963215734, 0.08878037797447333, 0.08702700420753415, 0.0818710158591672, 0.08230161968957123, 0.08374540900327879, 0.08837228937703165, 0.08874097631678934, 0.08913499289362958, 0.08969787371768712, 0.09002997340388105, 0.09122328075088298, 0.09045494842604448, 0.08829067165754328, 0.03766798474592964, 0.4101008963877123, 0.6579091791790382, 0.8839339740793378, 0.8847220072330185, 0.883033364760846, 0.854326442733912, 0.5307825450656458, 0.3474522606701096, 0.11846108382702672, 0.27045016393904, 0.2832838467275516, 0.28992584045143044, 0.2896444000394016, 0.2888000788033154, 0.2884623503088808, 0.29017913682225627, 0.29082644976992245, 0.2892222394213585, 0.24495166260923404, 0.20608474170806182, 0.16583876278794865, 0.08573800712044241, 0.00029551243263017946, 0.1772370994751136, 0.20740751164459706, 0.21095366083615943, 0.2119949903606659, 0.21328961625599815, 0.2127830235143464, 0.21255787118472338, 0.21067222042413067, 0.2103344919296962, 0.20957460281721849, 0.20597216554325035, 0.20791410438624877, 0.2080266805510603, 0.20808296863346606, 0.20903986603436384, 0.20974346706443578, 0.2110099489185652, 0.21289559967915786, 0.21354291262682404, 0.2138524970800557, 0.2140495053684758, 0.21343033646201257, 0.2112351012481882, 0.20025892517906643, 0.15044397224997533, 0.11747322798080576, 0.10094704698647677, 0.16254590996721222, 0.16400940010976175, 0.1641501203157761, 0.16448784881021064, 0.16350280736810993, 0.1628836384616467, 0.16282735037924093, 0.1624614778436035, 0.16127942811308274, 0.1611668519482712, 0.15998480221775044, 0.15764884679791172, 0.15511588308965277, 0.1545248582243924, 0.15339909657627734, 0.1528643597934227, 0.15244219917537957, 0.15249848725778534, 0.15387754527672623, 0.1560446364493477, 0.15694524576783983, 0.1572829742622743, 0.15894347269324396, 0.15970336180572167, 0.1596470737233159, 0.16018181050617056, 0.16136386023669133, 0.16150458044270574, 0.16220818147277768, 0.1624614778436035, 0.16260219804961792, 0.16251776592600928, 0.1630806467500668, 0.16336208716209558, 0.16384053586254452, 0.16434712860419629, 0.16448784881021064, 0.1399152864359793, 0.08602226193659146, 0.00044186144688512785, 0.9730942966100501, 0.9922885327104117, 0.9965382829320463, 0.9957502497783656, 0.9980862051982043, 0.9999999999999999, 0.9992401108875223, 0.5788244233989558, 0.13602015113350127, 0.07726946512249694, 0.3097111014170525, 0.3096829573758496, 0.3095703812110381, 0.3104428464883272, 0.3099925418290812, 0.30948594908742943, 0.30858533976893743, 0.31100572731238474, 0.31038655840592144, 0.3100206858702841, 0.3054050631130123, 0.2667070064590574, 0.10306066448081277, 0.011744508393960296, 0.18247189113884862, 0.20819554479827754, 0.22806523788750824, 0.22935986378284054, 0.2308796420077959, 0.23107665029621596, 0.22997903268930378, 0.22699576432179894, 0.226067010962104, 0.22356219129504804, 0.22297116642978762, 0.22502568143759763, 0.225841858632481, 0.22623587520932129, 0.22657360370375576, 0.2302604731013326, 0.22899399124720318, 0.22823410213472548, 0.22792451768149388, 0.1293866006219833, 0.06266270773820412, 0.030544727917481668, 0.14650380648157266, 0.1767586507746647, 0.17794070050518554, 0.17979820722457537, 0.1797419191421696, 0.1783065730408229, 0.17656164248624454, 0.1763927782390273, 0.1763083461154187, 0.17490114405527488, 0.17442269535482594, 0.17152385911092968, 0.17076396999845203, 0.16946934410311978, 0.1695256321855255, 0.16918790369109096, 0.16730225293049825, 0.16724596484809254, 0.1686531669082363, 0.1694974881443226, 0.17293106117107357, 0.17473227980805764, 0.1757173212501583, 0.17636463419782447, 0.17729338755751942, 0.17729338755751942, 0.17782812434037396, 0.1778562683815769, 0.1790664621533006, 0.17878502174127178, 0.17858801345285166, 0.1772370994751136, 0.14689782305841295, 0.09308923068263372, 0.4791382294583678, 0.5330059243206732, 0.594922814967001, 0.5957389921618844, 0.5908137849513811, 0.5067475338783896, 0.20225715210447068, 0.01735924461393412, 0.09361270984900724, 0.09335097026582043, 0.09264736923574854, 0.09273461576347747, 0.09289503679833389, 0.09403487046705036, 0.09412211699477924, 0.09357893699956374, 0.09196628343863897, 0.08855804004897064, 0.07874421288152764, 0.045759396591756596, 0.04963483106539268, 0.05123341260571604, 0.05167527405260122, 0.051897611978103914, 0.05211713549948635, 0.05284325176252058, 0.05134317436640726, 0.048855241124072996, 0.048469667759593604, 0.04885805552819328, 0.05085065364535693, 0.05123341260571604, 0.05217905239013268, 0.05256744015873238, 0.05002040442987207, 0.047251030775509045, 0.02487933242334267, 0.02653138764195148, 0.027158999760775654, 0.027158999760775654, 0.02711678369897133, 0.02719840141845964, 0.02678186960865711, 0.026227431996960426, 0.025101670348845362, 0.02483993076565863, 0.02350871761676257, 0.02220283410494911, 0.02057610852342287, 0.020120175055936246, 0.020258080857830374, 0.02131066799881795, 0.02234073990684321, 0.023331410157184457, 0.02496376454695129, 0.025020052629357026, 0.025059454287041066, 0.02557449024105371, 0.026128927852750367, 0.026089526195066326, 0.026880373752867198, 0.026961991472355507, 0.027060495616565594, 0.02719840141845964, 0.027308163179150885, 0.027240617480263962, 0.026961991472355507, 0.02653138764195148, 0.02543658443915961, 0.023058412957516566, 0.0019728972883216456, 0.117231189226461, 0.45634155608403804, 0.4797573983648311, 0.4774777310273981, 0.479785542406034, 0.36751896204776036, 0.1370277078085642, 0.10487032633015775, 0.10442846488327256, 0.10472397731590277, 0.10438343441734796, 0.10444816571211463, 0.10489002715899973, 0.10484499669307512, 0.1047436781447448, 0.10444816571211463, 0.10480840943951142, 0.06000872465277288, 0.06054909024386809, 0.061798685673275816, 0.06195910670813223, 0.06215048618831179, 0.06140748350055583, 0.059108115334280814, 0.05875912922336518, 0.05767558363705444, 0.05919254745788943, 0.0598511180220368, 0.06094029241658808, 0.06179024246091494, 0.06203228121525972, 0.06203228121525972, 0.06195066349577133, 0.06203228121525972, 0.06164107904253971, 0.0612076608080154, 0.060974065266031524, 0.06047591573674063, 0.06055753345622894, 0.05976668589842818, 0.05872535637392173, 0.02542814122679876, 0.0029917115798657468, 0.007548231850611403, 0.02183133276107116, 0.035478378340345895, 0.035514965593909625, 0.035388317408496656, 0.03538268860025609, 0.03515753627063309, 0.03508154735938532, 0.03460028425481612, 0.03438076073343371, 0.03434417347986998, 0.03411902115024695, 0.033663087682760384, 0.033001702714492764, 0.0328075088301929, 0.03271744789834369, 0.03234876095858602, 0.030404007711467285, 0.029818611654447436, 0.0296244177701476, 0.02948932637237378, 0.028180628456440066, 0.028315719854213883, 0.0288054261711439, 0.029323276529276826, 0.029458367927050644, 0.03092185806960021, 0.03138060594120709, 0.03192941474466318, 0.03255139805524676, 0.033288771934762124, 0.033325359188325826, 0.03341542012017504, 0.03387416799178192, 0.03400925938955571, 0.03400363058131517, 0.034324472651027976, 0.034332915863388797, 0.0344286056034786, 0.034693159590785616, 0.03517442269535481, 0.03514346425003165, 0.03524196839424171, 0.03517442269535481, 0.034369503116952554, 0.03208702137539929, 0.001398758847782955], \"yaxis\": \"y\"}, {\"hovertemplate\": \"variable=GB<br>index=%{x}<br>value=%{y}<extra></extra>\", \"legendgroup\": \"GB\", \"line\": {\"color\": \"#00cc96\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"GB\", \"showlegend\": true, \"type\": \"scattergl\", \"x\": [2, 8, 10, 15, 23, 24, 28, 38, 42, 43, 44, 47, 51, 64, 80, 84, 87, 90, 92, 94, 99, 100, 103, 109, 117, 120, 128, 136, 146, 148, 159, 164, 170, 175, 177, 179, 184, 187, 189, 194, 200, 204, 208, 209, 218, 222, 225, 226, 227, 228, 230, 233, 238, 239, 242, 243, 249, 251, 252, 256, 257, 262, 263, 267, 270, 272, 273, 274, 276, 277, 285, 289, 290, 291, 293, 295, 302, 303, 306, 307, 309, 310, 313, 316, 328, 335, 346, 349, 350, 361, 367, 376, 380, 396, 400, 405, 408, 411, 412, 415, 423, 424, 430, 431, 432, 443, 446, 449, 451, 452, 453, 456, 465, 471, 482, 490, 498, 512, 513, 514, 516, 518, 520, 532, 550, 552, 557, 574, 579, 583, 589, 591, 607, 612, 616, 620, 626, 629, 642, 643, 644, 652, 657, 659, 665, 669, 670, 680, 683, 686, 688, 696, 700, 702, 708, 709, 710, 712, 714, 720, 722, 725, 727, 733, 740, 741, 744, 745, 746, 756, 765, 766, 767, 770, 771, 773, 777, 780, 782, 783, 785, 789, 791, 793, 794, 801, 805, 808, 811, 817, 827, 831, 832, 834, 842, 843, 849, 858, 862, 865, 867, 870, 871, 879, 883, 888, 889, 890, 893, 896, 897, 899, 904, 906, 911, 912, 913, 914, 917, 919, 922, 925, 926, 942, 944, 947, 957, 958, 959, 963, 969, 971, 972, 979, 983, 986, 995, 996, 998, 1001, 1005, 1007, 1009, 1016, 1019, 1023, 1027, 1029, 1033, 1036, 1041, 1042, 1046, 1049, 1052, 1054, 1061, 1065, 1067, 1074, 1078, 1082, 1084, 1087, 1088, 1104, 1106, 1107, 1108, 1114, 1116, 1119, 1123, 1126, 1127, 1138, 1140, 1149, 1160, 1161, 1162, 1171, 1173, 1179, 1180, 1182, 1183, 1185, 1189, 1196, 1199, 1204, 1209, 1215, 1220, 1224, 1228, 1229, 1234, 1235, 1243, 1254, 1255, 1258, 1262, 1264, 1273, 1274, 1278, 1290, 1295, 1298, 1302, 1304, 1316, 1318, 1327, 1328, 1331, 1334, 1339, 1344, 1345, 1346, 1347, 1353, 1354, 1358, 1360, 1362, 1363, 1367, 1376, 1385, 1394, 1396, 1404, 1405, 1407, 1415, 1423, 1424, 1426, 1432, 1434, 1438, 1439, 1441, 1446, 1452, 1468, 1470, 1479, 1482, 1491, 1492, 1493, 1502, 1508, 1511, 1520, 1522, 1524, 1528, 1536, 1537, 1544, 1545, 1546, 1551, 1552, 1557, 1560, 1561, 1562, 1563, 1565, 1567, 1572, 1576, 1583, 1588, 1594, 1597, 1606, 1609, 1611, 1616, 1617, 1629, 1630, 1631, 1633, 1635, 1650, 1652, 1656, 1657, 1660, 1668, 1677, 1678, 1681, 1692, 1695, 1697, 1701, 1704, 1705, 1706, 1707, 1710, 1711, 1712, 1713, 1714, 1716, 1717, 1720, 1721, 1722, 1723, 1733, 1737, 1740, 1741, 1744, 1745, 1750, 1752, 1753, 1756, 1760, 1769, 1771, 1772, 1774, 1780, 1784, 1785, 1786, 1792, 1795, 1797, 1799, 1800, 1807, 1808, 1810, 1814, 1820, 1821, 1822, 1827, 1830, 1831, 1834, 1835, 1836, 1839, 1844, 1846, 1851, 1852, 1857, 1861, 1864], \"xaxis\": \"x\", \"y\": [0.2585097465903058, 0.33053474732593535, 0.4480726660347786, 0.7205746010478079, 0.7296437002929632, 0.7303322343250236, 0.7290718353298782, 0.09885304606556863, 0.10128346934173585, 0.10128346934173585, 0.10128346934173585, 0.10128346934173585, 0.1414635152584159, 0.16448069796976192, 0.1414635152584159, 0.13851980747689943, 0.07465947220933883, 0.07272057822899333, 0.07272057822899333, 0.07438632367913803, 0.0781264563797768, 0.0781264563797768, 0.0781264563797768, 0.08809674037990042, 0.09502927443600442, 0.09414752413429975, 0.11463770155063299, 0.093486062148911, 0.09459613641347994, 0.0877622510779276, 0.0781264563797768, 0.0781264563797768, 0.04442017040797108, 0.05022143258021258, 0.05669952999475439, 0.05669952999475439, 0.05669952999475439, 0.05669952999475439, 0.05831532731918718, 0.05831532731918718, 0.05831532731918718, 0.05831532731918718, 0.05831532731918718, 0.05637643333884165, 0.0604463703875337, 0.0604463703875337, 0.0604463703875337, 0.0604463703875337, 0.0604463703875337, 0.0604463703875337, 0.06020979979260113, 0.059598152970495866, 0.05893669098510709, 0.05893669098510709, 0.05833676305131921, 0.05833676305131921, 0.05893669098510709, 0.05893669098510709, 0.059598152970495866, 0.06020979979260113, 0.059877910034736526, 0.0604463703875337, 0.0604463703875337, 0.0604463703875337, 0.0604463703875337, 0.058042178788986376, 0.058042178788986376, 0.058042178788986376, 0.058042178788986376, 0.05637643333884165, 0.05831532731918718, 0.05831532731918718, 0.05831532731918718, 0.05831532731918718, 0.05831532731918718, 0.05831532731918718, 0.05669952999475439, 0.05669952999475439, 0.05669952999475439, 0.05669952999475439, 0.05669952999475439, 0.05669952999475439, 0.027480245089547772, 0.3068124631294773, 0.7081782605225132, 0.6995354143815429, 0.13868874237239723, 0.05763757423021143, 0.08410094191810943, 0.16329255714077903, 0.16630167327478337, 0.16630167327478337, 0.16630167327478337, 0.05763757423021143, 0.0829413958134618, 0.09280868246334587, 0.09280868246334587, 0.10511939575529997, 0.10511939575529997, 0.10511939575529997, 0.10555253377782445, 0.10528243029822507, 0.10400932149073103, 0.10400932149073103, 0.10400932149073103, 0.10400932149073103, 0.10467078347611977, 0.10555253377782445, 0.10511939575529997, 0.10511939575529997, 0.10511939575529997, 0.10511939575529997, 0.09280868246334587, 0.09071118912314644, 0.0674015475201711, 0.068833605726628, 0.0702706448674641, 0.0702706448674641, 0.0702706448674641, 0.0702706448674641, 0.06999749633726329, 0.06999749633726329, 0.06999749633726329, 0.07240168793581062, 0.06886002239313924, 0.06945995032692712, 0.06945995032692712, 0.07240168793581062, 0.06999749633726329, 0.06833175088711857, 0.0702706448674641, 0.0702706448674641, 0.068833605726628, 0.068833605726628, 0.0674015475201711, 0.044851493734591186, 0.364093238416479, 0.517714849856557, 0.9026589993998115, 0.9039193983949569, 0.9039193983949569, 0.8888589155536502, 0.8101818355520768, 0.7181227395657762, 0.19988924793350196, 0.22281267245317168, 0.22281267245317168, 0.2696737030302099, 0.2726828191642142, 0.2726828191642142, 0.28407714388999705, 0.28407714388999705, 0.2726828191642142, 0.2726828191642142, 0.26798202634131796, 0.24848753259358328, 0.2453628096767815, 0.2287155167949569, 0.22281267245317168, 0.14575857113660665, 0.14575857113660665, 0.14575857113660665, 0.18701767411419892, 0.18701767411419892, 0.18701767411419892, 0.18701767411419892, 0.18596424392990044, 0.1941594797003581, 0.19609937779249906, 0.19482626898500502, 0.19482626898500502, 0.19482626898500502, 0.19482626898500502, 0.19548773097039382, 0.19548773097039382, 0.19609937779249906, 0.18701767411419892, 0.18701767411419892, 0.18701767411419892, 0.18701767411419892, 0.18701767411419892, 0.18701767411419892, 0.18701767411419892, 0.18701767411419892, 0.18701767411419892, 0.09448419285978374, 0.10317156006066144, 0.13697943962875744, 0.14350465141668542, 0.14350465141668542, 0.14350465141668542, 0.14350465141668542, 0.14350465141668542, 0.14350465141668542, 0.14323150288648465, 0.14323150288648465, 0.14563569448503197, 0.14563569448503197, 0.15431779302547446, 0.15370614620336917, 0.15304468421798037, 0.15304468421798037, 0.15304468421798037, 0.15304468421798037, 0.15304468421798037, 0.15370614620336917, 0.15370614620336917, 0.15431779302547446, 0.1450672341322348, 0.14563569448503197, 0.14563569448503197, 0.14563569448503197, 0.14563569448503197, 0.14563569448503197, 0.14323150288648465, 0.14323150288648465, 0.14323150288648465, 0.14323150288648465, 0.14350465141668542, 0.14350465141668542, 0.14350465141668542, 0.14350465141668542, 0.14350465141668542, 0.14350465141668542, 0.13697943962875744, 0.10317156006066144, 0.986149308595025, 0.993628319319442, 0.993628319319442, 1.0, 1.0, 0.9964587727234804, 0.9964587727234804, 0.6832486176640491, 0.17303805807489045, 0.12527446104728643, 0.28287503118622803, 0.28287503118622803, 0.28287503118622803, 0.2836193677133174, 0.2902341035570244, 0.2902341035570244, 0.2911087176601682, 0.2902341035570244, 0.2836193677133174, 0.28287503118622803, 0.28287503118622803, 0.26531055832537864, 0.1613934331522722, 0.08761806919064927, 0.16075557629405587, 0.1816272508614946, 0.20183954741814938, 0.20183954741814938, 0.20183954741814938, 0.20183954741814938, 0.2007861172338509, 0.21030960427434428, 0.21030960427434428, 0.20964814228895548, 0.2090482143551676, 0.20964814228895548, 0.20964814228895548, 0.21030960427434428, 0.21030960427434428, 0.20183954741814938, 0.20183954741814938, 0.20183954741814938, 0.20183954741814938, 0.1569691853639599, 0.09411320837073964, 0.0001809629650350586, 0.11074157160010542, 0.1591557540115974, 0.16230568262903364, 0.16257792079174258, 0.16257792079174258, 0.16257792079174258, 0.16230477226154175, 0.16302607083731915, 0.16302607083731915, 0.16302607083731915, 0.16302607083731915, 0.16492844006638022, 0.16391698666392315, 0.16391698666392315, 0.16391698666392315, 0.16311135523644507, 0.16255061828598857, 0.16218988740785753, 0.16301609188360563, 0.16391698666392315, 0.1644485925454526, 0.16302607083731915, 0.16302607083731915, 0.16302607083731915, 0.16230477226154175, 0.16230477226154175, 0.16257792079174258, 0.16257792079174258, 0.16257792079174258, 0.16257792079174258, 0.16257792079174258, 0.16257792079174258, 0.14343570309029988, 0.10885266279575204, 0.3926203821049883, 0.3530882652157251, 0.45688944562380684, 0.6125620810438458, 0.6119902160807609, 0.5337867840063347, 0.5190107439833129, 0.04932367159708323, 0.055910069597358975, 0.07582832970410153, 0.07790805810057147, 0.07492298993721275, 0.07492298993721275, 0.07790805810057147, 0.08011734604277651, 0.055910069597358975, 0.055910069597358975, 0.055910069597358975, 0.055910069597358975, 0.019355872420970283, 0.019355872420970283, 0.019355872420970283, 0.019355872420970283, 0.019355872420970283, 0.01741697844062476, 0.021180217230968884, 0.03108673892437569, 0.03221319242179718, 0.04835031604634529, 0.03198682845888132, 0.03251065219708049, 0.03108673892437569, 0.03108673892437569, 0.03108673892437569, 0.019355872420970283, 0.019355872420970283, 0.019355872420970283, 0.0023381096933970946, 0.0023381096933970946, 0.0023381096933970946, 0.002931168489124031, 0.0032105829439158518, 0.0032105829439158518, 0.004100520847243216, 0.0038273723170424157, 0.0038273723170424157, 0.0038273723170424157, 0.0038273723170424157, 0.0072718231504592945, 0.0072718231504592945, 0.0072718231504592945, 0.0072718231504592945, 0.0032589119642452424, 0.0038273723170424157, 0.0038273723170424157, 0.0038273723170424157, 0.0038273723170424157, 0.004100520847243216, 0.004100520847243216, 0.004100520847243216, 0.004100520847243216, 0.0032105829439158518, 0.0032105829439158518, 0.0032105829439158518, 0.0032105829439158518, 0.0023381096933970946, 0.0023381096933970946, 0.0023381096933970946, 0.0023381096933970946, 0.0023381096933970946, 0.0, 0.10504410226421709, 0.43013141829003465, 0.5121804648279, 0.4992116218277194, 0.5121804648279, 0.3514635901040063, 0.10504410226421709, 0.0711673388363323, 0.08709512116187437, 0.09151495422122755, 0.08917484955834429, 0.08917484955834429, 0.0905814208212852, 0.09151495422122755, 0.09151495422122755, 0.08709512116187437, 0.07975899470816658, 0.02734256998424539, 0.02734256998424539, 0.02813463556088, 0.034968520896432376, 0.044875042589839154, 0.044875042589839154, 0.0422784001184473, 0.04087182885550639, 0.04087182885550639, 0.042939862103836074, 0.04382161240554075, 0.044875042589839154, 0.044875042589839154, 0.044875042589839154, 0.044875042589839154, 0.044875042589839154, 0.034968520896432376, 0.02813463556088, 0.02603714222068057, 0.024371396770535875, 0.02734256998424539, 0.02734256998424539, 0.02734256998424539, 0.02734256998424539, 0.02330146791776977, 0.02330146791776977, 0.011086465655300082, 0.011086465655300082, 0.013040127218813813, 0.01391260046933257, 0.01391260046933257, 0.01391260046933257, 0.01391260046933257, 0.01372985777095767, 0.01372985777095767, 0.01372985777095767, 0.01372985777095767, 0.01372985777095767, 0.01372985777095767, 0.01345670924075687, 0.01345670924075687, 0.01345670924075687, 0.01345670924075687, 0.01345670924075687, 0.011121923616896151, 0.011121923616896151, 0.010460461631507378, 0.008453962434778556, 0.008453962434778556, 0.009053890368566464, 0.009053890368566464, 0.010460461631507378, 0.01345670924075687, 0.01345670924075687, 0.01345670924075687, 0.01345670924075687, 0.01345670924075687, 0.01345670924075687, 0.01345670924075687, 0.01372985777095767, 0.01372985777095767, 0.01372985777095767, 0.01372985777095767, 0.01372985777095767, 0.01372985777095767, 0.01372985777095767, 0.01391260046933257, 0.01391260046933257, 0.013040127218813813, 0.013040127218813813, 0.013040127218813813, 0.012159146257002346, 0.011086465655300082], \"yaxis\": \"y\"}, {\"hovertemplate\": \"variable=RF<br>index=%{x}<br>value=%{y}<extra></extra>\", \"legendgroup\": \"RF\", \"line\": {\"color\": \"#ab63fa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"RF\", \"showlegend\": true, \"type\": \"scattergl\", \"x\": [2, 8, 10, 15, 23, 24, 28, 38, 42, 43, 44, 47, 51, 64, 80, 84, 87, 90, 92, 94, 99, 100, 103, 109, 117, 120, 128, 136, 146, 148, 159, 164, 170, 175, 177, 179, 184, 187, 189, 194, 200, 204, 208, 209, 218, 222, 225, 226, 227, 228, 230, 233, 238, 239, 242, 243, 249, 251, 252, 256, 257, 262, 263, 267, 270, 272, 273, 274, 276, 277, 285, 289, 290, 291, 293, 295, 302, 303, 306, 307, 309, 310, 313, 316, 328, 335, 346, 349, 350, 361, 367, 376, 380, 396, 400, 405, 408, 411, 412, 415, 423, 424, 430, 431, 432, 443, 446, 449, 451, 452, 453, 456, 465, 471, 482, 490, 498, 512, 513, 514, 516, 518, 520, 532, 550, 552, 557, 574, 579, 583, 589, 591, 607, 612, 616, 620, 626, 629, 642, 643, 644, 652, 657, 659, 665, 669, 670, 680, 683, 686, 688, 696, 700, 702, 708, 709, 710, 712, 714, 720, 722, 725, 727, 733, 740, 741, 744, 745, 746, 756, 765, 766, 767, 770, 771, 773, 777, 780, 782, 783, 785, 789, 791, 793, 794, 801, 805, 808, 811, 817, 827, 831, 832, 834, 842, 843, 849, 858, 862, 865, 867, 870, 871, 879, 883, 888, 889, 890, 893, 896, 897, 899, 904, 906, 911, 912, 913, 914, 917, 919, 922, 925, 926, 942, 944, 947, 957, 958, 959, 963, 969, 971, 972, 979, 983, 986, 995, 996, 998, 1001, 1005, 1007, 1009, 1016, 1019, 1023, 1027, 1029, 1033, 1036, 1041, 1042, 1046, 1049, 1052, 1054, 1061, 1065, 1067, 1074, 1078, 1082, 1084, 1087, 1088, 1104, 1106, 1107, 1108, 1114, 1116, 1119, 1123, 1126, 1127, 1138, 1140, 1149, 1160, 1161, 1162, 1171, 1173, 1179, 1180, 1182, 1183, 1185, 1189, 1196, 1199, 1204, 1209, 1215, 1220, 1224, 1228, 1229, 1234, 1235, 1243, 1254, 1255, 1258, 1262, 1264, 1273, 1274, 1278, 1290, 1295, 1298, 1302, 1304, 1316, 1318, 1327, 1328, 1331, 1334, 1339, 1344, 1345, 1346, 1347, 1353, 1354, 1358, 1360, 1362, 1363, 1367, 1376, 1385, 1394, 1396, 1404, 1405, 1407, 1415, 1423, 1424, 1426, 1432, 1434, 1438, 1439, 1441, 1446, 1452, 1468, 1470, 1479, 1482, 1491, 1492, 1493, 1502, 1508, 1511, 1520, 1522, 1524, 1528, 1536, 1537, 1544, 1545, 1546, 1551, 1552, 1557, 1560, 1561, 1562, 1563, 1565, 1567, 1572, 1576, 1583, 1588, 1594, 1597, 1606, 1609, 1611, 1616, 1617, 1629, 1630, 1631, 1633, 1635, 1650, 1652, 1656, 1657, 1660, 1668, 1677, 1678, 1681, 1692, 1695, 1697, 1701, 1704, 1705, 1706, 1707, 1710, 1711, 1712, 1713, 1714, 1716, 1717, 1720, 1721, 1722, 1723, 1733, 1737, 1740, 1741, 1744, 1745, 1750, 1752, 1753, 1756, 1760, 1769, 1771, 1772, 1774, 1780, 1784, 1785, 1786, 1792, 1795, 1797, 1799, 1800, 1807, 1808, 1810, 1814, 1820, 1821, 1822, 1827, 1830, 1831, 1834, 1835, 1836, 1839, 1844, 1846, 1851, 1852, 1857, 1861, 1864], \"xaxis\": \"x\", \"y\": [0.24508064284414013, 0.26588278999159864, 0.4066088845821516, 0.6888093608395779, 0.7178321208888766, 0.718299979683495, 0.7108757392662671, 0.050953673014288714, 0.11742230897829528, 0.1405298306914257, 0.14447232837816723, 0.08078073437899208, 0.16042529979370004, 0.16262479022779683, 0.1625061561271273, 0.13747598056092383, 0.04447923472982468, 0.07185663215273089, 0.09012099840015389, 0.028590349408115223, 0.10326642396892269, 0.10493899429624745, 0.10814981225999656, 0.10839279242096964, 0.10656644483100128, 0.10488219910853139, 0.10333947700111737, 0.10396638884233647, 0.10740173877241796, 0.1077147395737825, 0.10753490259048723, 0.09834497236471917, 0.0551131479720581, 0.034481962272244204, 0.0713724657867659, 0.07163517283328522, 0.07389619542983009, 0.07409467241140222, 0.07388208260625925, 0.07371843452638205, 0.0726725553476979, 0.07237026713458466, 0.07216546185083145, 0.07164898269506403, 0.07075359075468204, 0.07015303936327322, 0.06984303496092947, 0.06960011210425252, 0.06932493782140547, 0.0684984488508461, 0.06702677412526178, 0.06651127983211325, 0.06553385844938098, 0.06523120651974393, 0.06470796525406558, 0.06470796525406558, 0.06552880572594758, 0.06597719005807609, 0.06611997010534315, 0.06702677412526178, 0.0676996648499043, 0.06976830258134573, 0.06976775321784584, 0.07048246565970195, 0.07088476739236652, 0.07103985412921474, 0.07120326798539234, 0.0712699943391405, 0.07150198296780882, 0.07164898269506403, 0.0723351467048943, 0.0729264133795621, 0.07297915227557261, 0.07305283328707854, 0.07361668624950207, 0.0736510986633026, 0.07389619542983009, 0.07397959767203041, 0.07373352307906622, 0.07163517283328522, 0.0713724657867659, 0.06037400676559157, 0.07282722864115326, 0.3180542778052502, 0.6796510497983654, 0.6816618054548227, 0.13207363603947278, 0.04658595850667943, 0.07069038080092421, 0.17842131144310444, 0.17857293103324204, 0.17837305270115564, 0.17928236295524794, 0.022779054434881124, 0.08709261316223882, 0.11912159068504577, 0.12044325270872994, 0.12128812072990333, 0.12130254388972714, 0.12138465005353224, 0.11929071768482993, 0.11837331781640345, 0.11718278649224792, 0.11689681437508817, 0.11611274952723596, 0.11749689928111198, 0.11772243430942161, 0.11929071768482993, 0.12030740979641921, 0.12080449851795708, 0.1209680861813803, 0.12140973133884778, 0.11999294655153012, 0.10849907484108254, 0.08152221699299281, 0.08471239871222933, 0.08448608226169993, 0.08320857296139009, 0.08302861142412846, 0.08297143973297913, 0.08256742076578805, 0.08240445018912429, 0.08229150294785884, 0.08035590637487683, 0.07540617296938076, 0.07576608467776288, 0.07733227834071951, 0.08197314206365375, 0.08238272333628424, 0.08284229195153167, 0.08347877881968649, 0.08367498304657128, 0.08491960773191795, 0.08444149620418478, 0.08152221699299281, 0.04185053985359172, 0.31238350149073757, 0.46476642273995805, 0.8863251267787696, 0.8869958711970461, 0.8861497566890345, 0.8759666648690806, 0.8320306532167926, 0.7856721623207356, 0.1406151667337404, 0.2101445672309226, 0.16762015300786967, 0.28328801234427337, 0.2854771501179282, 0.2856806135204426, 0.28466949105492445, 0.28466949105492445, 0.28573632182088915, 0.2857098718627151, 0.2812384223714305, 0.2826088001506658, 0.27790617281546415, 0.24574946875247136, 0.16762015300786967, 0.14892047254372867, 0.13223254985084235, 0.10309616769563287, 0.19397191671783773, 0.20750670840320054, 0.20799723317809035, 0.20799958218064232, 0.20730850184107522, 0.20701056686011152, 0.2063192213105793, 0.20158109372712982, 0.2011287307719282, 0.20130484913263522, 0.20171275153146515, 0.2026891362273854, 0.20296057389125358, 0.20413065184399284, 0.20697533182182845, 0.20799723317809035, 0.20797410308037925, 0.20812693505893107, 0.20831277620641084, 0.2066484125037866, 0.2037076270646769, 0.19397191671783773, 0.17046191662326213, 0.089753985694944, 0.11385489125092105, 0.13431909541161988, 0.15159725566978974, 0.15888988564160436, 0.15804295656012765, 0.15781236596685008, 0.15756003762817952, 0.15711547942339346, 0.1560885485495707, 0.15599657752222465, 0.1548591582823887, 0.15303981791858293, 0.14986801565589383, 0.14939255575422647, 0.1486030493661876, 0.14788063162765305, 0.1476099280270831, 0.1460315850414792, 0.14756707293818158, 0.14894033487579308, 0.14901532772945023, 0.14986801565589383, 0.1520085300238965, 0.15309954699154235, 0.15331239219658885, 0.153834661657179, 0.154869823511719, 0.15506869783464985, 0.1558343732129258, 0.15584943335025667, 0.15607154669642112, 0.15630102909091456, 0.15663469164295438, 0.15710766520119332, 0.1575111632202391, 0.15804295656012765, 0.1583060827331104, 0.1516408846034827, 0.13431909541161988, 0.11385489125092105, 0.9402020955821152, 0.9805414169570775, 0.9960786480721199, 0.9996099898020611, 0.9996099898020611, 0.9999999999999999, 0.9994416761917694, 0.6272020661674904, 0.1584544558691162, 0.054275243133248774, 0.3052291331867134, 0.30552380040202964, 0.30539472162929737, 0.30600298542122295, 0.30580101382476466, 0.30477159660751574, 0.3040953159312051, 0.306044467101373, 0.30600298542122295, 0.30540199595978135, 0.3018456723982632, 0.2685148890271758, 0.11865304423887427, 0.041862132235311156, 0.1626358722156435, 0.18943194833759144, 0.22254673239939868, 0.22461288378741237, 0.22589286522347343, 0.22604912835595445, 0.2249070963570741, 0.2219983776443383, 0.22097315169172252, 0.21843185770682125, 0.2179984856795115, 0.21971976939444135, 0.22040590546250746, 0.22097315169172252, 0.22147906811640009, 0.2251999805089613, 0.22382776757148234, 0.22279692958656003, 0.22254673239939868, 0.1338878470260472, 0.07109731376261214, 0.012620182982684663, 0.12212076196276386, 0.16683365343781534, 0.17090099871584827, 0.17384338015064363, 0.17367958931744307, 0.17251938096413674, 0.17106132287592765, 0.1708138393550214, 0.17067298444795034, 0.16886877999575003, 0.16872324873336828, 0.16579262854867974, 0.16500690613854194, 0.16426895940934447, 0.163923277162794, 0.16344133923709517, 0.16181544992304667, 0.1612471665334305, 0.16287179068769417, 0.16426895940934447, 0.16731537892491857, 0.16886877999575003, 0.170162010090358, 0.17067298444795034, 0.1714284160913058, 0.171527225747049, 0.172279815855059, 0.17238529838297204, 0.17394410310685213, 0.17400400267193314, 0.1738730457796491, 0.17245600525415059, 0.15130274270924907, 0.10994032951002738, 0.40147525316576216, 0.37486395640425496, 0.40088250889271493, 0.5934007340149898, 0.5914434939408737, 0.5632071003830567, 0.5258295207703743, 0.047712447307219824, 0.08711148095693255, 0.08718686215375324, 0.08673905039099719, 0.08657143769275716, 0.08627767976875084, 0.08668709291720655, 0.08754159469037887, 0.08711148095693255, 0.08678762170183427, 0.0865815251432333, 0.0865439979331051, 0.02458533320788278, 0.0406181849178667, 0.027645280326057675, 0.04022144894107607, 0.043259590928877684, 0.04475965164538426, 0.04577152332905468, 0.045131696236101804, 0.04270437386595821, 0.04097820555177023, 0.04135057455110647, 0.043141752458104504, 0.04330360441702105, 0.04408653928098452, 0.04567039308611265, 0.042815601031160805, 0.04022144894107607, 0.027645280326057675, 0.016972234879815307, 0.02007416852425359, 0.0202628029009784, 0.02026241171627896, 0.019999514286891673, 0.019938253152791297, 0.019451173265918498, 0.01794685214075109, 0.017802177262997654, 0.01685662227839621, 0.015720346756536524, 0.013818547404952669, 0.013746544793657611, 0.01345178049255033, 0.013814199855874315, 0.014778562489338065, 0.0157114361752827, 0.01767379574891431, 0.017802177262997654, 0.01794685214075109, 0.018094894237888598, 0.018747796655733645, 0.018844332609611908, 0.019664808417790697, 0.019655742972860463, 0.019938253152791297, 0.019999514286891673, 0.02007183846526997, 0.020006651750090032, 0.01990765455301391, 0.016972234879815307, 0.01639548845076208, 0.015483071451358188, 0.0, 0.16920212808454524, 0.4499063531771833, 0.47679298361814004, 0.4779120566879861, 0.47679298361814004, 0.36616992477030386, 0.16920212808454524, 0.09868993712359486, 0.09829338381693425, 0.09846794785780755, 0.09842187615089582, 0.09792764080443411, 0.09876093288419938, 0.09866608953820896, 0.09846794785780755, 0.09829338381693425, 0.09861728853659826, 0.05223272005880958, 0.05371571025172875, 0.05486089777560957, 0.055109640096674156, 0.055494290211773295, 0.05458086446732188, 0.052342239867203544, 0.051941103164030555, 0.05093655939408201, 0.05255276455270691, 0.05340226664443892, 0.05394710973540187, 0.05522327025360074, 0.055506580325731136, 0.05551216631063108, 0.055494290211773295, 0.055413232574510723, 0.05486089777560957, 0.0546652864824331, 0.054454188821624394, 0.053938528298658944, 0.05371571025172875, 0.05223272005880958, 0.04963513478512718, 0.024137006180050297, 0.0068438662947180595, 0.01491772052621873, 0.01491772052621873, 0.028209535365732313, 0.028472079023969726, 0.028290614314509854, 0.02827635028066522, 0.028079952829376476, 0.028033311394628058, 0.027575199539834366, 0.027325416269654512, 0.02725966550970846, 0.027083372394578115, 0.026770338915556163, 0.02594733603527649, 0.02580049279543256, 0.025621381350828504, 0.02539924838453872, 0.023446455313235914, 0.022715809435520068, 0.022710037330054944, 0.022496634236902485, 0.021091679708406058, 0.02113673935514554, 0.021558778392319483, 0.022243271579366453, 0.022496634236902485, 0.02378132078190534, 0.024436635663258682, 0.02495496296714611, 0.025621381350828504, 0.026311265201047818, 0.026386752955444126, 0.026400136586920808, 0.02680821543156514, 0.027083372394578115, 0.02714762613898647, 0.027325416269654512, 0.027476463290420278, 0.027575199539834366, 0.027918264732332027, 0.028354127838960658, 0.028290614314509854, 0.028505898977650296, 0.028436764422696825, 0.027716492043348473, 0.025201350128998212, 0.01491772052621873], \"yaxis\": \"y\"}],                        {\"legend\": {\"title\": {\"text\": \"variable\"}, \"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"index\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"value\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('4e3e523e-306e-478f-b14b-0927d9afac36');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ToREP = pd.DataFrame()\n",
    "ToREP['Analytic Function'] =  np.ravel(x_test.iloc[:,[-1]].to_numpy())\n",
    "ToREP['Signal'] = y_test.to_numpy()\n",
    "ToREP['GB'] =  GBC\n",
    "ToREP['RF'] = RFC\n",
    "\n",
    "NormREP = pd.DataFrame(MMS.fit_transform(ToREP), columns = ToREP.columns, index=x_test.index)\n",
    "NormREP.sort_index().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Adamax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1398/1398 [==============================] - 1s 508us/step - loss: 53.9973 - MAE: 53.9973\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 491us/step - loss: 0.3967 - MAE: 0.3967\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 504us/step - loss: 0.3969 - MAE: 0.3969\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 578us/step - loss: 0.3968 - MAE: 0.3968\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 520us/step - loss: 0.3968 - MAE: 0.3968\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 561us/step - loss: 0.3969 - MAE: 0.3969\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 496us/step - loss: 0.3968 - MAE: 0.3968\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Test score: 0.3622997336229982\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ModelDenseAdamax1 = Sequential()\n",
    "ModelDenseAdamax1.add(Dense(14))\n",
    "ModelDenseAdamax1.add(Dense(70, activation='relu'))\n",
    "ModelDenseAdamax1.add(Dense(7, activation='relu'))\n",
    "ModelDenseAdamax1.add(Dense(1, activation='linear'))\n",
    "ModelDenseAdamax1.compile(optimizer='Adamax', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseAdamax1.fit(x_train.values, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "\n",
    "DenseAdamaxPrediction1 = ModelDenseAdamax1.predict(x_test)\n",
    "score = mean_absolute_error(DenseAdamaxPrediction1, y_test)\n",
    "print('Test score:', score)\n",
    "scores.append(score)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1398/1398 [==============================] - 1s 444us/step - loss: 5321.3496 - MAE: 5321.3496A: 0s - loss: 8568.9609 - MAE: 856\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 414us/step - loss: 299.4949 - MAE: 299.4949\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 461us/step - loss: 73.9937 - MAE: 73.9937\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 539us/step - loss: 50.2878 - MAE: 50.2878\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 470us/step - loss: 77.0375 - MAE: 77.0375\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 430us/step - loss: 15.6559 - MAE: 15.6559\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 463us/step - loss: 10.4589 - MAE: 10.4589\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 485us/step - loss: 2.2055 - MAE: 2.2055\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 471us/step - loss: 28.6860 - MAE: 28.6860\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 454us/step - loss: 9.1880 - MAE: 9.1880\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 420us/step - loss: 1.4931 - MAE: 1.4931\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 455us/step - loss: 0.4715 - MAE: 0.4715\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 418us/step - loss: 8.7838 - MAE: 8.7838\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 557us/step - loss: 3.5014 - MAE: 3.5014\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 473us/step - loss: 0.4492 - MAE: 0.4492\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 473us/step - loss: 7.0106 - MAE: 7.0106\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 478us/step - loss: 2.1492 - MAE: 2.1492\n",
      "Epoch 18/50\n",
      "1398/1398 [==============================] - 1s 540us/step - loss: 1.4213 - MAE: 1.4213\n",
      "Epoch 19/50\n",
      "1398/1398 [==============================] - 1s 447us/step - loss: 0.3969 - MAE: 0.3969\n",
      "Epoch 20/50\n",
      "1398/1398 [==============================] - 1s 462us/step - loss: 0.7547 - MAE: 0.7547\n",
      "Epoch 21/50\n",
      "1398/1398 [==============================] - 1s 446us/step - loss: 0.4475 - MAE: 0.4475\n",
      "Epoch 22/50\n",
      "1398/1398 [==============================] - 1s 433us/step - loss: 3.4290 - MAE: 3.4290\n",
      "Epoch 23/50\n",
      "1398/1398 [==============================] - 1s 503us/step - loss: 8.9374 - MAE: 8.9374\n",
      "Epoch 24/50\n",
      "1398/1398 [==============================] - 1s 486us/step - loss: 0.5023 - MAE: 0.5023\n",
      "WARNING:tensorflow:Layer dense_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Test score: 0.3608442803539708\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ModelDenseAdamax1_1 = Sequential()\n",
    "ModelDenseAdamax1_1.add(Dense(14))\n",
    "ModelDenseAdamax1_1.add(Dropout(0.1))\n",
    "ModelDenseAdamax1_1.add(Dense(70, activation='relu'))\n",
    "ModelDenseAdamax1_1.add(Dropout(0.1))\n",
    "ModelDenseAdamax1_1.add(Dense(7, activation='relu'))\n",
    "ModelDenseAdamax1_1.add(Dense(1, activation='linear'))\n",
    "ModelDenseAdamax1_1.compile(optimizer='Adamax', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseAdamax1_1.fit(x_train.values, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "\n",
    "DenseAdamaxPrediction1_1 = ModelDenseAdamax1_1.predict(x_test)\n",
    "score = mean_absolute_error(DenseAdamaxPrediction1_1, y_test)\n",
    "print('Test score:', score)\n",
    "scores.append(score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_8 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 522us/step - loss: 1501.4769 - MAE: 1501.4769\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 530us/step - loss: 1607.7480 - MAE: 1607.7480\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 597us/step - loss: 889.5793 - MAE: 889.5793\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 492us/step - loss: 1041.0135 - MAE: 1041.0135\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 436us/step - loss: 934.3096 - MAE: 934.3096\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 551us/step - loss: 681.3392 - MAE: 681.3392\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 508us/step - loss: 702.8773 - MAE: 702.8773\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 516us/step - loss: 597.2174 - MAE: 597.2174\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 471us/step - loss: 534.1129 - MAE: 534.1129\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 491us/step - loss: 518.0357 - MAE: 518.0357\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 534us/step - loss: 473.8168 - MAE: 473.8168\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 456us/step - loss: 425.2192 - MAE: 425.2192\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 375us/step - loss: 431.9375 - MAE: 431.9375\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 504us/step - loss: 341.6856 - MAE: 341.6856\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 443us/step - loss: 293.4515 - MAE: 293.4515\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 459us/step - loss: 312.0822 - MAE: 312.0822\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 513us/step - loss: 272.3686 - MAE: 272.3686\n",
      "Epoch 18/50\n",
      "1398/1398 [==============================] - 1s 439us/step - loss: 244.1942 - MAE: 244.1942\n",
      "Epoch 19/50\n",
      "1398/1398 [==============================] - 1s 447us/step - loss: 242.7392 - MAE: 242.7392\n",
      "Epoch 20/50\n",
      "1398/1398 [==============================] - 1s 490us/step - loss: 195.6845 - MAE: 195.6845\n",
      "Epoch 21/50\n",
      "1398/1398 [==============================] - 1s 471us/step - loss: 164.4156 - MAE: 164.4156\n",
      "Epoch 22/50\n",
      "1398/1398 [==============================] - 1s 479us/step - loss: 182.8033 - MAE: 182.8033\n",
      "Epoch 23/50\n",
      "1398/1398 [==============================] - 1s 590us/step - loss: 138.3985 - MAE: 138.3985\n",
      "Epoch 24/50\n",
      "1398/1398 [==============================] - 1s 415us/step - loss: 133.1159 - MAE: 133.1159\n",
      "Epoch 25/50\n",
      "1398/1398 [==============================] - 1s 435us/step - loss: 129.1572 - MAE: 129.1572\n",
      "Epoch 26/50\n",
      "1398/1398 [==============================] - 1s 389us/step - loss: 103.2036 - MAE: 103.2036\n",
      "Epoch 27/50\n",
      "1398/1398 [==============================] - 1s 420us/step - loss: 72.6404 - MAE: 72.6404\n",
      "Epoch 28/50\n",
      "1398/1398 [==============================] - 1s 475us/step - loss: 89.9846 - MAE: 89.9846\n",
      "Epoch 29/50\n",
      "1398/1398 [==============================] - 1s 515us/step - loss: 62.7572 - MAE: 62.7572\n",
      "Epoch 30/50\n",
      "1398/1398 [==============================] - 1s 490us/step - loss: 57.9438 - MAE: 57.9438\n",
      "Epoch 31/50\n",
      "1398/1398 [==============================] - 1s 524us/step - loss: 52.6677 - MAE: 52.6677\n",
      "Epoch 32/50\n",
      "1398/1398 [==============================] - 1s 494us/step - loss: 25.7293 - MAE: 25.7293\n",
      "Epoch 33/50\n",
      "1398/1398 [==============================] - 1s 458us/step - loss: 0.2993 - MAE: 0.2993\n",
      "Epoch 34/50\n",
      "1398/1398 [==============================] - 1s 407us/step - loss: 0.2990 - MAE: 0.2990\n",
      "Epoch 35/50\n",
      "1398/1398 [==============================] - 1s 480us/step - loss: 0.2960 - MAE: 0.2960\n",
      "Epoch 36/50\n",
      "1398/1398 [==============================] - 1s 476us/step - loss: 0.2912 - MAE: 0.2912\n",
      "Epoch 37/50\n",
      "1398/1398 [==============================] - 1s 438us/step - loss: 0.2972 - MAE: 0.2972\n",
      "Epoch 38/50\n",
      "1398/1398 [==============================] - 1s 437us/step - loss: 0.2912 - MAE: 0.2912\n",
      "Epoch 39/50\n",
      "1398/1398 [==============================] - 1s 433us/step - loss: 0.2977 - MAE: 0.2977\n",
      "Epoch 40/50\n",
      "1398/1398 [==============================] - 1s 548us/step - loss: 0.2784 - MAE: 0.2784\n",
      "Epoch 41/50\n",
      "1398/1398 [==============================] - 1s 441us/step - loss: 0.2802 - MAE: 0.2802\n",
      "Epoch 42/50\n",
      "1398/1398 [==============================] - 1s 467us/step - loss: 0.2824 - MAE: 0.2824\n",
      "Epoch 43/50\n",
      "1398/1398 [==============================] - 1s 523us/step - loss: 0.2819 - MAE: 0.2819\n",
      "Epoch 44/50\n",
      "1398/1398 [==============================] - 1s 418us/step - loss: 0.2769 - MAE: 0.2769\n",
      "Epoch 45/50\n",
      "1398/1398 [==============================] - 1s 373us/step - loss: 0.2794 - MAE: 0.2794\n",
      "Epoch 46/50\n",
      "1398/1398 [==============================] - 1s 429us/step - loss: 0.2854 - MAE: 0.2854\n",
      "Epoch 47/50\n",
      "1398/1398 [==============================] - 1s 418us/step - loss: 0.2899 - MAE: 0.2899\n",
      "Epoch 48/50\n",
      "1398/1398 [==============================] - 1s 421us/step - loss: 0.2894 - MAE: 0.2894\n",
      "Epoch 49/50\n",
      "1398/1398 [==============================] - 1s 514us/step - loss: 0.2854 - MAE: 0.2854\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (1, 70)                   1050      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (1, 7)                    497       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (1, 1)                    8         \n",
      "=================================================================\n",
      "Total params: 1,681\n",
      "Trainable params: 1,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.2813041900197341\n"
     ]
    }
   ],
   "source": [
    "ModelDenseAdamax2 = Sequential()\n",
    "ModelDenseAdamax2.add(Dense(14))\n",
    "ModelDenseAdamax2.add(Dense(70, activation='relu'))\n",
    "ModelDenseAdamax2.add(Dense(7, activation='elu'))\n",
    "ModelDenseAdamax2.add(Dense(1, activation='linear'))\n",
    "ModelDenseAdamax2.compile(optimizer='Adamax', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseAdamax2.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseAdamax2.summary()\n",
    "\n",
    "DenseAdamaxPrediction2 = ModelDenseAdamax2.predict(x_test)\n",
    "score = mean_absolute_error(DenseAdamaxPrediction2, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_12 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 451us/step - loss: 2895.2402 - MAE: 2895.2402\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 446us/step - loss: 274.6917 - MAE: 274.6917\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 440us/step - loss: 117.2415 - MAE: 117.2415\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 431us/step - loss: 25.5229 - MAE: 25.5229\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 418us/step - loss: 35.7404 - MAE: 35.7404\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 501us/step - loss: 15.8671 - MAE: 15.8671\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 475us/step - loss: 2.7692 - MAE: 2.7692\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 414us/step - loss: 3.3869 - MAE: 3.3869\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 468us/step - loss: 30.4729 - MAE: 30.4729\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 495us/step - loss: 3.0762 - MAE: 3.0762\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 574us/step - loss: 11.9620 - MAE: 11.9620\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 464us/step - loss: 0.8399 - MAE: 0.8399\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 457us/step - loss: 10.2478 - MAE: 10.2478\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 437us/step - loss: 0.5947 - MAE: 0.5947\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 419us/step - loss: 0.8328 - MAE: 0.8328\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 473us/step - loss: 1.5082 - MAE: 1.5082\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 451us/step - loss: 51.2166 - MAE: 51.2166\n",
      "Epoch 18/50\n",
      "1398/1398 [==============================] - 1s 435us/step - loss: 1.8704 - MAE: 1.8704\n",
      "Epoch 19/50\n",
      "1398/1398 [==============================] - 1s 501us/step - loss: 2.5884 - MAE: 2.5884\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (1, 14)                   0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (1, 70)                   1050      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (1, 70)                   0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (1, 7)                    497       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (1, 1)                    8         \n",
      "=================================================================\n",
      "Total params: 1,681\n",
      "Trainable params: 1,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.3635167336671817\n"
     ]
    }
   ],
   "source": [
    "ModelDenseAdamax2_1 = Sequential()\n",
    "ModelDenseAdamax2_1.add(Dense(14))\n",
    "ModelDenseAdamax2_1.add(Dropout(0.1))\n",
    "ModelDenseAdamax2_1.add(Dense(70, activation='relu'))\n",
    "ModelDenseAdamax2_1.add(Dropout(0.1))\n",
    "ModelDenseAdamax2_1.add(Dense(7, activation='elu'))\n",
    "ModelDenseAdamax2_1.add(Dense(1, activation='linear'))\n",
    "ModelDenseAdamax2_1.compile(optimizer='Adamax', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseAdamax2_1.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseAdamax2_1.summary()\n",
    "\n",
    "DenseAdamaxPrediction2_1 = ModelDenseAdamax2_1.predict(x_test)\n",
    "score = mean_absolute_error(DenseAdamaxPrediction2_1, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_16 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 464us/step - loss: 994.5252 - MAE: 994.5252\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 505us/step - loss: 0.3317 - MAE: 0.3317\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 426us/step - loss: 0.3191 - MAE: 0.3191\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 559us/step - loss: 0.3165 - MAE: 0.3165\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 527us/step - loss: 0.3231 - MAE: 0.3231\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 474us/step - loss: 0.3387 - MAE: 0.3387 0s - loss: 0.3396 - MAE: 0.339\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 407us/step - loss: 0.3296 - MAE: 0.3296\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 550us/step - loss: 0.4018 - MAE: 0.4018\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 547us/step - loss: 0.3164 - MAE: 0.3164\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 516us/step - loss: 0.3121 - MAE: 0.3121\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 416us/step - loss: 0.3599 - MAE: 0.3599\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 505us/step - loss: 0.3851 - MAE: 0.3851\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 516us/step - loss: 0.3156 - MAE: 0.3156\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 500us/step - loss: 0.3009 - MAE: 0.3009\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 526us/step - loss: 0.5582 - MAE: 0.5582\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 578us/step - loss: 0.3120 - MAE: 0.3120\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 513us/step - loss: 0.3340 - MAE: 0.3340\n",
      "Epoch 18/50\n",
      "1398/1398 [==============================] - 1s 460us/step - loss: 0.3126 - MAE: 0.3126\n",
      "Epoch 19/50\n",
      "1398/1398 [==============================] - 1s 513us/step - loss: 0.3023 - MAE: 0.3023\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (1, 70)                   1050      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (1, 7)                    497       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (1, 1)                    8         \n",
      "=================================================================\n",
      "Total params: 1,681\n",
      "Trainable params: 1,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.2905956879300888\n"
     ]
    }
   ],
   "source": [
    "ModelDenseAdamax3 = Sequential()\n",
    "ModelDenseAdamax3.add(Dense(14))\n",
    "ModelDenseAdamax3.add(Dense(70, activation='relu'))\n",
    "ModelDenseAdamax3.add(Dense(7, activation='selu'))\n",
    "ModelDenseAdamax3.add(Dense(1, activation='linear'))\n",
    "ModelDenseAdamax3.compile(optimizer='Adamax', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseAdamax3.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseAdamax3.summary()\n",
    "\n",
    "DenseAdamaxPrediction3 = ModelDenseAdamax3.predict(x_test)\n",
    "score = mean_absolute_error(DenseAdamaxPrediction3, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_20 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 495us/step - loss: 1447.1593 - MAE: 1447.1593\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 467us/step - loss: 1255.7318 - MAE: 1255.7318\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 419us/step - loss: 1246.6656 - MAE: 1246.6656\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 511us/step - loss: 886.9648 - MAE: 886.9648\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 491us/step - loss: 675.3927 - MAE: 675.3927\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 527us/step - loss: 732.2470 - MAE: 732.2470\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 477us/step - loss: 728.8492 - MAE: 728.8492\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 489us/step - loss: 488.5508 - MAE: 488.5508\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 414us/step - loss: 440.2354 - MAE: 440.2354\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 462us/step - loss: 314.8572 - MAE: 314.8572\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 521us/step - loss: 319.2454 - MAE: 319.2454\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 492us/step - loss: 336.0250 - MAE: 336.0250\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 518us/step - loss: 206.0585 - MAE: 206.0585\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 425us/step - loss: 201.7879 - MAE: 201.7879\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 541us/step - loss: 160.2584 - MAE: 160.2584\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 447us/step - loss: 17.7902 - MAE: 17.7902\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 475us/step - loss: 0.3904 - MAE: 0.3904\n",
      "Epoch 18/50\n",
      "1398/1398 [==============================] - 1s 485us/step - loss: 0.3859 - MAE: 0.3859\n",
      "Epoch 19/50\n",
      "1398/1398 [==============================] - 1s 596us/step - loss: 0.3775 - MAE: 0.3775\n",
      "Epoch 20/50\n",
      "1398/1398 [==============================] - 1s 439us/step - loss: 0.3547 - MAE: 0.3547\n",
      "Epoch 21/50\n",
      "1398/1398 [==============================] - 1s 525us/step - loss: 0.4241 - MAE: 0.4241\n",
      "Epoch 22/50\n",
      "1398/1398 [==============================] - 1s 530us/step - loss: 0.3140 - MAE: 0.3140\n",
      "Epoch 23/50\n",
      "1398/1398 [==============================] - 1s 447us/step - loss: 0.3060 - MAE: 0.3060\n",
      "Epoch 24/50\n",
      "1398/1398 [==============================] - 1s 464us/step - loss: 0.3015 - MAE: 0.3015\n",
      "Epoch 25/50\n",
      "1398/1398 [==============================] - 1s 561us/step - loss: 0.3270 - MAE: 0.3270\n",
      "Epoch 26/50\n",
      "1398/1398 [==============================] - 1s 525us/step - loss: 0.3514 - MAE: 0.3514\n",
      "Epoch 27/50\n",
      "1398/1398 [==============================] - 1s 451us/step - loss: 0.3001 - MAE: 0.3001 0s - loss: 0.2980 - MAE:\n",
      "Epoch 28/50\n",
      "1398/1398 [==============================] - 1s 391us/step - loss: 0.2956 - MAE: 0.2956\n",
      "Epoch 29/50\n",
      "1398/1398 [==============================] - 1s 394us/step - loss: 0.2888 - MAE: 0.2888\n",
      "Epoch 30/50\n",
      "1398/1398 [==============================] - 1s 441us/step - loss: 0.2888 - MAE: 0.2888\n",
      "Epoch 31/50\n",
      "1398/1398 [==============================] - 1s 578us/step - loss: 0.2842 - MAE: 0.2842\n",
      "Epoch 32/50\n",
      "1398/1398 [==============================] - 1s 488us/step - loss: 0.2861 - MAE: 0.2861\n",
      "Epoch 33/50\n",
      "1398/1398 [==============================] - 1s 544us/step - loss: 0.2864 - MAE: 0.2864\n",
      "Epoch 34/50\n",
      "1398/1398 [==============================] - 1s 510us/step - loss: 0.2894 - MAE: 0.2894\n",
      "Epoch 35/50\n",
      "1398/1398 [==============================] - 1s 494us/step - loss: 0.2944 - MAE: 0.2944\n",
      "Epoch 36/50\n",
      "1398/1398 [==============================] - 1s 412us/step - loss: 0.2845 - MAE: 0.2845\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (1, 70)                   1050      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (1, 7)                    497       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (1, 1)                    8         \n",
      "=================================================================\n",
      "Total params: 1,681\n",
      "Trainable params: 1,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.2841987558835783\n"
     ]
    }
   ],
   "source": [
    "ModelDenseAdamax4 = Sequential()\n",
    "ModelDenseAdamax4.add(Dense(14))\n",
    "ModelDenseAdamax4.add(Dense(70, activation='relu'))\n",
    "ModelDenseAdamax4.add(Dense(7, activation='elu'))\n",
    "ModelDenseAdamax4.add(Dense(1, activation='linear'))\n",
    "ModelDenseAdamax4.compile(optimizer='Adamax', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseAdamax4.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseAdamax4.summary()\n",
    "\n",
    "DenseAdamaxPrediction4 = ModelDenseAdamax4.predict(x_test)\n",
    "score = mean_absolute_error(DenseAdamaxPrediction4, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_24 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 407us/step - loss: 960.0843 - MAE: 960.0843\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 417us/step - loss: 816.6016 - MAE: 816.6016\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 492us/step - loss: 618.0947 - MAE: 618.0947\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 542us/step - loss: 732.4305 - MAE: 732.4305\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 477us/step - loss: 528.0739 - MAE: 528.0739\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 502us/step - loss: 502.1068 - MAE: 502.1068\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 509us/step - loss: 455.1639 - MAE: 455.1639\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 528us/step - loss: 526.6514 - MAE: 526.6514\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 581us/step - loss: 435.3614 - MAE: 435.3614\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 467us/step - loss: 344.9108 - MAE: 344.9108\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 400us/step - loss: 351.3675 - MAE: 351.3675\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 505us/step - loss: 294.2308 - MAE: 294.2308\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 507us/step - loss: 286.6149 - MAE: 286.6149\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 565us/step - loss: 250.7001 - MAE: 250.7001\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 421us/step - loss: 150.7768 - MAE: 150.7768\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 454us/step - loss: 118.4695 - MAE: 118.4695\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 412us/step - loss: 98.6840 - MAE: 98.6840\n",
      "Epoch 18/50\n",
      "1398/1398 [==============================] - 1s 509us/step - loss: 114.8481 - MAE: 114.8481\n",
      "Epoch 19/50\n",
      "1398/1398 [==============================] - 1s 496us/step - loss: 83.1457 - MAE: 83.1457\n",
      "Epoch 20/50\n",
      "1398/1398 [==============================] - 1s 435us/step - loss: 74.0671 - MAE: 74.0671\n",
      "Epoch 21/50\n",
      "1398/1398 [==============================] - 1s 426us/step - loss: 67.4972 - MAE: 67.4972\n",
      "Epoch 22/50\n",
      "1398/1398 [==============================] - 1s 481us/step - loss: 41.0373 - MAE: 41.0373\n",
      "Epoch 23/50\n",
      "1398/1398 [==============================] - 1s 411us/step - loss: 6.7207 - MAE: 6.7207\n",
      "Epoch 24/50\n",
      "1398/1398 [==============================] - 1s 393us/step - loss: 0.2795 - MAE: 0.2795\n",
      "Epoch 25/50\n",
      "1398/1398 [==============================] - 1s 555us/step - loss: 0.2803 - MAE: 0.2803\n",
      "Epoch 26/50\n",
      "1398/1398 [==============================] - 1s 464us/step - loss: 0.2825 - MAE: 0.2825\n",
      "Epoch 27/50\n",
      "1398/1398 [==============================] - 1s 524us/step - loss: 0.2752 - MAE: 0.2752\n",
      "Epoch 28/50\n",
      "1398/1398 [==============================] - 1s 536us/step - loss: 0.3010 - MAE: 0.3010\n",
      "Epoch 29/50\n",
      "1398/1398 [==============================] - 1s 484us/step - loss: 0.2746 - MAE: 0.2746\n",
      "Epoch 30/50\n",
      "1398/1398 [==============================] - 1s 422us/step - loss: 0.2828 - MAE: 0.2828\n",
      "Epoch 31/50\n",
      "1398/1398 [==============================] - 1s 439us/step - loss: 0.2879 - MAE: 0.2879 0s - loss: 0.2870 - MAE: 0.28\n",
      "Epoch 32/50\n",
      "1398/1398 [==============================] - 1s 421us/step - loss: 0.2733 - MAE: 0.2733\n",
      "Epoch 33/50\n",
      "1398/1398 [==============================] - 1s 424us/step - loss: 0.2774 - MAE: 0.2774\n",
      "Epoch 34/50\n",
      "1398/1398 [==============================] - 1s 442us/step - loss: 0.2853 - MAE: 0.2853\n",
      "Epoch 35/50\n",
      "1398/1398 [==============================] - 1s 458us/step - loss: 0.2665 - MAE: 0.2665\n",
      "Epoch 36/50\n",
      "1398/1398 [==============================] - 1s 489us/step - loss: 0.2687 - MAE: 0.2687\n",
      "Epoch 37/50\n",
      "1398/1398 [==============================] - 1s 400us/step - loss: 0.2698 - MAE: 0.2698\n",
      "Epoch 38/50\n",
      "1398/1398 [==============================] - 1s 405us/step - loss: 0.2759 - MAE: 0.2759\n",
      "Epoch 39/50\n",
      "1398/1398 [==============================] - 1s 421us/step - loss: 0.2822 - MAE: 0.2822\n",
      "Epoch 40/50\n",
      "1398/1398 [==============================] - 1s 420us/step - loss: 0.2817 - MAE: 0.2817\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_24 (Dense)             (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (1, 70)                   1050      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (1, 7)                    497       \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (1, 1)                    8         \n",
      "=================================================================\n",
      "Total params: 1,681\n",
      "Trainable params: 1,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.27613229823629143\n"
     ]
    }
   ],
   "source": [
    "ModelDenseAdamax5 = Sequential()\n",
    "ModelDenseAdamax5.add(Dense(14))\n",
    "ModelDenseAdamax5.add(Dense(70, activation='relu'))\n",
    "ModelDenseAdamax5.add(Dense(7, activation='elu'))\n",
    "ModelDenseAdamax5.add(Dense(1, activation='linear'))\n",
    "ModelDenseAdamax5.compile(optimizer='Adamax', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseAdamax5.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseAdamax5.summary()\n",
    "\n",
    "DenseAdamaxPrediction5 = ModelDenseAdamax5.predict(x_test)\n",
    "score = mean_absolute_error(DenseAdamaxPrediction5, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_28 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 396us/step - loss: 6520.5322 - MAE: 6520.5322\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 409us/step - loss: 5234.0845 - MAE: 5234.0845\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 404us/step - loss: 4730.3726 - MAE: 4730.3726\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 405us/step - loss: 4313.0562 - MAE: 4313.0562\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 411us/step - loss: 3803.4092 - MAE: 3803.4092\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 400us/step - loss: 4048.6460 - MAE: 4048.6460\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 411us/step - loss: 3061.1699 - MAE: 3061.1699\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 408us/step - loss: 2838.0400 - MAE: 2838.0400\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 495us/step - loss: 2514.2688 - MAE: 2514.2688\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 473us/step - loss: 1980.8849 - MAE: 1980.8849\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 417us/step - loss: 1656.6376 - MAE: 1656.6376\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 405us/step - loss: 1586.3927 - MAE: 1586.3927\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 418us/step - loss: 1277.7446 - MAE: 1277.7446\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 397us/step - loss: 154.1501 - MAE: 154.1501\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 420us/step - loss: 0.3927 - MAE: 0.3927\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 456us/step - loss: 0.3883 - MAE: 0.3883\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 417us/step - loss: 0.3827 - MAE: 0.3827 0s - loss: 0.3869 - MAE: 0.3\n",
      "Epoch 18/50\n",
      "1398/1398 [==============================] - 1s 371us/step - loss: 0.3793 - MAE: 0.3793\n",
      "Epoch 19/50\n",
      "1398/1398 [==============================] - 1s 428us/step - loss: 0.3685 - MAE: 0.3685\n",
      "Epoch 20/50\n",
      "1398/1398 [==============================] - 1s 410us/step - loss: 0.4094 - MAE: 0.4094\n",
      "Epoch 21/50\n",
      "1398/1398 [==============================] - 1s 385us/step - loss: 0.3252 - MAE: 0.3252\n",
      "Epoch 22/50\n",
      "1398/1398 [==============================] - 1s 416us/step - loss: 0.3453 - MAE: 0.3453\n",
      "Epoch 23/50\n",
      "1398/1398 [==============================] - 1s 417us/step - loss: 0.3433 - MAE: 0.3433\n",
      "Epoch 24/50\n",
      "1398/1398 [==============================] - 1s 404us/step - loss: 0.3275 - MAE: 0.3275\n",
      "Epoch 25/50\n",
      "1398/1398 [==============================] - 1s 411us/step - loss: 0.5902 - MAE: 0.5902\n",
      "Epoch 26/50\n",
      "1398/1398 [==============================] - 1s 407us/step - loss: 0.3284 - MAE: 0.3284\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_28 (Dense)             (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (1, 70)                   1050      \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (1, 7)                    497       \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (1, 1)                    8         \n",
      "=================================================================\n",
      "Total params: 1,681\n",
      "Trainable params: 1,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.31758058384656396\n"
     ]
    }
   ],
   "source": [
    "ModelDenseAdamax6 = Sequential()\n",
    "ModelDenseAdamax6.add(Dense(14))\n",
    "ModelDenseAdamax6.add(Dense(70))\n",
    "ModelDenseAdamax6.add(Dense(7, activation='relu'))\n",
    "ModelDenseAdamax6.add(Dense(1, activation='linear'))\n",
    "ModelDenseAdamax6.compile(optimizer='Adamax', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseAdamax6.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseAdamax6.summary()\n",
    "\n",
    "DenseAdamaxPrediction6 = ModelDenseAdamax6.predict(x_test)\n",
    "score = mean_absolute_error(DenseAdamaxPrediction6, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_32 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 434us/step - loss: 5435.0503 - MAE: 5435.0503\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 427us/step - loss: 141.6655 - MAE: 141.6655\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 418us/step - loss: 37.4651 - MAE: 37.4651\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 399us/step - loss: 26.9519 - MAE: 26.9519\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 444us/step - loss: 1.0823 - MAE: 1.0823\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 425us/step - loss: 0.7028 - MAE: 0.7028\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 413us/step - loss: 9.9341 - MAE: 9.9341\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 516us/step - loss: 2.0796 - MAE: 2.0796\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 415us/step - loss: 5.4891 - MAE: 5.4891\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 402us/step - loss: 4.3457 - MAE: 4.3457\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 424us/step - loss: 5.6579 - MAE: 5.6579\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_32 (Dense)             (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (1, 14)                   0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (1, 70)                   1050      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (1, 70)                   0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (1, 7)                    497       \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (1, 1)                    8         \n",
      "=================================================================\n",
      "Total params: 1,681\n",
      "Trainable params: 1,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.3611956286956411\n"
     ]
    }
   ],
   "source": [
    "ModelDenseAdamax6_1 = Sequential()\n",
    "ModelDenseAdamax6_1.add(Dense(14))\n",
    "ModelDenseAdamax6_1.add(Dropout(0.1))\n",
    "ModelDenseAdamax6_1.add(Dense(70))\n",
    "ModelDenseAdamax6_1.add(Dropout(0.1))\n",
    "ModelDenseAdamax6_1.add(Dense(7, activation='relu'))\n",
    "ModelDenseAdamax6_1.add(Dense(1, activation='linear'))\n",
    "ModelDenseAdamax6_1.compile(optimizer='Adamax', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseAdamax6_1.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseAdamax6_1.summary()\n",
    "\n",
    "DenseAdamaxPrediction6_1 = ModelDenseAdamax6_1.predict(x_test)\n",
    "score = mean_absolute_error(DenseAdamaxPrediction6_1, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_36 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 452us/step - loss: 1620.2378 - MAE: 1620.2378\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 445us/step - loss: 936.3625 - MAE: 936.3625\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 436us/step - loss: 0.4556 - MAE: 0.4556\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 452us/step - loss: 0.3884 - MAE: 0.3884\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 462us/step - loss: 0.3845 - MAE: 0.3845\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 451us/step - loss: 0.3782 - MAE: 0.3782\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 469us/step - loss: 0.3893 - MAE: 0.3893\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 466us/step - loss: 0.3267 - MAE: 0.3267\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 448us/step - loss: 0.3225 - MAE: 0.3225\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 440us/step - loss: 0.3330 - MAE: 0.3330\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 447us/step - loss: 0.3011 - MAE: 0.3011\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 482us/step - loss: 0.3111 - MAE: 0.3111\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 445us/step - loss: 0.3673 - MAE: 0.3673\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 524us/step - loss: 0.2947 - MAE: 0.2947\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 476us/step - loss: 0.3013 - MAE: 0.3013\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 450us/step - loss: 0.3302 - MAE: 0.3302\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 431us/step - loss: 0.3205 - MAE: 0.3205\n",
      "Epoch 18/50\n",
      "1398/1398 [==============================] - 1s 443us/step - loss: 0.2944 - MAE: 0.2944\n",
      "Epoch 19/50\n",
      "1398/1398 [==============================] - 1s 449us/step - loss: 0.3106 - MAE: 0.3106\n",
      "Epoch 20/50\n",
      "1398/1398 [==============================] - 1s 453us/step - loss: 0.3755 - MAE: 0.3755\n",
      "Epoch 21/50\n",
      "1398/1398 [==============================] - 1s 469us/step - loss: 0.2817 - MAE: 0.2817\n",
      "Epoch 22/50\n",
      "1398/1398 [==============================] - 1s 449us/step - loss: 0.2848 - MAE: 0.2848\n",
      "Epoch 23/50\n",
      "1398/1398 [==============================] - 1s 444us/step - loss: 0.2855 - MAE: 0.2855\n",
      "Epoch 24/50\n",
      "1398/1398 [==============================] - 1s 399us/step - loss: 0.3685 - MAE: 0.3685\n",
      "Epoch 25/50\n",
      "1398/1398 [==============================] - 1s 446us/step - loss: 0.3655 - MAE: 0.3655\n",
      "Epoch 26/50\n",
      "1398/1398 [==============================] - 1s 437us/step - loss: 0.2777 - MAE: 0.2777\n",
      "Epoch 27/50\n",
      "1398/1398 [==============================] - 1s 438us/step - loss: 0.2745 - MAE: 0.2745\n",
      "Epoch 28/50\n",
      "1398/1398 [==============================] - 1s 454us/step - loss: 0.2849 - MAE: 0.2849\n",
      "Epoch 29/50\n",
      "1398/1398 [==============================] - 1s 510us/step - loss: 0.2913 - MAE: 0.2913\n",
      "Epoch 30/50\n",
      "1398/1398 [==============================] - 1s 441us/step - loss: 0.3036 - MAE: 0.3036\n",
      "Epoch 31/50\n",
      "1398/1398 [==============================] - 1s 444us/step - loss: 0.3040 - MAE: 0.3040\n",
      "Epoch 32/50\n",
      "1398/1398 [==============================] - 1s 438us/step - loss: 0.2818 - MAE: 0.2818\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_36 (Dense)             (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (1, 192)                  2880      \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (1, 24)                   4632      \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (1, 9)                    225       \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (1, 1)                    10        \n",
      "=================================================================\n",
      "Total params: 7,873\n",
      "Trainable params: 7,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.27896134299104675\n"
     ]
    }
   ],
   "source": [
    "ModelDenseAdamax7 = Sequential()\n",
    "ModelDenseAdamax7.add(Dense(14))\n",
    "ModelDenseAdamax7.add(Dense(192, activation='relu'))\n",
    "ModelDenseAdamax7.add(Dense(24, activation='relu'))\n",
    "ModelDenseAdamax7.add(Dense(9, activation='relu'))\n",
    "ModelDenseAdamax7.add(Dense(1, activation='linear'))\n",
    "ModelDenseAdamax7.compile(optimizer='Adamax', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseAdamax7.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseAdamax7.summary()\n",
    "\n",
    "DenseAdamaxPrediction7 = ModelDenseAdamax7.predict(x_test)\n",
    "score = mean_absolute_error(DenseAdamaxPrediction7, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense RMSprop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1398/1398 [==============================] - 1s 402us/step - loss: 143.3467 - MAE: 143.3467\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 406us/step - loss: 0.3979 - MAE: 0.3979\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 411us/step - loss: 0.3969 - MAE: 0.3969\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 398us/step - loss: 0.3968 - MAE: 0.3968\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 414us/step - loss: 0.3969 - MAE: 0.3969\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 408us/step - loss: 0.3969 - MAE: 0.3969\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 409us/step - loss: 0.3968 - MAE: 0.3968\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 404us/step - loss: 0.3969 - MAE: 0.3969\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 412us/step - loss: 0.3965 - MAE: 0.3965\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 415us/step - loss: 0.3970 - MAE: 0.3970\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 410us/step - loss: 0.3969 - MAE: 0.3969\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 421us/step - loss: 0.3968 - MAE: 0.3968\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 405us/step - loss: 0.3969 - MAE: 0.3969\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 405us/step - loss: 0.3969 - MAE: 0.3969\n",
      "WARNING:tensorflow:Layer dense_41 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Test score: 0.36088373135372703\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ModelDenseRMSprop1 = Sequential()\n",
    "ModelDenseRMSprop1.add(Dense(14))\n",
    "ModelDenseRMSprop1.add(Dense(70, activation='relu'))\n",
    "ModelDenseRMSprop1.add(Dense(7, activation='relu'))\n",
    "ModelDenseRMSprop1.add(Dense(1, activation='linear'))\n",
    "ModelDenseRMSprop1.compile(optimizer='RMSprop', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseRMSprop1.fit(x_train.values, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "\n",
    "DenseRMSpropPrediction1 = ModelDenseRMSprop1.predict(x_test)\n",
    "score = mean_absolute_error(DenseRMSpropPrediction1, y_test)\n",
    "print('Test score:', score)\n",
    "scores.append(score)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1398/1398 [==============================] - 1s 428us/step - loss: 2469.3354 - MAE: 2469.3354\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 437us/step - loss: 0.7950 - MAE: 0.7950\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 415us/step - loss: 1.6986 - MAE: 1.6986\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 449us/step - loss: 2.2900 - MAE: 2.2900\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 441us/step - loss: 4.6714 - MAE: 4.6714\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 400us/step - loss: 0.4121 - MAE: 0.4121\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 443us/step - loss: 0.3967 - MAE: 0.3967\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 439us/step - loss: 0.6188 - MAE: 0.6188\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 430us/step - loss: 0.3968 - MAE: 0.3968\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 438us/step - loss: 0.4242 - MAE: 0.4242\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 433us/step - loss: 0.3969 - MAE: 0.3969\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 435us/step - loss: 0.3966 - MAE: 0.3966\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 446us/step - loss: 4.8665 - MAE: 4.8665\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 446us/step - loss: 0.3969 - MAE: 0.3969\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 412us/step - loss: 0.3967 - MAE: 0.3967\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 426us/step - loss: 0.3969 - MAE: 0.3969\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 422us/step - loss: 0.3969 - MAE: 0.3969\n",
      "WARNING:tensorflow:Layer dense_45 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Test score: 0.36128120137028824\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ModelDenseRMSprop1_1 = Sequential()\n",
    "ModelDenseRMSprop1_1.add(Dense(14))\n",
    "ModelDenseRMSprop1_1.add(Dropout(0.1))\n",
    "ModelDenseRMSprop1_1.add(Dense(70, activation='relu'))\n",
    "ModelDenseRMSprop1_1.add(Dropout(0.1))\n",
    "ModelDenseRMSprop1_1.add(Dense(7, activation='relu'))\n",
    "ModelDenseRMSprop1_1.add(Dense(1, activation='linear'))\n",
    "ModelDenseRMSprop1_1.compile(optimizer='RMSprop', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseRMSprop1_1.fit(x_train.values, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "\n",
    "DenseRMSpropPrediction1_1 = ModelDenseRMSprop1_1.predict(x_test)\n",
    "score = mean_absolute_error(DenseRMSpropPrediction1_1, y_test)\n",
    "print('Test score:', score)\n",
    "scores.append(score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_49 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 395us/step - loss: 4996.4565 - MAE: 4996.4565\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 435us/step - loss: 2391.7458 - MAE: 2391.7458\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 456us/step - loss: 1510.2205 - MAE: 1510.2205\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 415us/step - loss: 764.5100 - MAE: 764.5100\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 428us/step - loss: 243.9336 - MAE: 243.9336\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 419us/step - loss: 32.1846 - MAE: 32.1846\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 462us/step - loss: 3.4642 - MAE: 3.4642\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 435us/step - loss: 2.6731 - MAE: 2.6731\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 425us/step - loss: 2.0005 - MAE: 2.0005\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 439us/step - loss: 0.7986 - MAE: 0.7986\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 421us/step - loss: 1.1739 - MAE: 1.1739\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 423us/step - loss: 1.3570 - MAE: 1.3570\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 420us/step - loss: 1.1764 - MAE: 1.1764\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 434us/step - loss: 0.9319 - MAE: 0.9319\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 436us/step - loss: 1.5534 - MAE: 1.5534\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_49 (Dense)             (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (1, 70)                   1050      \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (1, 7)                    497       \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (1, 1)                    8         \n",
      "=================================================================\n",
      "Total params: 1,681\n",
      "Trainable params: 1,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.2924536547056766\n"
     ]
    }
   ],
   "source": [
    "ModelDenseRMSprop2 = Sequential()\n",
    "ModelDenseRMSprop2.add(Dense(14))\n",
    "ModelDenseRMSprop2.add(Dense(70, activation='relu'))\n",
    "ModelDenseRMSprop2.add(Dense(7, activation='elu'))\n",
    "ModelDenseRMSprop2.add(Dense(1, activation='linear'))\n",
    "ModelDenseRMSprop2.compile(optimizer='RMSprop', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseRMSprop2.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseRMSprop2.summary()\n",
    "\n",
    "DenseRMSpropPrediction2 = ModelDenseRMSprop2.predict(x_test)\n",
    "score = mean_absolute_error(DenseRMSpropPrediction2, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_53 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 431us/step - loss: 333.8429 - MAE: 333.8429\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 420us/step - loss: 17.2179 - MAE: 17.2179\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 449us/step - loss: 15.8537 - MAE: 15.8537\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 437us/step - loss: 8.4667 - MAE: 8.4667\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 428us/step - loss: 32.1366 - MAE: 32.1366\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 429us/step - loss: 18.9453 - MAE: 18.9453\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - ETA: 0s - loss: 19.8871 - MAE: 19.887 - 1s 431us/step - loss: 18.2306 - MAE: 18.2306\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 445us/step - loss: 16.5886 - MAE: 16.5886\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 459us/step - loss: 38.3590 - MAE: 38.3590\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_53 (Dense)             (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (1, 14)                   0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (1, 70)                   1050      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (1, 70)                   0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (1, 7)                    497       \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (1, 1)                    8         \n",
      "=================================================================\n",
      "Total params: 1,681\n",
      "Trainable params: 1,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.3683833017261023\n"
     ]
    }
   ],
   "source": [
    "ModelDenseRMSprop2_1 = Sequential()\n",
    "ModelDenseRMSprop2_1.add(Dense(14))\n",
    "ModelDenseRMSprop2_1.add(Dropout(0.1))\n",
    "ModelDenseRMSprop2_1.add(Dense(70, activation='relu'))\n",
    "ModelDenseRMSprop2_1.add(Dropout(0.1))\n",
    "ModelDenseRMSprop2_1.add(Dense(7, activation='elu'))\n",
    "ModelDenseRMSprop2_1.add(Dense(1, activation='linear'))\n",
    "ModelDenseRMSprop2_1.compile(optimizer='RMSprop', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseRMSprop2_1.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseRMSprop2_1.summary()\n",
    "\n",
    "DenseRMSpropPrediction2_1 = ModelDenseRMSprop2_1.predict(x_test)\n",
    "score = mean_absolute_error(DenseRMSpropPrediction2_1, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_57 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 421us/step - loss: 3002.2686 - MAE: 3002.2686\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 422us/step - loss: 1314.3665 - MAE: 1314.3665\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 421us/step - loss: 29.3865 - MAE: 29.3865\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 450us/step - loss: 5.0271 - MAE: 5.0271 0s - loss: 3.4625 - MAE: 3.\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 471us/step - loss: 7.4151 - MAE: 7.4151\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 416us/step - loss: 7.4550 - MAE: 7.4550\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 426us/step - loss: 6.6075 - MAE: 6.6075\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 475us/step - loss: 11.7066 - MAE: 11.7066\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 547us/step - loss: 10.2022 - MAE: 10.2022\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_57 (Dense)             (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (1, 70)                   1050      \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (1, 7)                    497       \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (1, 1)                    8         \n",
      "=================================================================\n",
      "Total params: 1,681\n",
      "Trainable params: 1,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.32735008490388856\n"
     ]
    }
   ],
   "source": [
    "ModelDenseRMSprop3 = Sequential()\n",
    "ModelDenseRMSprop3.add(Dense(14))\n",
    "ModelDenseRMSprop3.add(Dense(70, activation='relu'))\n",
    "ModelDenseRMSprop3.add(Dense(7, activation='selu'))\n",
    "ModelDenseRMSprop3.add(Dense(1, activation='linear'))\n",
    "ModelDenseRMSprop3.compile(optimizer='RMSprop', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseRMSprop3.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseRMSprop3.summary()\n",
    "\n",
    "DenseRMSpropPrediction3 = ModelDenseRMSprop3.predict(x_test)\n",
    "score = mean_absolute_error(DenseRMSpropPrediction3, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_61 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 464us/step - loss: 12.4573 - MAE: 12.4573\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 432us/step - loss: 40.9226 - MAE: 40.9226\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 503us/step - loss: 32.9836 - MAE: 32.9836\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 549us/step - loss: 24.4204 - MAE: 24.4204\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 471us/step - loss: 20.3280 - MAE: 20.3280\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 410us/step - loss: 37.2463 - MAE: 37.2463\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_61 (Dense)             (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (1, 70)                   1050      \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (1, 7)                    497       \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (1, 1)                    8         \n",
      "=================================================================\n",
      "Total params: 1,681\n",
      "Trainable params: 1,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.33129830878566113\n"
     ]
    }
   ],
   "source": [
    "ModelDenseRMSprop4 = Sequential()\n",
    "ModelDenseRMSprop4.add(Dense(14))\n",
    "ModelDenseRMSprop4.add(Dense(70, activation='relu'))\n",
    "ModelDenseRMSprop4.add(Dense(7, activation='elu'))\n",
    "ModelDenseRMSprop4.add(Dense(1, activation='linear'))\n",
    "ModelDenseRMSprop4.compile(optimizer='RMSprop', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseRMSprop4.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseRMSprop4.summary()\n",
    "\n",
    "DenseRMSpropPrediction4 = ModelDenseRMSprop4.predict(x_test)\n",
    "score = mean_absolute_error(DenseRMSpropPrediction4, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_65 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 456us/step - loss: 4580.0024 - MAE: 4580.0024\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 460us/step - loss: 2340.9124 - MAE: 2340.9124\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 409us/step - loss: 1261.3170 - MAE: 1261.3170\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 391us/step - loss: 546.0839 - MAE: 546.0839\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 390us/step - loss: 127.1524 - MAE: 127.1524\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 426us/step - loss: 5.2656 - MAE: 5.2656\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 474us/step - loss: 4.4278 - MAE: 4.4278\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 509us/step - loss: 5.0170 - MAE: 5.0170\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 441us/step - loss: 2.2570 - MAE: 2.2570\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 448us/step - loss: 1.7051 - MAE: 1.7051\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 483us/step - loss: 24.3915 - MAE: 24.3915\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 446us/step - loss: 1.5935 - MAE: 1.5935\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 407us/step - loss: 0.4667 - MAE: 0.4667\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 432us/step - loss: 0.6320 - MAE: 0.6320\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 465us/step - loss: 2.3901 - MAE: 2.3901\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 470us/step - loss: 0.6826 - MAE: 0.6826\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 429us/step - loss: 0.8054 - MAE: 0.8054\n",
      "Epoch 18/50\n",
      "1398/1398 [==============================] - 1s 426us/step - loss: 1.3948 - MAE: 1.3948\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_65 (Dense)             (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (1, 70)                   1050      \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (1, 7)                    497       \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (1, 1)                    8         \n",
      "=================================================================\n",
      "Total params: 1,681\n",
      "Trainable params: 1,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.30647536860484376\n"
     ]
    }
   ],
   "source": [
    "ModelDenseRMSprop5 = Sequential()\n",
    "ModelDenseRMSprop5.add(Dense(14))\n",
    "ModelDenseRMSprop5.add(Dense(70, activation='relu'))\n",
    "ModelDenseRMSprop5.add(Dense(7, activation='elu'))\n",
    "ModelDenseRMSprop5.add(Dense(1, activation='linear'))\n",
    "ModelDenseRMSprop5.compile(optimizer='RMSprop', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseRMSprop5.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseRMSprop5.summary()\n",
    "\n",
    "DenseRMSpropPrediction5 = ModelDenseRMSprop5.predict(x_test)\n",
    "score = mean_absolute_error(DenseRMSpropPrediction5, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_69 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 418us/step - loss: 72.0829 - MAE: 72.0829\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 461us/step - loss: 0.3969 - MAE: 0.3969\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 413us/step - loss: 0.3969 - MAE: 0.3969\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 410us/step - loss: 0.3968 - MAE: 0.3968\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 407us/step - loss: 0.3968 - MAE: 0.3968\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 389us/step - loss: 0.3967 - MAE: 0.3967\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 429us/step - loss: 0.3962 - MAE: 0.3962\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 412us/step - loss: 0.3971 - MAE: 0.3971\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 390us/step - loss: 0.3968 - MAE: 0.3968\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 453us/step - loss: 0.3969 - MAE: 0.3969\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 409us/step - loss: 0.3968 - MAE: 0.3968\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 391us/step - loss: 0.3967 - MAE: 0.3967\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_69 (Dense)             (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (1, 70)                   1050      \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (1, 7)                    497       \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (1, 1)                    8         \n",
      "=================================================================\n",
      "Total params: 1,681\n",
      "Trainable params: 1,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.36135176666210755\n"
     ]
    }
   ],
   "source": [
    "ModelDenseRMSprop6 = Sequential()\n",
    "ModelDenseRMSprop6.add(Dense(14))\n",
    "ModelDenseRMSprop6.add(Dense(70))\n",
    "ModelDenseRMSprop6.add(Dense(7, activation='relu'))\n",
    "ModelDenseRMSprop6.add(Dense(1, activation='linear'))\n",
    "ModelDenseRMSprop6.compile(optimizer='RMSprop', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseRMSprop6.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseRMSprop6.summary()\n",
    "\n",
    "DenseRMSpropPrediction6 = ModelDenseRMSprop6.predict(x_test)\n",
    "score = mean_absolute_error(DenseRMSpropPrediction6, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_73 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 420us/step - loss: 469.8287 - MAE: 469.8287\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 416us/step - loss: 3.2509 - MAE: 3.2509\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 432us/step - loss: 4.6820 - MAE: 4.6820\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 423us/step - loss: 0.3978 - MAE: 0.3978\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 444us/step - loss: 0.3969 - MAE: 0.3969\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 438us/step - loss: 0.5965 - MAE: 0.5965\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 484us/step - loss: 0.3967 - MAE: 0.3967\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 524us/step - loss: 0.3967 - MAE: 0.3967\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 457us/step - loss: 6.0176 - MAE: 6.0176\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 438us/step - loss: 0.3969 - MAE: 0.3969\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 496us/step - loss: 0.3969 - MAE: 0.3969\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 500us/step - loss: 0.7749 - MAE: 0.7749\n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_73 (Dense)             (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (1, 14)                   0         \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (1, 70)                   1050      \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (1, 70)                   0         \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (1, 7)                    497       \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (1, 1)                    8         \n",
      "=================================================================\n",
      "Total params: 1,681\n",
      "Trainable params: 1,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.3612873776622911\n"
     ]
    }
   ],
   "source": [
    "ModelDenseRMSprop6_1 = Sequential()\n",
    "ModelDenseRMSprop6_1.add(Dense(14))\n",
    "ModelDenseRMSprop6_1.add(Dropout(0.1))\n",
    "ModelDenseRMSprop6_1.add(Dense(70))\n",
    "ModelDenseRMSprop6_1.add(Dropout(0.1))\n",
    "ModelDenseRMSprop6_1.add(Dense(7, activation='relu'))\n",
    "ModelDenseRMSprop6_1.add(Dense(1, activation='linear'))\n",
    "ModelDenseRMSprop6_1.compile(optimizer='RMSprop', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseRMSprop6_1.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseRMSprop6_1.summary()\n",
    "\n",
    "DenseRMSpropPrediction6_1 = ModelDenseRMSprop6_1.predict(x_test)\n",
    "score = mean_absolute_error(DenseRMSpropPrediction6_1, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_77 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 436us/step - loss: 106.4688 - MAE: 106.4688\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 445us/step - loss: 0.3969 - MAE: 0.3969\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 529us/step - loss: 0.3970 - MAE: 0.3970\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 477us/step - loss: 0.3974 - MAE: 0.3974\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 449us/step - loss: 0.3972 - MAE: 0.3972\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 453us/step - loss: 0.3974 - MAE: 0.3974\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 460us/step - loss: 0.3974 - MAE: 0.3974\n",
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_77 (Dense)             (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (1, 192)                  2880      \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (1, 24)                   4632      \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (1, 9)                    225       \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (1, 1)                    10        \n",
      "=================================================================\n",
      "Total params: 7,873\n",
      "Trainable params: 7,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.3623363904795351\n"
     ]
    }
   ],
   "source": [
    "ModelDenseRMSprop7 = Sequential()\n",
    "ModelDenseRMSprop7.add(Dense(14))\n",
    "ModelDenseRMSprop7.add(Dense(192, activation='relu'))\n",
    "ModelDenseRMSprop7.add(Dense(24, activation='relu'))\n",
    "ModelDenseRMSprop7.add(Dense(9, activation='relu'))\n",
    "ModelDenseRMSprop7.add(Dense(1, activation='linear'))\n",
    "ModelDenseRMSprop7.compile(optimizer='RMSprop', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseRMSprop7.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseRMSprop7.summary()\n",
    "\n",
    "DenseRMSpropPrediction7 = ModelDenseRMSprop7.predict(x_test)\n",
    "score = mean_absolute_error(DenseRMSpropPrediction7, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1398/1398 [==============================] - 1s 455us/step - loss: 136.6255 - MAE: 136.6255\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 459us/step - loss: 0.3970 - MAE: 0.3970\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 485us/step - loss: 0.3968 - MAE: 0.3968\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 475us/step - loss: 0.3968 - MAE: 0.3968\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 443us/step - loss: 0.3969 - MAE: 0.3969\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 442us/step - loss: 0.3968 - MAE: 0.3968\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 439us/step - loss: 0.3968 - MAE: 0.3968\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 459us/step - loss: 0.3968 - MAE: 0.3968\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 460us/step - loss: 0.3969 - MAE: 0.3969\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 420us/step - loss: 0.3969 - MAE: 0.3969\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 443us/step - loss: 0.3967 - MAE: 0.3967 0s - loss: 0.3798 - MAE: 0.37 - ETA: 0s - loss: 0.3701 - MAE: 0\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 472us/step - loss: 0.3969 - MAE: 0.3969\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 504us/step - loss: 0.3966 - MAE: 0.3966\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 496us/step - loss: 0.3969 - MAE: 0.3969 0s - loss: 0.3955 - MAE: 0.3\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 478us/step - loss: 0.3968 - MAE: 0.3968\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 486us/step - loss: 0.3968 - MAE: 0.3968\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 533us/step - loss: 0.3969 - MAE: 0.3969\n",
      "Epoch 18/50\n",
      "1398/1398 [==============================] - 1s 544us/step - loss: 0.3966 - MAE: 0.3966\n",
      "WARNING:tensorflow:Layer dense_82 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Test score: 0.3633210742094379\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ModelDenseAdam1 = Sequential()\n",
    "ModelDenseAdam1.add(Dense(14))\n",
    "ModelDenseAdam1.add(Dense(70, activation='relu'))\n",
    "ModelDenseAdam1.add(Dense(7, activation='relu'))\n",
    "ModelDenseAdam1.add(Dense(1, activation='linear'))\n",
    "ModelDenseAdam1.compile(optimizer='Adam', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseAdam1.fit(x_train.values, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "\n",
    "DenseAdamPrediction1 = ModelDenseAdam1.predict(x_test)\n",
    "score = mean_absolute_error(DenseAdamPrediction1, y_test)\n",
    "print('Test score:', score)\n",
    "scores.append(score)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1398/1398 [==============================] - 1s 454us/step - loss: 3253.1167 - MAE: 3253.1167\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 482us/step - loss: 594.1924 - MAE: 594.1924\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 480us/step - loss: 33.7483 - MAE: 33.7483\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 448us/step - loss: 31.5790 - MAE: 31.5790\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 495us/step - loss: 1.2953 - MAE: 1.2953\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 457us/step - loss: 0.4126 - MAE: 0.4126\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 456us/step - loss: 0.4087 - MAE: 0.4087\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 438us/step - loss: 1.1895 - MAE: 1.1895\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 455us/step - loss: 0.3967 - MAE: 0.3967\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 504us/step - loss: 0.4243 - MAE: 0.4243\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 481us/step - loss: 0.3970 - MAE: 0.3970\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 537us/step - loss: 0.3969 - MAE: 0.3969\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 461us/step - loss: 0.3967 - MAE: 0.3967\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 554us/step - loss: 0.3969 - MAE: 0.3969\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 499us/step - loss: 0.3969 - MAE: 0.3969\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 574us/step - loss: 7.7844 - MAE: 7.7844\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 457us/step - loss: 0.3969 - MAE: 0.3969\n",
      "Epoch 18/50\n",
      "1398/1398 [==============================] - 1s 485us/step - loss: 0.3968 - MAE: 0.3968\n",
      "WARNING:tensorflow:Layer dense_86 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Test score: 0.3608694997556306\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ModelDenseAdam1_1 = Sequential()\n",
    "ModelDenseAdam1_1.add(Dense(14))\n",
    "ModelDenseAdam1_1.add(Dropout(0.1))\n",
    "ModelDenseAdam1_1.add(Dense(70, activation='relu'))\n",
    "ModelDenseAdam1_1.add(Dropout(0.1))\n",
    "ModelDenseAdam1_1.add(Dense(7, activation='relu'))\n",
    "ModelDenseAdam1_1.add(Dense(1, activation='linear'))\n",
    "ModelDenseAdam1_1.compile(optimizer='Adam', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseAdam1_1.fit(x_train.values, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "\n",
    "DenseAdamPrediction1_1 = ModelDenseAdam1_1.predict(x_test)\n",
    "score = mean_absolute_error(DenseAdamPrediction1_1, y_test)\n",
    "print('Test score:', score)\n",
    "scores.append(score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_90 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 418us/step - loss: 216.1688 - MAE: 216.1688\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 411us/step - loss: 0.4974 - MAE: 0.4974\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 426us/step - loss: 0.3985 - MAE: 0.3985\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 421us/step - loss: 0.4005 - MAE: 0.4005\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 510us/step - loss: 0.3996 - MAE: 0.3996\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 461us/step - loss: 0.3987 - MAE: 0.3987\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 435us/step - loss: 0.3987 - MAE: 0.3987\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 424us/step - loss: 0.3991 - MAE: 0.3991\n",
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_90 (Dense)             (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (1, 70)                   1050      \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (1, 7)                    497       \n",
      "_________________________________________________________________\n",
      "dense_93 (Dense)             (1, 1)                    8         \n",
      "=================================================================\n",
      "Total params: 1,681\n",
      "Trainable params: 1,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.3623949189222446\n"
     ]
    }
   ],
   "source": [
    "ModelDenseAdam2 = Sequential()\n",
    "ModelDenseAdam2.add(Dense(14))\n",
    "ModelDenseAdam2.add(Dense(70, activation='relu'))\n",
    "ModelDenseAdam2.add(Dense(7, activation='elu'))\n",
    "ModelDenseAdam2.add(Dense(1, activation='linear'))\n",
    "ModelDenseAdam2.compile(optimizer='Adam', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseAdam2.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseAdam2.summary()\n",
    "\n",
    "DenseAdamPrediction2 = ModelDenseAdam2.predict(x_test)\n",
    "score = mean_absolute_error(DenseAdamPrediction2, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_94 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 435us/step - loss: 2786.1035 - MAE: 2786.1035\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 408us/step - loss: 7.7620 - MAE: 7.7620\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 434us/step - loss: 1.3134 - MAE: 1.3134\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 437us/step - loss: 4.5763 - MAE: 4.5763\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 415us/step - loss: 1.0417 - MAE: 1.0417\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 422us/step - loss: 3.1264 - MAE: 3.1264\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 414us/step - loss: 0.4141 - MAE: 0.4141\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 441us/step - loss: 0.3969 - MAE: 0.3969\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 452us/step - loss: 0.3969 - MAE: 0.3969\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 451us/step - loss: 0.3973 - MAE: 0.3973\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 468us/step - loss: 0.3972 - MAE: 0.3972\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 478us/step - loss: 0.3974 - MAE: 0.3974\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 477us/step - loss: 0.3975 - MAE: 0.3975\n",
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_94 (Dense)             (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (1, 14)                   0         \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (1, 70)                   1050      \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (1, 70)                   0         \n",
      "_________________________________________________________________\n",
      "dense_96 (Dense)             (1, 7)                    497       \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (1, 1)                    8         \n",
      "=================================================================\n",
      "Total params: 1,681\n",
      "Trainable params: 1,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.36715718621705273\n"
     ]
    }
   ],
   "source": [
    "ModelDenseAdam2_1 = Sequential()\n",
    "ModelDenseAdam2_1.add(Dense(14))\n",
    "ModelDenseAdam2_1.add(Dropout(0.1))\n",
    "ModelDenseAdam2_1.add(Dense(70, activation='relu'))\n",
    "ModelDenseAdam2_1.add(Dropout(0.1))\n",
    "ModelDenseAdam2_1.add(Dense(7, activation='elu'))\n",
    "ModelDenseAdam2_1.add(Dense(1, activation='linear'))\n",
    "ModelDenseAdam2_1.compile(optimizer='Adam', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseAdam2_1.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseAdam2_1.summary()\n",
    "\n",
    "DenseAdamPrediction2_1 = ModelDenseAdam2_1.predict(x_test)\n",
    "score = mean_absolute_error(DenseAdamPrediction2_1, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_98 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 438us/step - loss: 3334.4282 - MAE: 3334.4282\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 465us/step - loss: 1353.2906 - MAE: 1353.2906\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 527us/step - loss: 884.3973 - MAE: 884.3973\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 511us/step - loss: 585.2657 - MAE: 585.2657\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 462us/step - loss: 278.6118 - MAE: 278.6118\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 465us/step - loss: 166.9951 - MAE: 166.9951\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 439us/step - loss: 113.7171 - MAE: 113.7171\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 488us/step - loss: 0.3040 - MAE: 0.3040\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 530us/step - loss: 0.2946 - MAE: 0.2946\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 444us/step - loss: 0.2865 - MAE: 0.2865\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 477us/step - loss: 0.2788 - MAE: 0.2788\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 461us/step - loss: 0.2760 - MAE: 0.2760\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 435us/step - loss: 6.5201 - MAE: 6.5201\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 451us/step - loss: 0.2891 - MAE: 0.2891\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 528us/step - loss: 0.2808 - MAE: 0.2808\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 525us/step - loss: 0.2883 - MAE: 0.2883\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 584us/step - loss: 0.2820 - MAE: 0.2820\n",
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_98 (Dense)             (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (1, 70)                   1050      \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (1, 7)                    497       \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            (1, 1)                    8         \n",
      "=================================================================\n",
      "Total params: 1,681\n",
      "Trainable params: 1,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.28414164699113337\n"
     ]
    }
   ],
   "source": [
    "ModelDenseAdam3 = Sequential()\n",
    "ModelDenseAdam3.add(Dense(14))\n",
    "ModelDenseAdam3.add(Dense(70, activation='relu'))\n",
    "ModelDenseAdam3.add(Dense(7, activation='selu'))\n",
    "ModelDenseAdam3.add(Dense(1, activation='linear'))\n",
    "ModelDenseAdam3.compile(optimizer='Adam', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseAdam3.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseAdam3.summary()\n",
    "\n",
    "DenseAdamPrediction3 = ModelDenseAdam3.predict(x_test)\n",
    "score = mean_absolute_error(DenseAdamPrediction3, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_102 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 424us/step - loss: 1965.9988 - MAE: 1965.9988\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 473us/step - loss: 869.1717 - MAE: 869.1717\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 475us/step - loss: 196.6253 - MAE: 196.6253\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 441us/step - loss: 0.3613 - MAE: 0.3613\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 495us/step - loss: 0.3360 - MAE: 0.3360\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 519us/step - loss: 0.3196 - MAE: 0.3196\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 462us/step - loss: 0.3117 - MAE: 0.3117\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 453us/step - loss: 0.3080 - MAE: 0.3080\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 531us/step - loss: 0.3053 - MAE: 0.3053\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 436us/step - loss: 0.3047 - MAE: 0.3047\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 453us/step - loss: 0.3048 - MAE: 0.3048\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 438us/step - loss: 2.6888 - MAE: 2.6888\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 423us/step - loss: 0.3724 - MAE: 0.3724\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 422us/step - loss: 0.3743 - MAE: 0.3743\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 419us/step - loss: 0.3517 - MAE: 0.3517\n",
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_102 (Dense)            (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dense_103 (Dense)            (1, 70)                   1050      \n",
      "_________________________________________________________________\n",
      "dense_104 (Dense)            (1, 7)                    497       \n",
      "_________________________________________________________________\n",
      "dense_105 (Dense)            (1, 1)                    8         \n",
      "=================================================================\n",
      "Total params: 1,681\n",
      "Trainable params: 1,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.32097116288211663\n"
     ]
    }
   ],
   "source": [
    "ModelDenseAdam4 = Sequential()\n",
    "ModelDenseAdam4.add(Dense(14))\n",
    "ModelDenseAdam4.add(Dense(70, activation='relu'))\n",
    "ModelDenseAdam4.add(Dense(7, activation='elu'))\n",
    "ModelDenseAdam4.add(Dense(1, activation='linear'))\n",
    "ModelDenseAdam4.compile(optimizer='Adam', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseAdam4.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseAdam4.summary()\n",
    "\n",
    "DenseAdamPrediction4 = ModelDenseAdam4.predict(x_test)\n",
    "score = mean_absolute_error(DenseAdamPrediction4, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_106 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 427us/step - loss: 2472.5500 - MAE: 2472.5500\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 407us/step - loss: 1166.4645 - MAE: 1166.4645\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 407us/step - loss: 849.9190 - MAE: 849.9190\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 458us/step - loss: 334.5523 - MAE: 334.5523\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 450us/step - loss: 0.3143 - MAE: 0.3143\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 451us/step - loss: 0.3062 - MAE: 0.3062\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 457us/step - loss: 0.2984 - MAE: 0.2984\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 448us/step - loss: 0.3006 - MAE: 0.3006\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 473us/step - loss: 0.2986 - MAE: 0.2986\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 469us/step - loss: 112.8197 - MAE: 112.8197\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 476us/step - loss: 0.4267 - MAE: 0.4267\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 475us/step - loss: 0.3949 - MAE: 0.3949\n",
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_106 (Dense)            (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dense_107 (Dense)            (1, 70)                   1050      \n",
      "_________________________________________________________________\n",
      "dense_108 (Dense)            (1, 7)                    497       \n",
      "_________________________________________________________________\n",
      "dense_109 (Dense)            (1, 1)                    8         \n",
      "=================================================================\n",
      "Total params: 1,681\n",
      "Trainable params: 1,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.36086323748218935\n"
     ]
    }
   ],
   "source": [
    "ModelDenseAdam5 = Sequential()\n",
    "ModelDenseAdam5.add(Dense(14))\n",
    "ModelDenseAdam5.add(Dense(70, activation='relu'))\n",
    "ModelDenseAdam5.add(Dense(7, activation='elu'))\n",
    "ModelDenseAdam5.add(Dense(1, activation='linear'))\n",
    "ModelDenseAdam5.compile(optimizer='Adam', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseAdam5.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseAdam5.summary()\n",
    "\n",
    "DenseAdamPrediction5 = ModelDenseAdam5.predict(x_test)\n",
    "score = mean_absolute_error(DenseAdamPrediction5, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_110 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 455us/step - loss: 6995.5537 - MAE: 6995.5537\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 432us/step - loss: 2520.0459 - MAE: 2520.0459\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 450us/step - loss: 1602.2378 - MAE: 1602.2378\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 433us/step - loss: 1100.0509 - MAE: 1100.0509\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 412us/step - loss: 506.7382 - MAE: 506.7382\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 420us/step - loss: 308.8519 - MAE: 308.8519\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 429us/step - loss: 0.3798 - MAE: 0.3798\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 472us/step - loss: 0.3418 - MAE: 0.3418\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 571us/step - loss: 0.3307 - MAE: 0.3307\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 522us/step - loss: 0.3159 - MAE: 0.3159\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 431us/step - loss: 0.3004 - MAE: 0.3004\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 426us/step - loss: 0.2965 - MAE: 0.2965\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 434us/step - loss: 0.2962 - MAE: 0.2962\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 455us/step - loss: 5.5510 - MAE: 5.5510\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 497us/step - loss: 0.3701 - MAE: 0.3701\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 440us/step - loss: 0.3633 - MAE: 0.3633\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 425us/step - loss: 0.3561 - MAE: 0.3561\n",
      "Epoch 18/50\n",
      "1398/1398 [==============================] - 1s 411us/step - loss: 0.3518 - MAE: 0.3518\n",
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_110 (Dense)            (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dense_111 (Dense)            (1, 70)                   1050      \n",
      "_________________________________________________________________\n",
      "dense_112 (Dense)            (1, 7)                    497       \n",
      "_________________________________________________________________\n",
      "dense_113 (Dense)            (1, 1)                    8         \n",
      "=================================================================\n",
      "Total params: 1,681\n",
      "Trainable params: 1,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.3281484951814901\n"
     ]
    }
   ],
   "source": [
    "ModelDenseAdam6 = Sequential()\n",
    "ModelDenseAdam6.add(Dense(14))\n",
    "ModelDenseAdam6.add(Dense(70))\n",
    "ModelDenseAdam6.add(Dense(7, activation='relu'))\n",
    "ModelDenseAdam6.add(Dense(1, activation='linear'))\n",
    "ModelDenseAdam6.compile(optimizer='Adam', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseAdam6.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseAdam6.summary()\n",
    "\n",
    "DenseAdamPrediction6 = ModelDenseAdam6.predict(x_test)\n",
    "score = mean_absolute_error(DenseAdamPrediction6, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_114 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 437us/step - loss: 1244.6208 - MAE: 1244.6208\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 459us/step - loss: 1.0866 - MAE: 1.0866\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 448us/step - loss: 0.3969 - MAE: 0.3969\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 439us/step - loss: 4.8855 - MAE: 4.8855\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 438us/step - loss: 0.3967 - MAE: 0.3967\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 471us/step - loss: 0.3970 - MAE: 0.3970\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 476us/step - loss: 0.3968 - MAE: 0.3968\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 459us/step - loss: 0.3968 - MAE: 0.3968\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 416us/step - loss: 0.5764 - MAE: 0.5764\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 424us/step - loss: 0.3966 - MAE: 0.3966\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 428us/step - loss: 0.3968 - MAE: 0.3968\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 418us/step - loss: 0.3966 - MAE: 0.3966\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 420us/step - loss: 0.3971 - MAE: 0.3971\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 420us/step - loss: 1.7145 - MAE: 1.7145\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 433us/step - loss: 0.3969 - MAE: 0.3969\n",
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_114 (Dense)            (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (1, 14)                   0         \n",
      "_________________________________________________________________\n",
      "dense_115 (Dense)            (1, 70)                   1050      \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (1, 70)                   0         \n",
      "_________________________________________________________________\n",
      "dense_116 (Dense)            (1, 7)                    497       \n",
      "_________________________________________________________________\n",
      "dense_117 (Dense)            (1, 1)                    8         \n",
      "=================================================================\n",
      "Total params: 1,681\n",
      "Trainable params: 1,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.3611452976146346\n"
     ]
    }
   ],
   "source": [
    "ModelDenseAdam6_1 = Sequential()\n",
    "ModelDenseAdam6_1.add(Dense(14))\n",
    "ModelDenseAdam6_1.add(Dropout(0.1))\n",
    "ModelDenseAdam6_1.add(Dense(70))\n",
    "ModelDenseAdam6_1.add(Dropout(0.1))\n",
    "ModelDenseAdam6_1.add(Dense(7, activation='relu'))\n",
    "ModelDenseAdam6_1.add(Dense(1, activation='linear'))\n",
    "ModelDenseAdam6_1.compile(optimizer='Adam', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseAdam6_1.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseAdam6_1.summary()\n",
    "\n",
    "DenseAdamPrediction6_1 = ModelDenseAdam6_1.predict(x_test)\n",
    "score = mean_absolute_error(DenseAdamPrediction6_1, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_118 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 446us/step - loss: 1367.8485 - MAE: 1367.8485\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 446us/step - loss: 255.0799 - MAE: 255.0799\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 448us/step - loss: 46.2763 - MAE: 46.2763\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 449us/step - loss: 7.0563 - MAE: 7.0563\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 450us/step - loss: 0.4785 - MAE: 0.4785\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 451us/step - loss: 0.3946 - MAE: 0.3946\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 447us/step - loss: 0.3858 - MAE: 0.3858\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 445us/step - loss: 19.7525 - MAE: 19.7525\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 492us/step - loss: 0.3671 - MAE: 0.3671\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 503us/step - loss: 0.3648 - MAE: 0.3648\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 501us/step - loss: 0.3475 - MAE: 0.3475\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 502us/step - loss: 0.3316 - MAE: 0.3316\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 497us/step - loss: 0.3507 - MAE: 0.3507\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 450us/step - loss: 0.3489 - MAE: 0.3489\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 444us/step - loss: 0.3425 - MAE: 0.3425\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 450us/step - loss: 0.4132 - MAE: 0.4132\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 450us/step - loss: 0.3741 - MAE: 0.3741\n",
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_118 (Dense)            (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dense_119 (Dense)            (1, 192)                  2880      \n",
      "_________________________________________________________________\n",
      "dense_120 (Dense)            (1, 24)                   4632      \n",
      "_________________________________________________________________\n",
      "dense_121 (Dense)            (1, 9)                    225       \n",
      "_________________________________________________________________\n",
      "dense_122 (Dense)            (1, 1)                    10        \n",
      "=================================================================\n",
      "Total params: 7,873\n",
      "Trainable params: 7,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.3402762457899465\n"
     ]
    }
   ],
   "source": [
    "ModelDenseAdam7 = Sequential()\n",
    "ModelDenseAdam7.add(Dense(14))\n",
    "ModelDenseAdam7.add(Dense(192, activation='relu'))\n",
    "ModelDenseAdam7.add(Dense(24, activation='relu'))\n",
    "ModelDenseAdam7.add(Dense(9, activation='relu'))\n",
    "ModelDenseAdam7.add(Dense(1, activation='linear'))\n",
    "ModelDenseAdam7.compile(optimizer='Adam', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseAdam7.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseAdam7.summary()\n",
    "\n",
    "DenseAdamPrediction7 = ModelDenseAdam7.predict(x_test)\n",
    "score = mean_absolute_error(DenseAdamPrediction7, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "variable=analytic 0.38850000000000967<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "analytic 0.38850000000000967",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "mode": "lines",
         "name": "analytic 0.38850000000000967",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          2,
          8,
          10,
          15,
          23,
          24,
          28,
          38,
          42,
          43,
          44,
          47,
          51,
          64,
          80,
          84,
          87,
          90,
          92,
          94,
          99,
          100,
          103,
          109,
          117,
          120,
          128,
          136,
          146,
          148,
          159,
          164,
          170,
          175,
          177,
          179,
          184,
          187,
          189,
          194,
          200,
          204,
          208,
          209,
          218,
          222,
          225,
          226,
          227,
          228,
          230,
          233,
          238,
          239,
          242,
          243,
          249,
          251,
          252,
          256,
          257,
          262,
          263,
          267,
          270,
          272,
          273,
          274,
          276,
          277,
          285,
          289,
          290,
          291,
          293,
          295,
          302,
          303,
          306,
          307,
          309,
          310,
          313,
          316,
          328,
          335,
          346,
          349,
          350,
          361,
          367,
          376,
          380,
          396,
          400,
          405,
          408,
          411,
          412,
          415,
          423,
          424,
          430,
          431,
          432,
          443,
          446,
          449,
          451,
          452,
          453,
          456,
          465,
          471,
          482,
          490,
          498,
          512,
          513,
          514,
          516,
          518,
          520,
          532,
          550,
          552,
          557,
          574,
          579,
          583,
          589,
          591,
          607,
          612,
          616,
          620,
          626,
          629,
          642,
          643,
          644,
          652,
          657,
          659,
          665,
          669,
          670,
          680,
          683,
          686,
          688,
          696,
          700,
          702,
          708,
          709,
          710,
          712,
          714,
          720,
          722,
          725,
          727,
          733,
          740,
          741,
          744,
          745,
          746,
          756,
          765,
          766,
          767,
          770,
          771,
          773,
          777,
          780,
          782,
          783,
          785,
          789,
          791,
          793,
          794,
          801,
          805,
          808,
          811,
          817,
          827,
          831,
          832,
          834,
          842,
          843,
          849,
          858,
          862,
          865,
          867,
          870,
          871,
          879,
          883,
          888,
          889,
          890,
          893,
          896,
          897,
          899,
          904,
          906,
          911,
          912,
          913,
          914,
          917,
          919,
          922,
          925,
          926,
          942,
          944,
          947,
          957,
          958,
          959,
          963,
          969,
          971,
          972,
          979,
          983,
          986,
          995,
          996,
          998,
          1001,
          1005,
          1007,
          1009,
          1016,
          1019,
          1023,
          1027,
          1029,
          1033,
          1036,
          1041,
          1042,
          1046,
          1049,
          1052,
          1054,
          1061,
          1065,
          1067,
          1074,
          1078,
          1082,
          1084,
          1087,
          1088,
          1104,
          1106,
          1107,
          1108,
          1114,
          1116,
          1119,
          1123,
          1126,
          1127,
          1138,
          1140,
          1149,
          1160,
          1161,
          1162,
          1171,
          1173,
          1179,
          1180,
          1182,
          1183,
          1185,
          1189,
          1196,
          1199,
          1204,
          1209,
          1215,
          1220,
          1224,
          1228,
          1229,
          1234,
          1235,
          1243,
          1254,
          1255,
          1258,
          1262,
          1264,
          1273,
          1274,
          1278,
          1290,
          1295,
          1298,
          1302,
          1304,
          1316,
          1318,
          1327,
          1328,
          1331,
          1334,
          1339,
          1344,
          1345,
          1346,
          1347,
          1353,
          1354,
          1358,
          1360,
          1362,
          1363,
          1367,
          1376,
          1385,
          1394,
          1396,
          1404,
          1405,
          1407,
          1415,
          1423,
          1424,
          1426,
          1432,
          1434,
          1438,
          1439,
          1441,
          1446,
          1452,
          1468,
          1470,
          1479,
          1482,
          1491,
          1492,
          1493,
          1502,
          1508,
          1511,
          1520,
          1522,
          1524,
          1528,
          1536,
          1537,
          1544,
          1545,
          1546,
          1551,
          1552,
          1557,
          1560,
          1561,
          1562,
          1563,
          1565,
          1567,
          1572,
          1576,
          1583,
          1588,
          1594,
          1597,
          1606,
          1609,
          1611,
          1616,
          1617,
          1629,
          1630,
          1631,
          1633,
          1635,
          1650,
          1652,
          1656,
          1657,
          1660,
          1668,
          1677,
          1678,
          1681,
          1692,
          1695,
          1697,
          1701,
          1704,
          1705,
          1706,
          1707,
          1710,
          1711,
          1712,
          1713,
          1714,
          1716,
          1717,
          1720,
          1721,
          1722,
          1723,
          1733,
          1737,
          1740,
          1741,
          1744,
          1745,
          1750,
          1752,
          1753,
          1756,
          1760,
          1769,
          1771,
          1772,
          1774,
          1780,
          1784,
          1785,
          1786,
          1792,
          1795,
          1797,
          1799,
          1800,
          1807,
          1808,
          1810,
          1814,
          1820,
          1821,
          1822,
          1827,
          1830,
          1831,
          1834,
          1835,
          1836,
          1839,
          1844,
          1846,
          1851,
          1852,
          1857,
          1861,
          1864
         ],
         "xaxis": "x",
         "y": [
          34.3702833908113,
          47.333195360630626,
          51.654166017237046,
          62.45659265875317,
          72.17877663611768,
          72.17877663611766,
          64.6170779870564,
          1120.292660925031,
          1154.8604261778828,
          1154.8604261778826,
          1154.8604261778828,
          1154.8604261778828,
          1154.8604261778828,
          1154.8604261778828,
          1154.8604261778828,
          1154.8604261778826,
          18477.76681884612,
          18477.766818846125,
          18477.766818846125,
          18477.76681884612,
          18477.766818846125,
          18477.766818846125,
          18477.76681884612,
          18477.766818846125,
          18477.766818846125,
          18477.766818846125,
          18477.766818846125,
          18477.766818846125,
          18477.766818846125,
          18477.766818846125,
          18477.766818846125,
          18477.766818846125,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          295644.269101538,
          101.2605627533404,
          106.95711985644272,
          128.79392208500155,
          124.04679116574964,
          103.15941512104119,
          2060.7027533600244,
          2060.702753360025,
          2060.702753360025,
          2060.702753360025,
          2060.702753360025,
          2060.702753360025,
          2060.702753360025,
          32971.2440537604,
          32971.2440537604,
          32971.2440537604,
          32971.2440537604,
          32971.2440537604,
          32971.2440537604,
          32971.2440537604,
          32971.2440537604,
          32971.2440537604,
          32971.2440537604,
          32971.2440537604,
          32971.2440537604,
          32971.2440537604,
          32971.2440537604,
          32971.2440537604,
          32971.2440537604,
          32971.2440537604,
          32971.2440537604,
          32971.2440537604,
          32971.2440537604,
          527539.9048601664,
          527539.9048601664,
          527539.9048601664,
          527539.9048601664,
          527539.9048601664,
          527539.9048601664,
          527539.9048601664,
          527539.9048601664,
          527539.9048601664,
          527539.9048601664,
          527539.9048601664,
          527539.9048601664,
          527539.9048601664,
          527539.9048601664,
          527539.9048601664,
          527539.9048601664,
          527539.9048601664,
          527539.9048601664,
          527539.9048601664,
          527539.9048601664,
          527539.9048601664,
          527539.9048601664,
          95.27783759963852,
          110.431405166506,
          172.53131931308081,
          172.53131931308081,
          172.53131931308081,
          165.99448624502028,
          140.73854030024108,
          130.6361619223294,
          2760.501109009292,
          2760.501109009292,
          2760.501109009293,
          2760.501109009293,
          2760.501109009293,
          2760.501109009292,
          2760.501109009293,
          2760.501109009293,
          2760.501109009292,
          2760.501109009292,
          2760.501109009292,
          2760.501109009292,
          2760.501109009292,
          2760.501109009293,
          2760.501109009293,
          44168.01774414869,
          44168.017744148674,
          44168.01774414869,
          44168.01774414869,
          44168.017744148674,
          44168.017744148674,
          44168.01774414869,
          44168.01774414869,
          44168.01774414869,
          44168.01774414869,
          44168.01774414869,
          44168.01774414869,
          44168.01774414869,
          44168.01774414869,
          44168.01774414869,
          44168.01774414869,
          44168.01774414869,
          44168.01774414869,
          44168.017744148674,
          44168.01774414869,
          44168.01774414869,
          44168.01774414869,
          44168.01774414869,
          44168.01774414869,
          44168.01774414869,
          44168.017744148674,
          706688.2839063788,
          706688.283906379,
          706688.2839063788,
          706688.283906379,
          706688.283906379,
          706688.283906379,
          706688.283906379,
          706688.283906379,
          706688.283906379,
          706688.283906379,
          706688.283906379,
          706688.283906379,
          706688.283906379,
          706688.283906379,
          706688.283906379,
          706688.283906379,
          706688.283906379,
          706688.283906379,
          706688.283906379,
          706688.283906379,
          706688.283906379,
          706688.283906379,
          706688.283906379,
          706688.283906379,
          706688.283906379,
          706688.283906379,
          706688.283906379,
          706688.283906379,
          706688.283906379,
          706688.283906379,
          706688.283906379,
          706688.283906379,
          706688.283906379,
          706688.283906379,
          706688.283906379,
          706688.283906379,
          706688.283906379,
          706688.283906379,
          706688.283906379,
          706688.2839063788,
          706688.283906379,
          307.0895627147497,
          312.1317015573819,
          317.1738404000141,
          328.7411000978174,
          328.7411000978174,
          327.25811808527857,
          322.2159792426463,
          286.9210073442208,
          266.7524519736919,
          5259.857601565079,
          5259.857601565079,
          5259.857601565079,
          5259.857601565079,
          5259.857601565079,
          5259.857601565079,
          5259.857601565079,
          5259.857601565079,
          5259.857601565079,
          5259.857601565079,
          5259.857601565079,
          5259.857601565079,
          5259.857601565079,
          5259.857601565079,
          84157.72162504126,
          84157.72162504126,
          84157.72162504126,
          84157.72162504126,
          84157.72162504126,
          84157.72162504126,
          84157.72162504126,
          84157.72162504126,
          84157.72162504126,
          84157.72162504126,
          84157.72162504126,
          84157.72162504126,
          84157.72162504126,
          84157.72162504126,
          84157.72162504126,
          84157.72162504126,
          84157.72162504126,
          84157.72162504126,
          84157.72162504126,
          84157.72162504126,
          84157.72162504126,
          84157.72162504126,
          1346523.5460006602,
          1346523.5460006602,
          1346523.5460006602,
          1346523.5460006602,
          1346523.5460006602,
          1346523.5460006602,
          1346523.5460006602,
          1346523.5460006602,
          1346523.5460006602,
          1346523.5460006602,
          1346523.5460006602,
          1346523.5460006602,
          1346523.5460006602,
          1346523.5460006602,
          1346523.5460006602,
          1346523.5460006602,
          1346523.5460006602,
          1346523.5460006602,
          1346523.5460006602,
          1346523.5460006602,
          1346523.5460006602,
          1346523.5460006602,
          1346523.5460006602,
          1346523.5460006602,
          1346523.5460006602,
          1346523.5460006602,
          1346523.5460006602,
          1346523.5460006602,
          1346523.5460006602,
          1346523.5460006602,
          1346523.5460006602,
          1346523.5460006602,
          1346523.5460006602,
          1346523.5460006602,
          1346523.5460006602,
          18.688432473055506,
          19.805309970492,
          24.272819960238007,
          35.58727460818167,
          29.857207447420503,
          26.506574955111002,
          22.039064965365004,
          531.32543903568,
          569.3963937309068,
          569.3963937309068,
          569.3963937309068,
          569.3963937309068,
          569.3963937309068,
          569.3963937309068,
          569.3963937309068,
          569.3963937309068,
          569.3963937309068,
          569.3963937309068,
          569.3963937309068,
          9110.342299694508,
          9110.342299694508,
          9110.342299694508,
          9110.342299694508,
          9110.342299694508,
          9110.342299694508,
          9110.342299694508,
          9110.342299694508,
          9110.342299694508,
          9110.342299694508,
          9110.342299694508,
          9110.342299694508,
          9110.342299694508,
          9110.342299694508,
          9110.342299694508,
          9110.342299694508,
          9110.342299694508,
          9110.342299694508,
          145765.47679511213,
          145765.47679511213,
          145765.47679511213,
          145765.47679511213,
          145765.47679511213,
          145765.47679511213,
          145765.47679511213,
          145765.47679511213,
          145765.47679511213,
          145765.47679511213,
          145765.47679511213,
          145765.47679511213,
          145765.47679511213,
          145765.47679511213,
          145765.47679511213,
          145765.47679511213,
          145765.47679511213,
          145765.47679511213,
          145765.47679511213,
          145765.47679511213,
          145765.47679511213,
          145765.47679511213,
          145765.47679511213,
          145765.47679511213,
          145765.47679511213,
          145765.47679511213,
          145765.47679511213,
          145765.47679511213,
          145765.47679511213,
          145765.47679511213,
          145765.47679511213,
          145765.47679511213,
          145765.47679511213,
          145765.47679511213,
          48.11936578306415,
          52.67397813301963,
          56.31766801298402,
          60.99109633728617,
          56.31766801298402,
          50.852133193037446,
          48.11936578306415,
          975.8575413965785,
          975.8575413965785,
          975.8575413965785,
          975.8575413965788,
          975.8575413965785,
          975.8575413965788,
          975.8575413965785,
          975.8575413965785,
          975.8575413965785,
          975.8575413965785,
          15613.720662345257,
          15613.72066234526,
          15613.720662345257,
          15613.72066234526,
          15613.72066234526,
          15613.720662345257,
          15613.72066234526,
          15613.72066234526,
          15613.720662345257,
          15613.72066234526,
          15613.72066234526,
          15613.72066234526,
          15613.72066234526,
          15613.72066234526,
          15613.72066234526,
          15613.72066234526,
          15613.72066234526,
          15613.720662345257,
          15613.720662345257,
          15613.720662345257,
          15613.720662345257,
          15613.72066234526,
          15613.720662345257,
          15613.72066234526,
          15613.720662345257,
          15613.720662345257,
          249819.53059752416,
          249819.5305975241,
          249819.5305975241,
          249819.5305975241,
          249819.5305975241,
          249819.5305975241,
          249819.53059752416,
          249819.5305975241,
          249819.53059752416,
          249819.5305975241,
          249819.5305975241,
          249819.5305975241,
          249819.5305975241,
          249819.5305975241,
          249819.53059752416,
          249819.5305975241,
          249819.5305975241,
          249819.5305975241,
          249819.5305975241,
          249819.5305975241,
          249819.5305975241,
          249819.5305975241,
          249819.5305975241,
          249819.5305975241,
          249819.5305975241,
          249819.5305975241,
          249819.5305975241,
          249819.5305975241,
          249819.5305975241,
          249819.5305975241,
          249819.53059752416,
          249819.5305975241,
          249819.5305975241,
          249819.5305975241,
          249819.5305975241,
          249819.5305975241,
          249819.5305975241,
          249819.5305975241,
          249819.53059752416,
          249819.5305975241,
          249819.53059752416,
          249819.5305975241,
          249819.5305975241,
          249819.53059752416,
          249819.53059752416,
          249819.5305975241,
          249819.53059752416
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "variable"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "index"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "value"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"592fe131-da33-4224-8379-6e5efbe76b38\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"592fe131-da33-4224-8379-6e5efbe76b38\")) {                    Plotly.newPlot(                        \"592fe131-da33-4224-8379-6e5efbe76b38\",                        [{\"hovertemplate\": \"variable=analytic 0.38850000000000967<br>index=%{x}<br>value=%{y}<extra></extra>\", \"legendgroup\": \"analytic 0.38850000000000967\", \"line\": {\"color\": \"#636efa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"analytic 0.38850000000000967\", \"orientation\": \"v\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [2, 8, 10, 15, 23, 24, 28, 38, 42, 43, 44, 47, 51, 64, 80, 84, 87, 90, 92, 94, 99, 100, 103, 109, 117, 120, 128, 136, 146, 148, 159, 164, 170, 175, 177, 179, 184, 187, 189, 194, 200, 204, 208, 209, 218, 222, 225, 226, 227, 228, 230, 233, 238, 239, 242, 243, 249, 251, 252, 256, 257, 262, 263, 267, 270, 272, 273, 274, 276, 277, 285, 289, 290, 291, 293, 295, 302, 303, 306, 307, 309, 310, 313, 316, 328, 335, 346, 349, 350, 361, 367, 376, 380, 396, 400, 405, 408, 411, 412, 415, 423, 424, 430, 431, 432, 443, 446, 449, 451, 452, 453, 456, 465, 471, 482, 490, 498, 512, 513, 514, 516, 518, 520, 532, 550, 552, 557, 574, 579, 583, 589, 591, 607, 612, 616, 620, 626, 629, 642, 643, 644, 652, 657, 659, 665, 669, 670, 680, 683, 686, 688, 696, 700, 702, 708, 709, 710, 712, 714, 720, 722, 725, 727, 733, 740, 741, 744, 745, 746, 756, 765, 766, 767, 770, 771, 773, 777, 780, 782, 783, 785, 789, 791, 793, 794, 801, 805, 808, 811, 817, 827, 831, 832, 834, 842, 843, 849, 858, 862, 865, 867, 870, 871, 879, 883, 888, 889, 890, 893, 896, 897, 899, 904, 906, 911, 912, 913, 914, 917, 919, 922, 925, 926, 942, 944, 947, 957, 958, 959, 963, 969, 971, 972, 979, 983, 986, 995, 996, 998, 1001, 1005, 1007, 1009, 1016, 1019, 1023, 1027, 1029, 1033, 1036, 1041, 1042, 1046, 1049, 1052, 1054, 1061, 1065, 1067, 1074, 1078, 1082, 1084, 1087, 1088, 1104, 1106, 1107, 1108, 1114, 1116, 1119, 1123, 1126, 1127, 1138, 1140, 1149, 1160, 1161, 1162, 1171, 1173, 1179, 1180, 1182, 1183, 1185, 1189, 1196, 1199, 1204, 1209, 1215, 1220, 1224, 1228, 1229, 1234, 1235, 1243, 1254, 1255, 1258, 1262, 1264, 1273, 1274, 1278, 1290, 1295, 1298, 1302, 1304, 1316, 1318, 1327, 1328, 1331, 1334, 1339, 1344, 1345, 1346, 1347, 1353, 1354, 1358, 1360, 1362, 1363, 1367, 1376, 1385, 1394, 1396, 1404, 1405, 1407, 1415, 1423, 1424, 1426, 1432, 1434, 1438, 1439, 1441, 1446, 1452, 1468, 1470, 1479, 1482, 1491, 1492, 1493, 1502, 1508, 1511, 1520, 1522, 1524, 1528, 1536, 1537, 1544, 1545, 1546, 1551, 1552, 1557, 1560, 1561, 1562, 1563, 1565, 1567, 1572, 1576, 1583, 1588, 1594, 1597, 1606, 1609, 1611, 1616, 1617, 1629, 1630, 1631, 1633, 1635, 1650, 1652, 1656, 1657, 1660, 1668, 1677, 1678, 1681, 1692, 1695, 1697, 1701, 1704, 1705, 1706, 1707, 1710, 1711, 1712, 1713, 1714, 1716, 1717, 1720, 1721, 1722, 1723, 1733, 1737, 1740, 1741, 1744, 1745, 1750, 1752, 1753, 1756, 1760, 1769, 1771, 1772, 1774, 1780, 1784, 1785, 1786, 1792, 1795, 1797, 1799, 1800, 1807, 1808, 1810, 1814, 1820, 1821, 1822, 1827, 1830, 1831, 1834, 1835, 1836, 1839, 1844, 1846, 1851, 1852, 1857, 1861, 1864], \"xaxis\": \"x\", \"y\": [34.3702833908113, 47.333195360630626, 51.654166017237046, 62.45659265875317, 72.17877663611768, 72.17877663611766, 64.6170779870564, 1120.292660925031, 1154.8604261778828, 1154.8604261778826, 1154.8604261778828, 1154.8604261778828, 1154.8604261778828, 1154.8604261778828, 1154.8604261778828, 1154.8604261778826, 18477.76681884612, 18477.766818846125, 18477.766818846125, 18477.76681884612, 18477.766818846125, 18477.766818846125, 18477.76681884612, 18477.766818846125, 18477.766818846125, 18477.766818846125, 18477.766818846125, 18477.766818846125, 18477.766818846125, 18477.766818846125, 18477.766818846125, 18477.766818846125, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 295644.269101538, 101.2605627533404, 106.95711985644272, 128.79392208500155, 124.04679116574964, 103.15941512104119, 2060.7027533600244, 2060.702753360025, 2060.702753360025, 2060.702753360025, 2060.702753360025, 2060.702753360025, 2060.702753360025, 32971.2440537604, 32971.2440537604, 32971.2440537604, 32971.2440537604, 32971.2440537604, 32971.2440537604, 32971.2440537604, 32971.2440537604, 32971.2440537604, 32971.2440537604, 32971.2440537604, 32971.2440537604, 32971.2440537604, 32971.2440537604, 32971.2440537604, 32971.2440537604, 32971.2440537604, 32971.2440537604, 32971.2440537604, 32971.2440537604, 527539.9048601664, 527539.9048601664, 527539.9048601664, 527539.9048601664, 527539.9048601664, 527539.9048601664, 527539.9048601664, 527539.9048601664, 527539.9048601664, 527539.9048601664, 527539.9048601664, 527539.9048601664, 527539.9048601664, 527539.9048601664, 527539.9048601664, 527539.9048601664, 527539.9048601664, 527539.9048601664, 527539.9048601664, 527539.9048601664, 527539.9048601664, 527539.9048601664, 95.27783759963852, 110.431405166506, 172.53131931308081, 172.53131931308081, 172.53131931308081, 165.99448624502028, 140.73854030024108, 130.6361619223294, 2760.501109009292, 2760.501109009292, 2760.501109009293, 2760.501109009293, 2760.501109009293, 2760.501109009292, 2760.501109009293, 2760.501109009293, 2760.501109009292, 2760.501109009292, 2760.501109009292, 2760.501109009292, 2760.501109009292, 2760.501109009293, 2760.501109009293, 44168.01774414869, 44168.017744148674, 44168.01774414869, 44168.01774414869, 44168.017744148674, 44168.017744148674, 44168.01774414869, 44168.01774414869, 44168.01774414869, 44168.01774414869, 44168.01774414869, 44168.01774414869, 44168.01774414869, 44168.01774414869, 44168.01774414869, 44168.01774414869, 44168.01774414869, 44168.01774414869, 44168.017744148674, 44168.01774414869, 44168.01774414869, 44168.01774414869, 44168.01774414869, 44168.01774414869, 44168.01774414869, 44168.017744148674, 706688.2839063788, 706688.283906379, 706688.2839063788, 706688.283906379, 706688.283906379, 706688.283906379, 706688.283906379, 706688.283906379, 706688.283906379, 706688.283906379, 706688.283906379, 706688.283906379, 706688.283906379, 706688.283906379, 706688.283906379, 706688.283906379, 706688.283906379, 706688.283906379, 706688.283906379, 706688.283906379, 706688.283906379, 706688.283906379, 706688.283906379, 706688.283906379, 706688.283906379, 706688.283906379, 706688.283906379, 706688.283906379, 706688.283906379, 706688.283906379, 706688.283906379, 706688.283906379, 706688.283906379, 706688.283906379, 706688.283906379, 706688.283906379, 706688.283906379, 706688.283906379, 706688.283906379, 706688.2839063788, 706688.283906379, 307.0895627147497, 312.1317015573819, 317.1738404000141, 328.7411000978174, 328.7411000978174, 327.25811808527857, 322.2159792426463, 286.9210073442208, 266.7524519736919, 5259.857601565079, 5259.857601565079, 5259.857601565079, 5259.857601565079, 5259.857601565079, 5259.857601565079, 5259.857601565079, 5259.857601565079, 5259.857601565079, 5259.857601565079, 5259.857601565079, 5259.857601565079, 5259.857601565079, 5259.857601565079, 84157.72162504126, 84157.72162504126, 84157.72162504126, 84157.72162504126, 84157.72162504126, 84157.72162504126, 84157.72162504126, 84157.72162504126, 84157.72162504126, 84157.72162504126, 84157.72162504126, 84157.72162504126, 84157.72162504126, 84157.72162504126, 84157.72162504126, 84157.72162504126, 84157.72162504126, 84157.72162504126, 84157.72162504126, 84157.72162504126, 84157.72162504126, 84157.72162504126, 1346523.5460006602, 1346523.5460006602, 1346523.5460006602, 1346523.5460006602, 1346523.5460006602, 1346523.5460006602, 1346523.5460006602, 1346523.5460006602, 1346523.5460006602, 1346523.5460006602, 1346523.5460006602, 1346523.5460006602, 1346523.5460006602, 1346523.5460006602, 1346523.5460006602, 1346523.5460006602, 1346523.5460006602, 1346523.5460006602, 1346523.5460006602, 1346523.5460006602, 1346523.5460006602, 1346523.5460006602, 1346523.5460006602, 1346523.5460006602, 1346523.5460006602, 1346523.5460006602, 1346523.5460006602, 1346523.5460006602, 1346523.5460006602, 1346523.5460006602, 1346523.5460006602, 1346523.5460006602, 1346523.5460006602, 1346523.5460006602, 1346523.5460006602, 18.688432473055506, 19.805309970492, 24.272819960238007, 35.58727460818167, 29.857207447420503, 26.506574955111002, 22.039064965365004, 531.32543903568, 569.3963937309068, 569.3963937309068, 569.3963937309068, 569.3963937309068, 569.3963937309068, 569.3963937309068, 569.3963937309068, 569.3963937309068, 569.3963937309068, 569.3963937309068, 569.3963937309068, 9110.342299694508, 9110.342299694508, 9110.342299694508, 9110.342299694508, 9110.342299694508, 9110.342299694508, 9110.342299694508, 9110.342299694508, 9110.342299694508, 9110.342299694508, 9110.342299694508, 9110.342299694508, 9110.342299694508, 9110.342299694508, 9110.342299694508, 9110.342299694508, 9110.342299694508, 9110.342299694508, 145765.47679511213, 145765.47679511213, 145765.47679511213, 145765.47679511213, 145765.47679511213, 145765.47679511213, 145765.47679511213, 145765.47679511213, 145765.47679511213, 145765.47679511213, 145765.47679511213, 145765.47679511213, 145765.47679511213, 145765.47679511213, 145765.47679511213, 145765.47679511213, 145765.47679511213, 145765.47679511213, 145765.47679511213, 145765.47679511213, 145765.47679511213, 145765.47679511213, 145765.47679511213, 145765.47679511213, 145765.47679511213, 145765.47679511213, 145765.47679511213, 145765.47679511213, 145765.47679511213, 145765.47679511213, 145765.47679511213, 145765.47679511213, 145765.47679511213, 145765.47679511213, 48.11936578306415, 52.67397813301963, 56.31766801298402, 60.99109633728617, 56.31766801298402, 50.852133193037446, 48.11936578306415, 975.8575413965785, 975.8575413965785, 975.8575413965785, 975.8575413965788, 975.8575413965785, 975.8575413965788, 975.8575413965785, 975.8575413965785, 975.8575413965785, 975.8575413965785, 15613.720662345257, 15613.72066234526, 15613.720662345257, 15613.72066234526, 15613.72066234526, 15613.720662345257, 15613.72066234526, 15613.72066234526, 15613.720662345257, 15613.72066234526, 15613.72066234526, 15613.72066234526, 15613.72066234526, 15613.72066234526, 15613.72066234526, 15613.72066234526, 15613.72066234526, 15613.720662345257, 15613.720662345257, 15613.720662345257, 15613.720662345257, 15613.72066234526, 15613.720662345257, 15613.72066234526, 15613.720662345257, 15613.720662345257, 249819.53059752416, 249819.5305975241, 249819.5305975241, 249819.5305975241, 249819.5305975241, 249819.5305975241, 249819.53059752416, 249819.5305975241, 249819.53059752416, 249819.5305975241, 249819.5305975241, 249819.5305975241, 249819.5305975241, 249819.5305975241, 249819.53059752416, 249819.5305975241, 249819.5305975241, 249819.5305975241, 249819.5305975241, 249819.5305975241, 249819.5305975241, 249819.5305975241, 249819.5305975241, 249819.5305975241, 249819.5305975241, 249819.5305975241, 249819.5305975241, 249819.5305975241, 249819.5305975241, 249819.5305975241, 249819.53059752416, 249819.5305975241, 249819.5305975241, 249819.5305975241, 249819.5305975241, 249819.5305975241, 249819.5305975241, 249819.5305975241, 249819.53059752416, 249819.5305975241, 249819.53059752416, 249819.5305975241, 249819.5305975241, 249819.53059752416, 249819.53059752416, 249819.5305975241, 249819.53059752416], \"yaxis\": \"y\"}],                        {\"legend\": {\"title\": {\"text\": \"variable\"}, \"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"index\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"value\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('592fe131-da33-4224-8379-6e5efbe76b38');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_test.iloc[:,-1:].sort_index().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 1)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(scores).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.276132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.278961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.281304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.284142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.284199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.290596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.292454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.306475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.317581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.320971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.327350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.328148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.331298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.340276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.360844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.360863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.360869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.360884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.361145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.361196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.361281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.361287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.361352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.362300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.362336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.362395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.363321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.363517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.367157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.368383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "6   0.276132\n",
       "9   0.278961\n",
       "2   0.281304\n",
       "24  0.284142\n",
       "5   0.284199\n",
       "4   0.290596\n",
       "12  0.292454\n",
       "16  0.306475\n",
       "7   0.317581\n",
       "25  0.320971\n",
       "14  0.327350\n",
       "27  0.328148\n",
       "15  0.331298\n",
       "29  0.340276\n",
       "1   0.360844\n",
       "26  0.360863\n",
       "21  0.360869\n",
       "10  0.360884\n",
       "28  0.361145\n",
       "8   0.361196\n",
       "11  0.361281\n",
       "18  0.361287\n",
       "17  0.361352\n",
       "0   0.362300\n",
       "19  0.362336\n",
       "22  0.362395\n",
       "20  0.363321\n",
       "3   0.363517\n",
       "23  0.367157\n",
       "13  0.368383"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(scores).sort_values(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r</th>\n",
       "      <th>Area</th>\n",
       "      <th>In</th>\n",
       "      <th>Z</th>\n",
       "      <th>tplus</th>\n",
       "      <th>tminus</th>\n",
       "      <th>tminus_</th>\n",
       "      <th>analytic 0.38850000000000967</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>0.017656</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.738067</td>\n",
       "      <td>0.608110</td>\n",
       "      <td>0.733957</td>\n",
       "      <td>527539.904860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0.653277</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.349666</td>\n",
       "      <td>0.400344</td>\n",
       "      <td>0.733957</td>\n",
       "      <td>295644.269102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>0.375193</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.381512</td>\n",
       "      <td>0.373447</td>\n",
       "      <td>0.733957</td>\n",
       "      <td>295644.269102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1391</th>\n",
       "      <td>0.015228</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.101948</td>\n",
       "      <td>0.067172</td>\n",
       "      <td>0.137838</td>\n",
       "      <td>9110.342300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1570</th>\n",
       "      <td>0.111675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.007982</td>\n",
       "      <td>0.018392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.852133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0.344295</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.081755</td>\n",
       "      <td>0.114293</td>\n",
       "      <td>0.176597</td>\n",
       "      <td>18477.766819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>0.150077</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.034517</td>\n",
       "      <td>0.059596</td>\n",
       "      <td>0.037257</td>\n",
       "      <td>2060.702753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>0.011256</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.026482</td>\n",
       "      <td>0.003774</td>\n",
       "      <td>0.006579</td>\n",
       "      <td>172.531319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1781</th>\n",
       "      <td>0.172589</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.563224</td>\n",
       "      <td>0.501488</td>\n",
       "      <td>0.578920</td>\n",
       "      <td>249819.530598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1480</th>\n",
       "      <td>0.233503</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.331250</td>\n",
       "      <td>0.295633</td>\n",
       "      <td>0.578920</td>\n",
       "      <td>145765.476795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1398 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             r      Area   In    Z     tplus    tminus   tminus_  \\\n",
       "547   0.017656  1.000000  1.0  0.5  0.738067  0.608110  0.733957   \n",
       "193   0.653277  1.000000  0.0  0.5  0.349666  0.400344  0.733957   \n",
       "214   0.375193  1.000000  0.0  0.5  0.381512  0.373447  0.733957   \n",
       "1391  0.015228  0.238095  0.0  1.0  0.101948  0.067172  0.137838   \n",
       "1570  0.111675  0.000000  1.0  1.0  0.007982  0.018392  0.000000   \n",
       "...        ...       ...  ...  ...       ...       ...       ...   \n",
       "156   0.344295  0.238095  0.0  0.5  0.081755  0.114293  0.176597   \n",
       "384   0.150077  0.047619  1.0  0.5  0.034517  0.059596  0.037257   \n",
       "645   0.011256  0.000000  0.0  0.0  0.026482  0.003774  0.006579   \n",
       "1781  0.172589  1.000000  1.0  1.0  0.563224  0.501488  0.578920   \n",
       "1480  0.233503  1.000000  0.0  1.0  0.331250  0.295633  0.578920   \n",
       "\n",
       "      analytic 0.38850000000000967  \n",
       "547                  527539.904860  \n",
       "193                  295644.269102  \n",
       "214                  295644.269102  \n",
       "1391                   9110.342300  \n",
       "1570                     50.852133  \n",
       "...                            ...  \n",
       "156                   18477.766819  \n",
       "384                    2060.702753  \n",
       "645                     172.531319  \n",
       "1781                 249819.530598  \n",
       "1480                 145765.476795  \n",
       "\n",
       "[1398 rows x 8 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "variable=Data<br>r=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "Data",
         "marker": {
          "color": "#636efa",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "Data",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          0.12690355329949238,
          0.7391304347826085,
          0.22886779960273668,
          0.9847715736040608,
          0.49746192893401014,
          0.2182741116751269,
          0.4016773339218715,
          0.8695652173913043,
          0.06598984771573603,
          0.03089825645552858,
          0.45464577355992053,
          0.13242109909512248,
          0.42639593908629436,
          0.1500772456411388,
          0.004414036636504083,
          0.09710880600308981,
          0.21761200617965126,
          0.5685279187817259,
          0.033767380269256235,
          0.056278967115427056,
          0.7316265725005517,
          0.598984771573604,
          0.481129993378945,
          0.5473405429265062,
          0.21761200617965126,
          0.24762745530787905,
          0.9847715736040608,
          0.4619289340101523,
          0.5473405429265062,
          0.3575369675568307,
          0.07945265945707349,
          0.07879055396159788,
          0.47208121827411165,
          0.8474950342087839,
          0.4016773339218715,
          0.13705583756345177,
          0.7541381593467225,
          0.1986316486426837,
          0.3502538071065989,
          0.02626351798719929,
          0.12381372765393953,
          0.09644670050761421,
          0.11917898918561023,
          0.8783932906643125,
          0.11255793423085411,
          0.04502317369234164,
          0.7856985212977267,
          0.47208121827411165,
          0.15758110792319577,
          0.10505407194879716,
          0.1765614654601633,
          0.26484219819024496,
          0.7360406091370558,
          0.7241227102184948,
          0.01125579342308541,
          0.8730964467005076,
          0.1500772456411388,
          0.06378282939748399,
          0.020304568527918777,
          0.5890531891414698,
          0.4314720812182741,
          0.044140366365040824,
          0.08629441624365482,
          0.4089604943721033,
          0.2138600750386228,
          0.7166188479364378,
          0.36393732067976164,
          0.28139483557713524,
          0.18009269476936657,
          0.49746192893401014,
          0.12756565879496798,
          0.09379827852571175,
          0.10505407194879716,
          0.16883690134628115,
          0.02626351798719929,
          0.22070183182520414,
          0.06003089825645552,
          0.6797616420216287,
          0.4923857868020304,
          0.14632531450011033,
          0.2030456852791878,
          0.07128669167954092,
          0.4619289340101523,
          0.2639593908629441,
          0.18097550209666738,
          0.416243654822335,
          0.11675126903553298,
          0.964467005076142,
          0.2030456852791878,
          0.4414036636504083,
          0.16508497020525267,
          1,
          0.05076142131979695,
          0.7724564113882144,
          0.07945265945707349,
          0.015228426395939085,
          0.22511586846170822,
          0.9340101522842639,
          0.0375193114102847,
          0.10593687927609799,
          0.12690355329949238,
          0.14213197969543148,
          0.11630986537188258,
          0.0812182741116751,
          0.12381372765393953,
          0.8426395939086294,
          0.5440300154491282,
          0.2401235930258221,
          0.3096446700507614,
          0.04502317369234164,
          0.38578680203045684,
          0.026484219819024497,
          0.2030456852791878,
          0.11167512690355329,
          0.2780843080997572,
          0.06598984771573603,
          0.598984771573604,
          0.2182741116751269,
          0.08254248510262634,
          0.4652394614875303,
          0.16331935555065105,
          0.3310527477378062,
          0.17258883248730963,
          0.3264180092694769,
          0.09710880600308981,
          0.13242109909512248,
          0.1013021408077687,
          0.013242109909512249,
          0.964467005076142,
          0.09710880600308981,
          0.3959390862944162,
          0.750386228205694,
          0.0375193114102847,
          0.1388214522180534,
          0.6294416243654821,
          0.1765614654601633,
          0.4771573604060913,
          0.375193114102847,
          0.2081218274111675,
          0.19885235047450892,
          0.18009269476936657,
          0.7208121827411167,
          0.7391304347826085,
          0.09379827852571175,
          0.10659898477157359,
          0.851909070845288,
          0.02251158684617082,
          0.26925623482674904,
          0.5605826528360185,
          0.37144118296181855,
          0.13506952107702494,
          0.0812182741116751,
          0.048775104833370116,
          0.949238578680203,
          0.1878172588832487,
          0.3264180092694769,
          0.5440300154491282,
          0.6565879496799822,
          0.05076142131979695,
          0.3564334583977047,
          0.16331935555065105,
          0.42021628779518866,
          0.2182741116751269,
          0.22511586846170822,
          0.365482233502538,
          0.05076142131979695,
          0.45464577355992053,
          0.3096446700507614,
          0.322224674464798,
          0.9187817258883249,
          0.057382476274553076,
          0.6903553299492385,
          0.6115647759876407,
          0.6903553299492385,
          0.42374751710439196,
          0.42816155374089604,
          0.01765614654601633,
          0.375193114102847,
          0.42374751710439196,
          0.11167512690355329,
          0.12690355329949238,
          0.7208121827411167,
          0.4771573604060913,
          0.34142573383359076,
          0.38071065989847713,
          0.35268152725667623,
          0.026484219819024497,
          0.06091370558375634,
          0.46700507614213194,
          0.13197969543147206,
          0.6267932023835797,
          0.1015228426395939,
          0.1538291767821673,
          0.3076583535643346,
          0.6142131979695431,
          0.2513793864489075,
          0.7592143014787022,
          0.6003089825645552,
          0.06753476053851247,
          0.17214742882365922,
          0.15890531891414697,
          0.03553299492385787,
          0.28139483557713524,
          0.2030456852791878,
          0.2295299050982123,
          1,
          0.11630986537188258,
          0.2030456852791878,
          0.8386669609357756,
          0.182741116751269,
          0.9949238578680203,
          0.23835797837122047,
          0.5025380710659898,
          0.29640256014124916,
          0.6598984771573604,
          0.39770470094901783,
          0.1013021408077687,
          0.30390642242330607,
          0.22511586846170822,
          0.12756565879496798,
          0.06753476053851247,
          0.17214742882365922,
          0.09269476936658573,
          0.005076142131979694,
          0.1979695431472081,
          0.7512690355329948,
          0.766497461928934,
          0.17258883248730963,
          0.10659898477157359,
          0.6135510924740675,
          0.31516221584639154,
          0.3972632972853674,
          0.5532994923857867,
          0.1500772456411388,
          0.17214742882365922,
          0.41116751269035534,
          0.6142131979695431,
          0.851909070845288,
          0.31979695431472077,
          0.38843522401235925,
          0.8883248730964466,
          0.3248730964467004,
          0.11255793423085411,
          0.30390642242330607,
          0.375193114102847,
          0.17258883248730963,
          0.21319796954314718,
          0.015228426395939085,
          0.08629441624365482,
          0.06003089825645552,
          0.47649525491061573,
          0.8474950342087839,
          0.044140366365040824,
          0.7166188479364378,
          0.9695431472081217,
          0.7969543147208121,
          0.322224674464798,
          0.026484219819024497,
          0.38402118737585517,
          0.4619289340101523,
          0.4016773339218715,
          0.056278967115427056,
          0.06621054954756124,
          0.4414036636504083,
          0.35268152725667623,
          0.1388214522180534,
          0.12690355329949238,
          0.18384462591039502,
          0.1875965570514235,
          0.19134848819245198,
          0.7062458618406532,
          0.10593687927609799,
          0.4678878834694328,
          0.7724564113882144,
          0.16883690134628115,
          0.2791878172588832,
          0.38402118737585517,
          0.44162436548223344,
          0.27411167512690354,
          0.45685279187817257,
          0.005076142131979694,
          0.6142131979695431,
          0.06003089825645552,
          0.11476495254910615,
          0.19421761200617962,
          0.45685279187817257,
          0.11255793423085411,
          0.40101522842639586,
          0.25888324873096447,
          0.8578680203045684,
          0.25888324873096447,
          0.13683513573162656,
          0.19510041933348046,
          0.34142573383359076,
          0.1613330390642242,
          0.0913705583756345,
          0.06621054954756124,
          0.0375193114102847,
          0.3564334583977047,
          0.18009269476936657,
          0.02626351798719929,
          0.416243654822335,
          0.15758110792319577,
          0.18009269476936657,
          0.0812182741116751,
          0.7512690355329948,
          0.416243654822335,
          0.24873096446700507,
          0.056278967115427056,
          0.0812182741116751,
          0.4414036636504083,
          0.2401235930258221,
          0.6665195321121165,
          0.21628779518870006,
          0.8426395939086294,
          0.21187375855219598,
          0.07062458618406532,
          0.964467005076142,
          0.41491944383138374,
          0.7327300816596777,
          0.11630986537188258,
          0.11167512690355329,
          0.020304568527918777,
          0.13242109909512248,
          0.36393732067976164,
          0.7278746413595233,
          0.07614213197969542,
          0.004414036636504083,
          0.29640256014124916,
          0.4016773339218715,
          0.7203707790774663,
          0.42816155374089604,
          0.22842639593908629,
          0.14213197969543148,
          0.0812182741116751,
          0.4771573604060913,
          0.2560141249172368,
          0.4193334804678878,
          0,
          0.45398366806444485,
          0.24365482233502536,
          0.8254248510262634,
          0.36768925182079004,
          0.17258883248730963,
          0.34142573383359076,
          0.4652394614875303,
          0.8386669609357756,
          0.1613330390642242,
          0.07062458618406532,
          0.09379827852571175,
          0.09379827852571175,
          0.3864489075259324,
          0.07879055396159788,
          0.1388214522180534,
          0.37894504524387546,
          0.36018538953873314,
          0.6446700507614214,
          0.182741116751269,
          0.640035312293092,
          0.3707790774663429,
          0.39770470094901783,
          0.851909070845288,
          0.03553299492385787,
          0.3350253807106599,
          0.22842639593908629,
          0.1500772456411388,
          0.12359302582211432,
          0.6532774222026042,
          0.6790995365261531,
          0.46700507614213194,
          0.7391304347826085,
          0.06003089825645552,
          0.21628779518870006,
          0.05076142131979695,
          0.33016994041050535,
          0.3489295961156477,
          0.057382476274553076,
          0.22070183182520414,
          0.4923857868020304,
          0.4990068417567865,
          0.21187375855219598,
          0.24277201500772455,
          0.17258883248730963,
          0.45398366806444485,
          0.4771573604060913,
          0.11255793423085411,
          0.8274111675126903,
          0.0750386228205694,
          0.21628779518870006,
          0.7278746413595233,
          0.12690355329949238,
          0.48775104833370114,
          0.09710880600308981,
          0.18384462591039502,
          0.3864489075259324,
          0.35532994923857864,
          0.12690355329949238,
          0.052968439638048995,
          0.24873096446700507,
          0.5102626351798719,
          0.5208563231074818,
          0.9746192893401014,
          0.03045685279187817,
          0.033767380269256235,
          0.7353785036415801,
          0.3489295961156477,
          0.5532994923857867,
          0.3376738026925623,
          0.00375193114102847,
          0.31781063782829394,
          0.12756565879496798,
          0.3502538071065989,
          0.48775104833370114,
          0.033767380269256235,
          0.8274111675126903,
          0.23835797837122047,
          0.21628779518870006,
          0.08254248510262634,
          0.8254248510262634,
          0.03045685279187817,
          0.07128669167954092,
          0.7360406091370558,
          0.38578680203045684,
          0.06003089825645552,
          0.41116751269035534,
          0.07879055396159788,
          0.44162436548223344,
          0.7203707790774663,
          0.8563231074817921,
          0.09644670050761421,
          0.1986316486426837,
          0.38843522401235925,
          0.21187375855219598,
          0.2081218274111675,
          0.4325755903774001,
          0.8730964467005076,
          0.3487088942838225,
          0.5076142131979695,
          0.34142573383359076,
          0.7969543147208121,
          0.3089825645552858,
          0.21319796954314718,
          0.5552858088722136,
          0.34142573383359076,
          0.2780843080997572,
          0.5552858088722136,
          0.18980357536967554,
          0.781725888324873,
          0.7724564113882144,
          0.9035532994923856,
          0.17258883248730963,
          0.048775104833370116,
          0.375193114102847,
          0.21187375855219598,
          0.18384462591039502,
          0.6446700507614214,
          0.11035091591260207,
          0.09644670050761421,
          0.06003089825645552,
          0.45464577355992053,
          0.5076142131979695,
          0.3502538071065989,
          0.36393732067976164,
          0.35532994923857864,
          0.4414036636504083,
          0.2030456852791878,
          0.033767380269256235
         ],
         "xaxis": "x",
         "y": [
          0.6061,
          0.5017199999999999,
          1.0674,
          0.61416,
          0.6765100000000001,
          0.61165,
          0.9469799999999999,
          0.6339899999999999,
          0.8710399999999999,
          0.7925800000000001,
          0.81788,
          0.93976,
          0.71763,
          1.1096,
          0.7520899999999999,
          1.1566,
          1.1199,
          0.62097,
          1.2945,
          1.2389,
          1.0221,
          0.6221800000000001,
          0.8186399999999999,
          0.7819699999999999,
          1.0676,
          1.26,
          0.50716,
          0.71532,
          0.82004,
          0.90781,
          0.93891,
          1.3026,
          0.71337,
          0.75536,
          0.77772,
          0.7104699999999999,
          0.85883,
          0.9490700000000001,
          0.68693,
          1.0944,
          1.2454,
          2.3007,
          0.6539699999999999,
          0.7548699999999999,
          1.304,
          3.6409,
          0.7864899999999999,
          0.7125100000000001,
          1.3705,
          1.5278,
          0.76707,
          0.95321,
          0.62507,
          1.1282,
          1.2924,
          0.62537,
          1.059,
          4.041,
          0.82965,
          1.1388,
          0.61888,
          1.1538,
          2.5568,
          1.0779,
          1.2554,
          1.0834,
          1.1275,
          1.3205,
          1.2487,
          0.51078,
          1.5303,
          1.6032,
          1.1025,
          1.4478,
          1.042,
          0.98244,
          4.0506,
          0.786,
          0.6197600000000001,
          1.2499,
          0.61002,
          3.9577,
          0.71529,
          0.56183,
          1.1025,
          0.58934,
          1.2188,
          0.62227,
          0.83265,
          0.77761,
          1.0894,
          0.50512,
          0.8342700000000001,
          0.8252200000000001,
          0.8936700000000001,
          2.6169,
          1.2589,
          0.59665,
          1.5312,
          1.6153,
          0.91669,
          0.8723200000000001,
          1.1024,
          2.2049,
          1.3052,
          0.62634,
          1.0841,
          1.121,
          0.6164,
          1.232,
          0.58841,
          2.9172,
          0.7659699999999999,
          0.87284,
          0.90636,
          0.57326,
          0.59299,
          0.77994,
          1.1013,
          1.1321,
          0.76658,
          0.9528,
          0.57953,
          1.2507,
          0.79771,
          1.4552,
          2.8378,
          3.0621,
          0.59442,
          3.05,
          0.7203,
          0.60868,
          1.5251,
          1.1069,
          0.6221399999999999,
          1.0976,
          0.68219,
          0.8156,
          0.8269200000000001,
          0.92106,
          1.3173,
          0.5956600000000001,
          1.0777,
          1.5293,
          0.67374,
          0.81921,
          3.6377,
          0.7723,
          0.7803800000000001,
          0.7228,
          1.3067,
          2.5994,
          1.0418,
          0.59595,
          0.60818,
          1.3138,
          1.0811,
          1.0846,
          0.5716399999999999,
          1.2497,
          0.76059,
          1.0776,
          0.58305,
          0.95134,
          0.7205600000000001,
          0.6025,
          0.77841,
          0.6158100000000001,
          0.81415,
          0.5969399999999999,
          1.154,
          1.1347,
          1.139,
          0.62342,
          0.84915,
          0.81657,
          1.0968,
          1.1299,
          0.9006200000000001,
          1.806,
          0.8726799999999999,
          0.6248,
          0.59102,
          1.1268,
          0.68791,
          0.91755,
          0.88476,
          0.67375,
          0.66804,
          2.2026,
          0.78492,
          2.1216,
          1.6006,
          1.0735,
          0.59285,
          1.0693,
          0.8242799999999999,
          0.78452,
          1.2393,
          0.8959,
          0.9436899999999999,
          0.82934,
          1.2585,
          0.57904,
          0.7702899999999999,
          0.5269699999999999,
          1.0492,
          0.87255,
          0.78492,
          0.71281,
          0.57772,
          0.50015,
          0.66274,
          1.0686,
          0.59334,
          1.1301,
          0.98345,
          1.3183,
          1.2562,
          1.6001,
          4.0259,
          0.9451200000000001,
          0.93855,
          0.8302200000000001,
          0.68219,
          0.59531,
          0.62513,
          0.8318399999999999,
          2.614,
          0.78432,
          1.3151,
          0.904,
          0.6209899999999999,
          1.6005,
          0.9451,
          0.71973,
          0.62211,
          0.7089,
          0.7205600000000001,
          0.8154600000000001,
          0.62513,
          0.6167199999999999,
          1.2429,
          1.258,
          0.77674,
          0.8712,
          0.71668,
          2.1967,
          1.7347,
          1.0994,
          1.0806,
          0.7855,
          0.75364,
          1.1299,
          0.59053,
          0.62589,
          0.77318,
          3.0519,
          0.90138,
          0.67788,
          0.8157399999999999,
          1.2997,
          2.9248,
          0.81686,
          1.1269,
          1.6017,
          2.394,
          0.86634,
          0.7747,
          1.2566,
          0.7857,
          2.9538,
          0.81892,
          0.78601,
          1.2475,
          0.6150899999999999,
          0.9476100000000001,
          0.68533,
          0.7197,
          0.71503,
          0.60028,
          0.62231,
          1.5263,
          0.97016,
          0.8627,
          0.68455,
          1.9573,
          0.61843,
          1.3212,
          0.5965,
          1.2607,
          1.1536,
          1.0603,
          1.31,
          1.5853,
          0.71017,
          0.93606,
          1.0946,
          1.2399,
          1.5067,
          1.5998,
          0.58919,
          1.2448,
          0.5012,
          0.60434,
          0.5963,
          0.61856,
          0.6136,
          4.0533,
          0.57587,
          0.77969,
          1.1216,
          0.82481,
          0.64904,
          0.59718,
          1.0898,
          3.0134,
          0.59442,
          0.72556,
          0.7858,
          1.1023,
          0.60541,
          0.6007600000000001,
          0.89243,
          0.95988,
          1.0829,
          0.7089300000000001,
          0.79105,
          1.1245,
          0.90507,
          0.99729,
          0.77822,
          0.71834,
          0.87225,
          2.2048,
          0.70881,
          0.77279,
          0.89731,
          0.75271,
          1.0796,
          0.68258,
          0.82155,
          1.2371,
          1.1146,
          1.2117,
          1.0774,
          0.78128,
          1.2324,
          0.75937,
          1.0452,
          1.603,
          1.1301,
          2.3861,
          1.0578,
          0.54188,
          1.1485,
          0.62248,
          0.68083,
          0.78364,
          0.95011,
          1.0774,
          0.81386,
          0.57213,
          0.7205600000000001,
          0.68555,
          1.0513,
          0.7593,
          0.78561,
          1.1354,
          0.68376,
          0.83091,
          3.5357,
          0.76764,
          0.8712700000000001,
          1.074,
          1.0347,
          0.75542,
          0.52267,
          0.5905,
          1.0789,
          0.9499,
          0.7723,
          0.8712700000000001,
          1.132,
          0.58855,
          1.3034,
          0.5967899999999999,
          3.0553,
          0.80937,
          0.8058,
          0.8345799999999999,
          1.0787,
          0.75942,
          1.4611,
          1.0765,
          0.58885,
          0.98703,
          0.9362299999999999,
          0.58368,
          1.0823,
          0.81886,
          0.58208,
          0.70508,
          4.0465,
          1.0207,
          1.2534,
          0.62138,
          1.3111,
          1.5966,
          0.9538399999999999,
          1.0546,
          0.7209800000000001,
          1.1337,
          3.6437,
          0.5967899999999999,
          0.9067799999999999,
          1.0598,
          1.5335,
          0.78432,
          0.67237,
          1.0469,
          0.59595,
          0.58905,
          1.6052,
          0.71917,
          1.2395,
          0.7168,
          1.1324,
          0.7842399999999999,
          0.60493,
          1.1008,
          0.77685,
          0.9057200000000001,
          0.87277,
          0.8921600000000001,
          0.59665,
          0.7762399999999999,
          0.62051,
          1.3105,
          0.62502,
          0.77496,
          0.81481,
          1.1364,
          1.1266,
          0.9538399999999999,
          1.0846,
          0.7648699999999999,
          0.6258699999999999,
          0.7858,
          0.62621,
          0.80479,
          1.6016,
          1.0732,
          0.77644,
          1.0649,
          0.6230899999999999,
          0.76017,
          0.60482,
          1.0433,
          0.535,
          0.78008,
          0.72027,
          1.0728,
          0.61741,
          0.82124,
          0.8327700000000001,
          4.0382
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=ANN<br>r=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "ANN",
         "marker": {
          "color": "#EF553B",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "ANN",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          0.12690355329949238,
          0.7391304347826085,
          0.22886779960273668,
          0.9847715736040608,
          0.49746192893401014,
          0.2182741116751269,
          0.4016773339218715,
          0.8695652173913043,
          0.06598984771573603,
          0.03089825645552858,
          0.45464577355992053,
          0.13242109909512248,
          0.42639593908629436,
          0.1500772456411388,
          0.004414036636504083,
          0.09710880600308981,
          0.21761200617965126,
          0.5685279187817259,
          0.033767380269256235,
          0.056278967115427056,
          0.7316265725005517,
          0.598984771573604,
          0.481129993378945,
          0.5473405429265062,
          0.21761200617965126,
          0.24762745530787905,
          0.9847715736040608,
          0.4619289340101523,
          0.5473405429265062,
          0.3575369675568307,
          0.07945265945707349,
          0.07879055396159788,
          0.47208121827411165,
          0.8474950342087839,
          0.4016773339218715,
          0.13705583756345177,
          0.7541381593467225,
          0.1986316486426837,
          0.3502538071065989,
          0.02626351798719929,
          0.12381372765393953,
          0.09644670050761421,
          0.11917898918561023,
          0.8783932906643125,
          0.11255793423085411,
          0.04502317369234164,
          0.7856985212977267,
          0.47208121827411165,
          0.15758110792319577,
          0.10505407194879716,
          0.1765614654601633,
          0.26484219819024496,
          0.7360406091370558,
          0.7241227102184948,
          0.01125579342308541,
          0.8730964467005076,
          0.1500772456411388,
          0.06378282939748399,
          0.020304568527918777,
          0.5890531891414698,
          0.4314720812182741,
          0.044140366365040824,
          0.08629441624365482,
          0.4089604943721033,
          0.2138600750386228,
          0.7166188479364378,
          0.36393732067976164,
          0.28139483557713524,
          0.18009269476936657,
          0.49746192893401014,
          0.12756565879496798,
          0.09379827852571175,
          0.10505407194879716,
          0.16883690134628115,
          0.02626351798719929,
          0.22070183182520414,
          0.06003089825645552,
          0.6797616420216287,
          0.4923857868020304,
          0.14632531450011033,
          0.2030456852791878,
          0.07128669167954092,
          0.4619289340101523,
          0.2639593908629441,
          0.18097550209666738,
          0.416243654822335,
          0.11675126903553298,
          0.964467005076142,
          0.2030456852791878,
          0.4414036636504083,
          0.16508497020525267,
          1,
          0.05076142131979695,
          0.7724564113882144,
          0.07945265945707349,
          0.015228426395939085,
          0.22511586846170822,
          0.9340101522842639,
          0.0375193114102847,
          0.10593687927609799,
          0.12690355329949238,
          0.14213197969543148,
          0.11630986537188258,
          0.0812182741116751,
          0.12381372765393953,
          0.8426395939086294,
          0.5440300154491282,
          0.2401235930258221,
          0.3096446700507614,
          0.04502317369234164,
          0.38578680203045684,
          0.026484219819024497,
          0.2030456852791878,
          0.11167512690355329,
          0.2780843080997572,
          0.06598984771573603,
          0.598984771573604,
          0.2182741116751269,
          0.08254248510262634,
          0.4652394614875303,
          0.16331935555065105,
          0.3310527477378062,
          0.17258883248730963,
          0.3264180092694769,
          0.09710880600308981,
          0.13242109909512248,
          0.1013021408077687,
          0.013242109909512249,
          0.964467005076142,
          0.09710880600308981,
          0.3959390862944162,
          0.750386228205694,
          0.0375193114102847,
          0.1388214522180534,
          0.6294416243654821,
          0.1765614654601633,
          0.4771573604060913,
          0.375193114102847,
          0.2081218274111675,
          0.19885235047450892,
          0.18009269476936657,
          0.7208121827411167,
          0.7391304347826085,
          0.09379827852571175,
          0.10659898477157359,
          0.851909070845288,
          0.02251158684617082,
          0.26925623482674904,
          0.5605826528360185,
          0.37144118296181855,
          0.13506952107702494,
          0.0812182741116751,
          0.048775104833370116,
          0.949238578680203,
          0.1878172588832487,
          0.3264180092694769,
          0.5440300154491282,
          0.6565879496799822,
          0.05076142131979695,
          0.3564334583977047,
          0.16331935555065105,
          0.42021628779518866,
          0.2182741116751269,
          0.22511586846170822,
          0.365482233502538,
          0.05076142131979695,
          0.45464577355992053,
          0.3096446700507614,
          0.322224674464798,
          0.9187817258883249,
          0.057382476274553076,
          0.6903553299492385,
          0.6115647759876407,
          0.6903553299492385,
          0.42374751710439196,
          0.42816155374089604,
          0.01765614654601633,
          0.375193114102847,
          0.42374751710439196,
          0.11167512690355329,
          0.12690355329949238,
          0.7208121827411167,
          0.4771573604060913,
          0.34142573383359076,
          0.38071065989847713,
          0.35268152725667623,
          0.026484219819024497,
          0.06091370558375634,
          0.46700507614213194,
          0.13197969543147206,
          0.6267932023835797,
          0.1015228426395939,
          0.1538291767821673,
          0.3076583535643346,
          0.6142131979695431,
          0.2513793864489075,
          0.7592143014787022,
          0.6003089825645552,
          0.06753476053851247,
          0.17214742882365922,
          0.15890531891414697,
          0.03553299492385787,
          0.28139483557713524,
          0.2030456852791878,
          0.2295299050982123,
          1,
          0.11630986537188258,
          0.2030456852791878,
          0.8386669609357756,
          0.182741116751269,
          0.9949238578680203,
          0.23835797837122047,
          0.5025380710659898,
          0.29640256014124916,
          0.6598984771573604,
          0.39770470094901783,
          0.1013021408077687,
          0.30390642242330607,
          0.22511586846170822,
          0.12756565879496798,
          0.06753476053851247,
          0.17214742882365922,
          0.09269476936658573,
          0.005076142131979694,
          0.1979695431472081,
          0.7512690355329948,
          0.766497461928934,
          0.17258883248730963,
          0.10659898477157359,
          0.6135510924740675,
          0.31516221584639154,
          0.3972632972853674,
          0.5532994923857867,
          0.1500772456411388,
          0.17214742882365922,
          0.41116751269035534,
          0.6142131979695431,
          0.851909070845288,
          0.31979695431472077,
          0.38843522401235925,
          0.8883248730964466,
          0.3248730964467004,
          0.11255793423085411,
          0.30390642242330607,
          0.375193114102847,
          0.17258883248730963,
          0.21319796954314718,
          0.015228426395939085,
          0.08629441624365482,
          0.06003089825645552,
          0.47649525491061573,
          0.8474950342087839,
          0.044140366365040824,
          0.7166188479364378,
          0.9695431472081217,
          0.7969543147208121,
          0.322224674464798,
          0.026484219819024497,
          0.38402118737585517,
          0.4619289340101523,
          0.4016773339218715,
          0.056278967115427056,
          0.06621054954756124,
          0.4414036636504083,
          0.35268152725667623,
          0.1388214522180534,
          0.12690355329949238,
          0.18384462591039502,
          0.1875965570514235,
          0.19134848819245198,
          0.7062458618406532,
          0.10593687927609799,
          0.4678878834694328,
          0.7724564113882144,
          0.16883690134628115,
          0.2791878172588832,
          0.38402118737585517,
          0.44162436548223344,
          0.27411167512690354,
          0.45685279187817257,
          0.005076142131979694,
          0.6142131979695431,
          0.06003089825645552,
          0.11476495254910615,
          0.19421761200617962,
          0.45685279187817257,
          0.11255793423085411,
          0.40101522842639586,
          0.25888324873096447,
          0.8578680203045684,
          0.25888324873096447,
          0.13683513573162656,
          0.19510041933348046,
          0.34142573383359076,
          0.1613330390642242,
          0.0913705583756345,
          0.06621054954756124,
          0.0375193114102847,
          0.3564334583977047,
          0.18009269476936657,
          0.02626351798719929,
          0.416243654822335,
          0.15758110792319577,
          0.18009269476936657,
          0.0812182741116751,
          0.7512690355329948,
          0.416243654822335,
          0.24873096446700507,
          0.056278967115427056,
          0.0812182741116751,
          0.4414036636504083,
          0.2401235930258221,
          0.6665195321121165,
          0.21628779518870006,
          0.8426395939086294,
          0.21187375855219598,
          0.07062458618406532,
          0.964467005076142,
          0.41491944383138374,
          0.7327300816596777,
          0.11630986537188258,
          0.11167512690355329,
          0.020304568527918777,
          0.13242109909512248,
          0.36393732067976164,
          0.7278746413595233,
          0.07614213197969542,
          0.004414036636504083,
          0.29640256014124916,
          0.4016773339218715,
          0.7203707790774663,
          0.42816155374089604,
          0.22842639593908629,
          0.14213197969543148,
          0.0812182741116751,
          0.4771573604060913,
          0.2560141249172368,
          0.4193334804678878,
          0,
          0.45398366806444485,
          0.24365482233502536,
          0.8254248510262634,
          0.36768925182079004,
          0.17258883248730963,
          0.34142573383359076,
          0.4652394614875303,
          0.8386669609357756,
          0.1613330390642242,
          0.07062458618406532,
          0.09379827852571175,
          0.09379827852571175,
          0.3864489075259324,
          0.07879055396159788,
          0.1388214522180534,
          0.37894504524387546,
          0.36018538953873314,
          0.6446700507614214,
          0.182741116751269,
          0.640035312293092,
          0.3707790774663429,
          0.39770470094901783,
          0.851909070845288,
          0.03553299492385787,
          0.3350253807106599,
          0.22842639593908629,
          0.1500772456411388,
          0.12359302582211432,
          0.6532774222026042,
          0.6790995365261531,
          0.46700507614213194,
          0.7391304347826085,
          0.06003089825645552,
          0.21628779518870006,
          0.05076142131979695,
          0.33016994041050535,
          0.3489295961156477,
          0.057382476274553076,
          0.22070183182520414,
          0.4923857868020304,
          0.4990068417567865,
          0.21187375855219598,
          0.24277201500772455,
          0.17258883248730963,
          0.45398366806444485,
          0.4771573604060913,
          0.11255793423085411,
          0.8274111675126903,
          0.0750386228205694,
          0.21628779518870006,
          0.7278746413595233,
          0.12690355329949238,
          0.48775104833370114,
          0.09710880600308981,
          0.18384462591039502,
          0.3864489075259324,
          0.35532994923857864,
          0.12690355329949238,
          0.052968439638048995,
          0.24873096446700507,
          0.5102626351798719,
          0.5208563231074818,
          0.9746192893401014,
          0.03045685279187817,
          0.033767380269256235,
          0.7353785036415801,
          0.3489295961156477,
          0.5532994923857867,
          0.3376738026925623,
          0.00375193114102847,
          0.31781063782829394,
          0.12756565879496798,
          0.3502538071065989,
          0.48775104833370114,
          0.033767380269256235,
          0.8274111675126903,
          0.23835797837122047,
          0.21628779518870006,
          0.08254248510262634,
          0.8254248510262634,
          0.03045685279187817,
          0.07128669167954092,
          0.7360406091370558,
          0.38578680203045684,
          0.06003089825645552,
          0.41116751269035534,
          0.07879055396159788,
          0.44162436548223344,
          0.7203707790774663,
          0.8563231074817921,
          0.09644670050761421,
          0.1986316486426837,
          0.38843522401235925,
          0.21187375855219598,
          0.2081218274111675,
          0.4325755903774001,
          0.8730964467005076,
          0.3487088942838225,
          0.5076142131979695,
          0.34142573383359076,
          0.7969543147208121,
          0.3089825645552858,
          0.21319796954314718,
          0.5552858088722136,
          0.34142573383359076,
          0.2780843080997572,
          0.5552858088722136,
          0.18980357536967554,
          0.781725888324873,
          0.7724564113882144,
          0.9035532994923856,
          0.17258883248730963,
          0.048775104833370116,
          0.375193114102847,
          0.21187375855219598,
          0.18384462591039502,
          0.6446700507614214,
          0.11035091591260207,
          0.09644670050761421,
          0.06003089825645552,
          0.45464577355992053,
          0.5076142131979695,
          0.3502538071065989,
          0.36393732067976164,
          0.35532994923857864,
          0.4414036636504083,
          0.2030456852791878,
          0.033767380269256235
         ],
         "xaxis": "x",
         "y": [
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8554558753967285,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          2.2721903324127197,
          2.500852584838867,
          0.8553754091262817,
          0.8553754091262817,
          2.912498950958252,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          1.7137593030929565,
          0.8654075264930725,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          1.998752474784851,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553962111473083,
          1.665574312210083,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          1.8096202611923218,
          0.8553754091262817,
          0.8715038299560547,
          0.8553963303565979,
          0.8553754091262817,
          2.131816864013672,
          0.8553754091262817,
          0.8650209307670593,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8653419613838196,
          0.8553754091262817,
          0.8553754091262817,
          2.4828805923461914,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          2.5383050441741943,
          1.8262050151824951,
          0.8554547429084778,
          0.8553754091262817,
          1.9734184741973877,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          2.6598119735717773,
          0.8553754091262817,
          0.8554551601409912,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8649894595146179,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          2.513949394226074,
          3.1611528396606445,
          2.9752774238586426,
          0.8553754091262817,
          2.7636935710906982,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553963303565979,
          0.8553754091262817,
          0.8553754091262817,
          0.8650104403495789,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          2.9170985221862793,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          2.368889093399048,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553968071937561,
          0.8553754091262817,
          0.8553754091262817,
          1.8789384365081787,
          0.8554549813270569,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          1.9948424100875854,
          0.8553754091262817,
          1.911723017692566,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.865374743938446,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8554537892341614,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8554031252861023,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          2.1829721927642822,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          1.7617793083190918,
          0.8553754091262817,
          0.8553754091262817,
          0.8654404282569885,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8650843501091003,
          2.202298641204834,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8554542660713196,
          0.8553754091262817,
          2.0571630001068115,
          3.1328747272491455,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          2.9718809127807617,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          2.6301157474517822,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          2.046367883682251,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          2.709810733795166,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          2.51377272605896,
          0.8553963303565979,
          0.8553754091262817,
          3.1412620544433594,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          1.6172422170639038,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553963303565979,
          2.8999693393707275,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8554547429084778,
          1.9734184741973877,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          3.0954668521881104,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          2.9550132751464844,
          0.8553754091262817,
          0.8554561138153076,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8554542660713196,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          2.8796472549438477,
          0.8553754091262817,
          0.8553754091262817,
          0.8651801943778992,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          1.8262050151824951,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          1.6097148656845093,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          2.914799451828003,
          0.8553754091262817,
          0.8553754091262817,
          0.8553962111473083,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553963303565979,
          0.8553754091262817,
          0.8553754091262817,
          0.8554537892341614,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8649999499320984,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8553754091262817,
          0.8650209307670593,
          1.6097164154052734
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=RFC<br>r=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "RFC",
         "marker": {
          "color": "#00cc96",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "RFC",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          0.12690355329949238,
          0.7391304347826085,
          0.22886779960273668,
          0.9847715736040608,
          0.49746192893401014,
          0.2182741116751269,
          0.4016773339218715,
          0.8695652173913043,
          0.06598984771573603,
          0.03089825645552858,
          0.45464577355992053,
          0.13242109909512248,
          0.42639593908629436,
          0.1500772456411388,
          0.004414036636504083,
          0.09710880600308981,
          0.21761200617965126,
          0.5685279187817259,
          0.033767380269256235,
          0.056278967115427056,
          0.7316265725005517,
          0.598984771573604,
          0.481129993378945,
          0.5473405429265062,
          0.21761200617965126,
          0.24762745530787905,
          0.9847715736040608,
          0.4619289340101523,
          0.5473405429265062,
          0.3575369675568307,
          0.07945265945707349,
          0.07879055396159788,
          0.47208121827411165,
          0.8474950342087839,
          0.4016773339218715,
          0.13705583756345177,
          0.7541381593467225,
          0.1986316486426837,
          0.3502538071065989,
          0.02626351798719929,
          0.12381372765393953,
          0.09644670050761421,
          0.11917898918561023,
          0.8783932906643125,
          0.11255793423085411,
          0.04502317369234164,
          0.7856985212977267,
          0.47208121827411165,
          0.15758110792319577,
          0.10505407194879716,
          0.1765614654601633,
          0.26484219819024496,
          0.7360406091370558,
          0.7241227102184948,
          0.01125579342308541,
          0.8730964467005076,
          0.1500772456411388,
          0.06378282939748399,
          0.020304568527918777,
          0.5890531891414698,
          0.4314720812182741,
          0.044140366365040824,
          0.08629441624365482,
          0.4089604943721033,
          0.2138600750386228,
          0.7166188479364378,
          0.36393732067976164,
          0.28139483557713524,
          0.18009269476936657,
          0.49746192893401014,
          0.12756565879496798,
          0.09379827852571175,
          0.10505407194879716,
          0.16883690134628115,
          0.02626351798719929,
          0.22070183182520414,
          0.06003089825645552,
          0.6797616420216287,
          0.4923857868020304,
          0.14632531450011033,
          0.2030456852791878,
          0.07128669167954092,
          0.4619289340101523,
          0.2639593908629441,
          0.18097550209666738,
          0.416243654822335,
          0.11675126903553298,
          0.964467005076142,
          0.2030456852791878,
          0.4414036636504083,
          0.16508497020525267,
          1,
          0.05076142131979695,
          0.7724564113882144,
          0.07945265945707349,
          0.015228426395939085,
          0.22511586846170822,
          0.9340101522842639,
          0.0375193114102847,
          0.10593687927609799,
          0.12690355329949238,
          0.14213197969543148,
          0.11630986537188258,
          0.0812182741116751,
          0.12381372765393953,
          0.8426395939086294,
          0.5440300154491282,
          0.2401235930258221,
          0.3096446700507614,
          0.04502317369234164,
          0.38578680203045684,
          0.026484219819024497,
          0.2030456852791878,
          0.11167512690355329,
          0.2780843080997572,
          0.06598984771573603,
          0.598984771573604,
          0.2182741116751269,
          0.08254248510262634,
          0.4652394614875303,
          0.16331935555065105,
          0.3310527477378062,
          0.17258883248730963,
          0.3264180092694769,
          0.09710880600308981,
          0.13242109909512248,
          0.1013021408077687,
          0.013242109909512249,
          0.964467005076142,
          0.09710880600308981,
          0.3959390862944162,
          0.750386228205694,
          0.0375193114102847,
          0.1388214522180534,
          0.6294416243654821,
          0.1765614654601633,
          0.4771573604060913,
          0.375193114102847,
          0.2081218274111675,
          0.19885235047450892,
          0.18009269476936657,
          0.7208121827411167,
          0.7391304347826085,
          0.09379827852571175,
          0.10659898477157359,
          0.851909070845288,
          0.02251158684617082,
          0.26925623482674904,
          0.5605826528360185,
          0.37144118296181855,
          0.13506952107702494,
          0.0812182741116751,
          0.048775104833370116,
          0.949238578680203,
          0.1878172588832487,
          0.3264180092694769,
          0.5440300154491282,
          0.6565879496799822,
          0.05076142131979695,
          0.3564334583977047,
          0.16331935555065105,
          0.42021628779518866,
          0.2182741116751269,
          0.22511586846170822,
          0.365482233502538,
          0.05076142131979695,
          0.45464577355992053,
          0.3096446700507614,
          0.322224674464798,
          0.9187817258883249,
          0.057382476274553076,
          0.6903553299492385,
          0.6115647759876407,
          0.6903553299492385,
          0.42374751710439196,
          0.42816155374089604,
          0.01765614654601633,
          0.375193114102847,
          0.42374751710439196,
          0.11167512690355329,
          0.12690355329949238,
          0.7208121827411167,
          0.4771573604060913,
          0.34142573383359076,
          0.38071065989847713,
          0.35268152725667623,
          0.026484219819024497,
          0.06091370558375634,
          0.46700507614213194,
          0.13197969543147206,
          0.6267932023835797,
          0.1015228426395939,
          0.1538291767821673,
          0.3076583535643346,
          0.6142131979695431,
          0.2513793864489075,
          0.7592143014787022,
          0.6003089825645552,
          0.06753476053851247,
          0.17214742882365922,
          0.15890531891414697,
          0.03553299492385787,
          0.28139483557713524,
          0.2030456852791878,
          0.2295299050982123,
          1,
          0.11630986537188258,
          0.2030456852791878,
          0.8386669609357756,
          0.182741116751269,
          0.9949238578680203,
          0.23835797837122047,
          0.5025380710659898,
          0.29640256014124916,
          0.6598984771573604,
          0.39770470094901783,
          0.1013021408077687,
          0.30390642242330607,
          0.22511586846170822,
          0.12756565879496798,
          0.06753476053851247,
          0.17214742882365922,
          0.09269476936658573,
          0.005076142131979694,
          0.1979695431472081,
          0.7512690355329948,
          0.766497461928934,
          0.17258883248730963,
          0.10659898477157359,
          0.6135510924740675,
          0.31516221584639154,
          0.3972632972853674,
          0.5532994923857867,
          0.1500772456411388,
          0.17214742882365922,
          0.41116751269035534,
          0.6142131979695431,
          0.851909070845288,
          0.31979695431472077,
          0.38843522401235925,
          0.8883248730964466,
          0.3248730964467004,
          0.11255793423085411,
          0.30390642242330607,
          0.375193114102847,
          0.17258883248730963,
          0.21319796954314718,
          0.015228426395939085,
          0.08629441624365482,
          0.06003089825645552,
          0.47649525491061573,
          0.8474950342087839,
          0.044140366365040824,
          0.7166188479364378,
          0.9695431472081217,
          0.7969543147208121,
          0.322224674464798,
          0.026484219819024497,
          0.38402118737585517,
          0.4619289340101523,
          0.4016773339218715,
          0.056278967115427056,
          0.06621054954756124,
          0.4414036636504083,
          0.35268152725667623,
          0.1388214522180534,
          0.12690355329949238,
          0.18384462591039502,
          0.1875965570514235,
          0.19134848819245198,
          0.7062458618406532,
          0.10593687927609799,
          0.4678878834694328,
          0.7724564113882144,
          0.16883690134628115,
          0.2791878172588832,
          0.38402118737585517,
          0.44162436548223344,
          0.27411167512690354,
          0.45685279187817257,
          0.005076142131979694,
          0.6142131979695431,
          0.06003089825645552,
          0.11476495254910615,
          0.19421761200617962,
          0.45685279187817257,
          0.11255793423085411,
          0.40101522842639586,
          0.25888324873096447,
          0.8578680203045684,
          0.25888324873096447,
          0.13683513573162656,
          0.19510041933348046,
          0.34142573383359076,
          0.1613330390642242,
          0.0913705583756345,
          0.06621054954756124,
          0.0375193114102847,
          0.3564334583977047,
          0.18009269476936657,
          0.02626351798719929,
          0.416243654822335,
          0.15758110792319577,
          0.18009269476936657,
          0.0812182741116751,
          0.7512690355329948,
          0.416243654822335,
          0.24873096446700507,
          0.056278967115427056,
          0.0812182741116751,
          0.4414036636504083,
          0.2401235930258221,
          0.6665195321121165,
          0.21628779518870006,
          0.8426395939086294,
          0.21187375855219598,
          0.07062458618406532,
          0.964467005076142,
          0.41491944383138374,
          0.7327300816596777,
          0.11630986537188258,
          0.11167512690355329,
          0.020304568527918777,
          0.13242109909512248,
          0.36393732067976164,
          0.7278746413595233,
          0.07614213197969542,
          0.004414036636504083,
          0.29640256014124916,
          0.4016773339218715,
          0.7203707790774663,
          0.42816155374089604,
          0.22842639593908629,
          0.14213197969543148,
          0.0812182741116751,
          0.4771573604060913,
          0.2560141249172368,
          0.4193334804678878,
          0,
          0.45398366806444485,
          0.24365482233502536,
          0.8254248510262634,
          0.36768925182079004,
          0.17258883248730963,
          0.34142573383359076,
          0.4652394614875303,
          0.8386669609357756,
          0.1613330390642242,
          0.07062458618406532,
          0.09379827852571175,
          0.09379827852571175,
          0.3864489075259324,
          0.07879055396159788,
          0.1388214522180534,
          0.37894504524387546,
          0.36018538953873314,
          0.6446700507614214,
          0.182741116751269,
          0.640035312293092,
          0.3707790774663429,
          0.39770470094901783,
          0.851909070845288,
          0.03553299492385787,
          0.3350253807106599,
          0.22842639593908629,
          0.1500772456411388,
          0.12359302582211432,
          0.6532774222026042,
          0.6790995365261531,
          0.46700507614213194,
          0.7391304347826085,
          0.06003089825645552,
          0.21628779518870006,
          0.05076142131979695,
          0.33016994041050535,
          0.3489295961156477,
          0.057382476274553076,
          0.22070183182520414,
          0.4923857868020304,
          0.4990068417567865,
          0.21187375855219598,
          0.24277201500772455,
          0.17258883248730963,
          0.45398366806444485,
          0.4771573604060913,
          0.11255793423085411,
          0.8274111675126903,
          0.0750386228205694,
          0.21628779518870006,
          0.7278746413595233,
          0.12690355329949238,
          0.48775104833370114,
          0.09710880600308981,
          0.18384462591039502,
          0.3864489075259324,
          0.35532994923857864,
          0.12690355329949238,
          0.052968439638048995,
          0.24873096446700507,
          0.5102626351798719,
          0.5208563231074818,
          0.9746192893401014,
          0.03045685279187817,
          0.033767380269256235,
          0.7353785036415801,
          0.3489295961156477,
          0.5532994923857867,
          0.3376738026925623,
          0.00375193114102847,
          0.31781063782829394,
          0.12756565879496798,
          0.3502538071065989,
          0.48775104833370114,
          0.033767380269256235,
          0.8274111675126903,
          0.23835797837122047,
          0.21628779518870006,
          0.08254248510262634,
          0.8254248510262634,
          0.03045685279187817,
          0.07128669167954092,
          0.7360406091370558,
          0.38578680203045684,
          0.06003089825645552,
          0.41116751269035534,
          0.07879055396159788,
          0.44162436548223344,
          0.7203707790774663,
          0.8563231074817921,
          0.09644670050761421,
          0.1986316486426837,
          0.38843522401235925,
          0.21187375855219598,
          0.2081218274111675,
          0.4325755903774001,
          0.8730964467005076,
          0.3487088942838225,
          0.5076142131979695,
          0.34142573383359076,
          0.7969543147208121,
          0.3089825645552858,
          0.21319796954314718,
          0.5552858088722136,
          0.34142573383359076,
          0.2780843080997572,
          0.5552858088722136,
          0.18980357536967554,
          0.781725888324873,
          0.7724564113882144,
          0.9035532994923856,
          0.17258883248730963,
          0.048775104833370116,
          0.375193114102847,
          0.21187375855219598,
          0.18384462591039502,
          0.6446700507614214,
          0.11035091591260207,
          0.09644670050761421,
          0.06003089825645552,
          0.45464577355992053,
          0.5076142131979695,
          0.3502538071065989,
          0.36393732067976164,
          0.35532994923857864,
          0.4414036636504083,
          0.2030456852791878,
          0.033767380269256235
         ],
         "xaxis": "x",
         "y": [
          0.6058129266666671,
          0.9265517904761912,
          1.0654115833333335,
          0.6145601016666659,
          0.6688154000000011,
          0.6118688999999999,
          0.9450864866666677,
          0.6731523333333332,
          0.8722395500000001,
          0.7925087400000004,
          0.817865999999999,
          0.9401625416666675,
          0.7182502999999995,
          1.1093323333333354,
          0.7535927383333327,
          1.156805733333332,
          1.1196458428571434,
          0.6214094833333339,
          1.2945815833333334,
          1.2336880000000003,
          1.0583391761904755,
          0.6218037749999988,
          0.8187005249999992,
          0.7804345,
          1.0646625333333324,
          1.2577784000000007,
          0.5258708999999996,
          0.7149085283333336,
          0.8203419266666664,
          0.9064743383333336,
          0.9382633999999986,
          1.3015287000000009,
          0.7096895533333333,
          0.7770466099999996,
          0.7766859899999994,
          0.7108158616666675,
          0.8417353000000005,
          0.9492596499999992,
          0.6865952533333334,
          1.0933358333333318,
          1.2401346666666646,
          2.5079229666666696,
          0.7821662466666668,
          0.7198264250000002,
          1.3035249666666673,
          3.645047750000003,
          0.7862216916666663,
          0.7096895533333333,
          1.5156119666666676,
          1.5313479999999984,
          0.7641212000000003,
          0.9531389666666672,
          0.6246905499999991,
          1.1129959333333341,
          1.293056449999999,
          0.6261895500000002,
          1.053290050000001,
          4.031295000000003,
          0.8305352000000005,
          1.1370883166666674,
          0.6187788999999991,
          1.153605666666667,
          2.7331356000000038,
          1.075121666666668,
          1.2578680666666655,
          1.0593756333333324,
          1.1278740500000006,
          1.320838541666668,
          1.2554359999999987,
          0.5499560000000002,
          1.5228249333333332,
          1.6027640166666657,
          1.102753666666667,
          1.4708350000000003,
          1.0397887904761915,
          0.9391063333333327,
          4.043130250000004,
          0.7850656300000004,
          0.620081724999999,
          1.2442524333333322,
          0.6095626999999992,
          3.8346528833333315,
          0.7149085283333336,
          0.6937817000000002,
          1.0904434999999995,
          0.5890298966666669,
          2.3763828833333314,
          0.6234114500000006,
          0.8324357333333342,
          0.7780197350000002,
          1.503885033333334,
          0.578369703333334,
          0.8309422166666668,
          0.8239928249999996,
          0.8917519333333337,
          2.61418113333333,
          1.2578597999999996,
          0.5965164000000007,
          1.527686666666666,
          1.6451752166666649,
          1.121331126666667,
          0.872401686666667,
          1.1039701999999991,
          2.2038123142857136,
          1.305305400000002,
          0.626070530000001,
          1.0820594999999995,
          1.1201579999999995,
          0.6160382857142871,
          1.235279966666669,
          0.5885207533333338,
          2.917715333333333,
          0.7698409000000009,
          0.8734327666666658,
          0.9073294399999995,
          0.5745014666666655,
          0.5918485999999998,
          0.8304386333333335,
          1.1010576190476176,
          1.1325334166666647,
          0.7617531466666679,
          0.9527109916666664,
          0.5778799749999998,
          1.2531129976190492,
          0.7980205266666672,
          1.3883646333333328,
          2.1614881499999994,
          3.052083083333332,
          0.5856000000000003,
          1.9568187333333311,
          0.7198140799999999,
          0.5702841535714281,
          1.527686666666666,
          1.1065672000000009,
          0.6225667349999995,
          1.097766500000001,
          0.6231608400000005,
          0.8154731500000012,
          0.8312960000000007,
          1.0207272000000005,
          1.3173693999999991,
          0.5950757699999992,
          0.9265517904761912,
          1.5305289999999991,
          0.676157166666667,
          0.8127658602380952,
          3.644430583333328,
          0.7727551750000009,
          0.781621916666667,
          0.7760782883333328,
          1.307132966666668,
          2.60729316666667,
          1.0453433333333348,
          0.5959304000000006,
          0.6083842333333324,
          1.3135709999999994,
          1.0820594999999995,
          1.0850400333333317,
          0.5742480733333326,
          0.8886894299999989,
          0.7617531466666679,
          1.075929266666668,
          0.5811629666666672,
          0.9515847183333341,
          0.7208824900000002,
          0.6017410750000005,
          0.7798373409523804,
          0.6160382857142871,
          0.8143527666666672,
          0.5962787933333331,
          1.1543090833333323,
          1.1377691333333335,
          1.1376647333333347,
          0.6241215333333335,
          0.832369333333333,
          0.8164441666666675,
          1.0981839999999996,
          1.0499554333333336,
          0.6264867500000003,
          1.8145049666666662,
          0.8730989916666667,
          0.6245264083333324,
          0.5895508899999994,
          1.1265074000000006,
          0.6869511533333323,
          1.1257646049999988,
          0.8895456900000004,
          0.671392843333334,
          0.6674191971428566,
          1.9387523333333327,
          0.7829601999999996,
          2.1091922333333315,
          1.6000406571428583,
          1.0708925333333346,
          0.5921883316666673,
          1.0672495666666677,
          0.8247220399999996,
          0.7825152999999989,
          1.2343078000000003,
          0.9009021133333321,
          0.942453146666667,
          0.8311250666666676,
          1.2589702666666653,
          0.5811943250000003,
          0.7716641999999996,
          0.578369703333334,
          1.0516167999999992,
          0.8729272500000007,
          0.7779711349999994,
          0.7138054499999998,
          0.578369703333334,
          0.7051882999999995,
          0.6123921999999999,
          1.070854999999999,
          0.5943239399999993,
          1.1295136666666654,
          1.0835076583333336,
          1.3184001249999986,
          1.2578597999999996,
          1.6006233999999997,
          3.976615999999999,
          0.9456816825000003,
          0.9393688333333337,
          0.8295013999999997,
          0.6782659933333339,
          0.5960380833333333,
          0.6256554333333326,
          0.8327010166666668,
          1.9366663333333343,
          0.7827008999999994,
          1.316333999999998,
          0.8951747499999992,
          0.6211833600000005,
          1.6010776571428567,
          0.9456816825000003,
          0.7189386999999995,
          0.6220351666666653,
          0.7383405666666669,
          0.7212110016666664,
          0.8157941750000016,
          0.6259462499999995,
          0.6166686190476198,
          1.2391794166666643,
          1.2561335333333323,
          0.7758760749999999,
          0.8717873566666665,
          0.7157228750000002,
          2.2077505833333353,
          3.2908274,
          1.099053249999998,
          1.0787679999999988,
          0.7770466099999996,
          0.7554341416666673,
          1.132782250000001,
          0.5835703000000002,
          0.6254319149999995,
          0.7748686499999997,
          3.0537295833333333,
          0.9043103333333338,
          0.6765485999999992,
          0.8158706366666683,
          1.2991140333333342,
          2.924791633333337,
          0.8174115000000003,
          1.1270031000000014,
          1.6006489999999989,
          1.845101213333333,
          0.9434375666666678,
          0.7168776500000001,
          1.2542634999999982,
          0.7858785142857146,
          1.4615720533333325,
          0.8180671999999996,
          0.7859281804761906,
          1.254387499999999,
          0.6152565500000001,
          0.948152983333334,
          0.6833901500000001,
          0.7202139699999995,
          0.7156926750000007,
          0.6000972499999992,
          0.6220351666666653,
          1.5312450333333334,
          0.9906676383333323,
          1.0096796999999993,
          0.6781110999999999,
          1.6252184833333325,
          0.6184661416666661,
          1.3213884666666673,
          0.597178869999999,
          1.2583162500000014,
          1.153775499999999,
          1.0644523333333347,
          1.3090627500000007,
          1.5881335000000023,
          0.710074978095238,
          0.9372569999999986,
          1.0953357499999985,
          1.1925244000000013,
          1.1157638016666673,
          1.598430483333335,
          0.5890298966666669,
          1.2519545000000007,
          1.1157638016666673,
          0.6041499599999995,
          0.5960380833333333,
          0.6187317999999994,
          0.6136930100000002,
          4.045095116666671,
          0.5744861666666657,
          0.7780197350000002,
          1.1201579999999995,
          0.8231963666666665,
          0.6898173333333346,
          0.5965081999999993,
          1.0343014166666653,
          3.0276020166666693,
          0.5856000000000003,
          0.8719689083333338,
          0.7866266654761902,
          1.1039701999999991,
          0.6057926133333331,
          0.6002558249999993,
          0.8949748749999987,
          0.9970522535714277,
          0.9985699133333334,
          0.7086632880952378,
          0.7912421300000001,
          1.124709166666666,
          0.8892886,
          1.059529173333332,
          0.7775024100000003,
          0.7179532000000002,
          0.872401686666667,
          2.2038123142857136,
          0.7005480683333335,
          0.7713992666666668,
          0.9077034716666668,
          0.7535927383333327,
          1.0771035000000013,
          0.6846994583333331,
          0.8230394583333335,
          0.9912268916666668,
          1.1146912333333334,
          1.2427637142857135,
          1.0787955,
          0.7779711349999994,
          1.5204346333333345,
          0.7564814599999999,
          1.0488383499999998,
          1.6027640166666657,
          1.1291659333333335,
          3.4539733238095254,
          1.0502892500000014,
          0.6731931295238092,
          1.0982229999999984,
          0.6229142100000004,
          0.6776963999999999,
          0.7853025999999997,
          0.9497377116666657,
          1.0743400000000012,
          0.8127658602380952,
          0.573210731666667,
          0.7212306600000002,
          0.6810213166666668,
          1.053290050000001,
          0.7599390066666661,
          0.7849445249999997,
          1.138229999999999,
          0.6674191971428566,
          0.9127755700000015,
          3.6085940000000067,
          0.7708093000000001,
          0.8705002249999994,
          1.071592416666666,
          1.208501566666665,
          0.7564992416666669,
          0.6060355000000016,
          0.6108144366666657,
          1.0812480000000015,
          0.9510090166666672,
          0.7714011999999993,
          0.8717873566666665,
          1.132162199999998,
          0.6231608400000005,
          1.3035249666666673,
          0.5962536749999998,
          2.949945483333334,
          0.8086613516666659,
          0.9985699133333334,
          0.8339493999999994,
          1.0803600000000009,
          0.7580594250000006,
          1.2654167499999998,
          1.074287,
          0.5880689499999994,
          1.121331126666667,
          0.9344977,
          0.5851931333333344,
          1.0801880000000017,
          0.8196514400000011,
          0.5803592999999997,
          0.7051280733333333,
          4.043722583333337,
          0.9556412428571442,
          1.208501566666665,
          0.6211833600000005,
          1.3099432500000008,
          1.596050499999999,
          0.9527617499999992,
          1.0500253333333343,
          0.7211677500000004,
          1.1330052833333328,
          3.6474082499999962,
          0.5962536749999998,
          0.9049432199999988,
          1.020426883333332,
          1.5314410833333343,
          0.7853557000000003,
          0.6700823933333341,
          1.0451925166666651,
          0.5950438666666659,
          0.5885207533333338,
          1.602909999999998,
          0.7189386999999995,
          1.2357432999999989,
          0.7175073999999997,
          1.1273098333333325,
          0.647220656666668,
          0.6050415999999994,
          0.8101564166666665,
          0.776451164999999,
          0.9038417000000006,
          0.8731829166666658,
          0.8430268999999995,
          0.5971802466666665,
          0.7753302899999999,
          0.6202150209523808,
          1.3090627500000007,
          0.6254319149999995,
          0.7739144999999997,
          0.8305707000000006,
          1.138019199999998,
          1.1265074000000006,
          0.9530506999999997,
          1.0829854999999993,
          0.7669323000000006,
          0.6253817166666661,
          0.7859281804761906,
          0.6251465799999997,
          1.3907183816666662,
          1.602053233333331,
          1.0751815000000002,
          0.7746462000000008,
          1.060822999999999,
          0.6229142100000004,
          0.7585618999999996,
          0.6050415999999994,
          1.0462959999999994,
          0.6824032999999998,
          0.7805580966666661,
          0.7211677500000004,
          1.0748578333333323,
          0.6171853933333324,
          0.7787504999999991,
          0.8324357333333342,
          4.043722583333337
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=GBC<br>r=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "GBC",
         "marker": {
          "color": "#ab63fa",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "GBC",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          0.12690355329949238,
          0.7391304347826085,
          0.22886779960273668,
          0.9847715736040608,
          0.49746192893401014,
          0.2182741116751269,
          0.4016773339218715,
          0.8695652173913043,
          0.06598984771573603,
          0.03089825645552858,
          0.45464577355992053,
          0.13242109909512248,
          0.42639593908629436,
          0.1500772456411388,
          0.004414036636504083,
          0.09710880600308981,
          0.21761200617965126,
          0.5685279187817259,
          0.033767380269256235,
          0.056278967115427056,
          0.7316265725005517,
          0.598984771573604,
          0.481129993378945,
          0.5473405429265062,
          0.21761200617965126,
          0.24762745530787905,
          0.9847715736040608,
          0.4619289340101523,
          0.5473405429265062,
          0.3575369675568307,
          0.07945265945707349,
          0.07879055396159788,
          0.47208121827411165,
          0.8474950342087839,
          0.4016773339218715,
          0.13705583756345177,
          0.7541381593467225,
          0.1986316486426837,
          0.3502538071065989,
          0.02626351798719929,
          0.12381372765393953,
          0.09644670050761421,
          0.11917898918561023,
          0.8783932906643125,
          0.11255793423085411,
          0.04502317369234164,
          0.7856985212977267,
          0.47208121827411165,
          0.15758110792319577,
          0.10505407194879716,
          0.1765614654601633,
          0.26484219819024496,
          0.7360406091370558,
          0.7241227102184948,
          0.01125579342308541,
          0.8730964467005076,
          0.1500772456411388,
          0.06378282939748399,
          0.020304568527918777,
          0.5890531891414698,
          0.4314720812182741,
          0.044140366365040824,
          0.08629441624365482,
          0.4089604943721033,
          0.2138600750386228,
          0.7166188479364378,
          0.36393732067976164,
          0.28139483557713524,
          0.18009269476936657,
          0.49746192893401014,
          0.12756565879496798,
          0.09379827852571175,
          0.10505407194879716,
          0.16883690134628115,
          0.02626351798719929,
          0.22070183182520414,
          0.06003089825645552,
          0.6797616420216287,
          0.4923857868020304,
          0.14632531450011033,
          0.2030456852791878,
          0.07128669167954092,
          0.4619289340101523,
          0.2639593908629441,
          0.18097550209666738,
          0.416243654822335,
          0.11675126903553298,
          0.964467005076142,
          0.2030456852791878,
          0.4414036636504083,
          0.16508497020525267,
          1,
          0.05076142131979695,
          0.7724564113882144,
          0.07945265945707349,
          0.015228426395939085,
          0.22511586846170822,
          0.9340101522842639,
          0.0375193114102847,
          0.10593687927609799,
          0.12690355329949238,
          0.14213197969543148,
          0.11630986537188258,
          0.0812182741116751,
          0.12381372765393953,
          0.8426395939086294,
          0.5440300154491282,
          0.2401235930258221,
          0.3096446700507614,
          0.04502317369234164,
          0.38578680203045684,
          0.026484219819024497,
          0.2030456852791878,
          0.11167512690355329,
          0.2780843080997572,
          0.06598984771573603,
          0.598984771573604,
          0.2182741116751269,
          0.08254248510262634,
          0.4652394614875303,
          0.16331935555065105,
          0.3310527477378062,
          0.17258883248730963,
          0.3264180092694769,
          0.09710880600308981,
          0.13242109909512248,
          0.1013021408077687,
          0.013242109909512249,
          0.964467005076142,
          0.09710880600308981,
          0.3959390862944162,
          0.750386228205694,
          0.0375193114102847,
          0.1388214522180534,
          0.6294416243654821,
          0.1765614654601633,
          0.4771573604060913,
          0.375193114102847,
          0.2081218274111675,
          0.19885235047450892,
          0.18009269476936657,
          0.7208121827411167,
          0.7391304347826085,
          0.09379827852571175,
          0.10659898477157359,
          0.851909070845288,
          0.02251158684617082,
          0.26925623482674904,
          0.5605826528360185,
          0.37144118296181855,
          0.13506952107702494,
          0.0812182741116751,
          0.048775104833370116,
          0.949238578680203,
          0.1878172588832487,
          0.3264180092694769,
          0.5440300154491282,
          0.6565879496799822,
          0.05076142131979695,
          0.3564334583977047,
          0.16331935555065105,
          0.42021628779518866,
          0.2182741116751269,
          0.22511586846170822,
          0.365482233502538,
          0.05076142131979695,
          0.45464577355992053,
          0.3096446700507614,
          0.322224674464798,
          0.9187817258883249,
          0.057382476274553076,
          0.6903553299492385,
          0.6115647759876407,
          0.6903553299492385,
          0.42374751710439196,
          0.42816155374089604,
          0.01765614654601633,
          0.375193114102847,
          0.42374751710439196,
          0.11167512690355329,
          0.12690355329949238,
          0.7208121827411167,
          0.4771573604060913,
          0.34142573383359076,
          0.38071065989847713,
          0.35268152725667623,
          0.026484219819024497,
          0.06091370558375634,
          0.46700507614213194,
          0.13197969543147206,
          0.6267932023835797,
          0.1015228426395939,
          0.1538291767821673,
          0.3076583535643346,
          0.6142131979695431,
          0.2513793864489075,
          0.7592143014787022,
          0.6003089825645552,
          0.06753476053851247,
          0.17214742882365922,
          0.15890531891414697,
          0.03553299492385787,
          0.28139483557713524,
          0.2030456852791878,
          0.2295299050982123,
          1,
          0.11630986537188258,
          0.2030456852791878,
          0.8386669609357756,
          0.182741116751269,
          0.9949238578680203,
          0.23835797837122047,
          0.5025380710659898,
          0.29640256014124916,
          0.6598984771573604,
          0.39770470094901783,
          0.1013021408077687,
          0.30390642242330607,
          0.22511586846170822,
          0.12756565879496798,
          0.06753476053851247,
          0.17214742882365922,
          0.09269476936658573,
          0.005076142131979694,
          0.1979695431472081,
          0.7512690355329948,
          0.766497461928934,
          0.17258883248730963,
          0.10659898477157359,
          0.6135510924740675,
          0.31516221584639154,
          0.3972632972853674,
          0.5532994923857867,
          0.1500772456411388,
          0.17214742882365922,
          0.41116751269035534,
          0.6142131979695431,
          0.851909070845288,
          0.31979695431472077,
          0.38843522401235925,
          0.8883248730964466,
          0.3248730964467004,
          0.11255793423085411,
          0.30390642242330607,
          0.375193114102847,
          0.17258883248730963,
          0.21319796954314718,
          0.015228426395939085,
          0.08629441624365482,
          0.06003089825645552,
          0.47649525491061573,
          0.8474950342087839,
          0.044140366365040824,
          0.7166188479364378,
          0.9695431472081217,
          0.7969543147208121,
          0.322224674464798,
          0.026484219819024497,
          0.38402118737585517,
          0.4619289340101523,
          0.4016773339218715,
          0.056278967115427056,
          0.06621054954756124,
          0.4414036636504083,
          0.35268152725667623,
          0.1388214522180534,
          0.12690355329949238,
          0.18384462591039502,
          0.1875965570514235,
          0.19134848819245198,
          0.7062458618406532,
          0.10593687927609799,
          0.4678878834694328,
          0.7724564113882144,
          0.16883690134628115,
          0.2791878172588832,
          0.38402118737585517,
          0.44162436548223344,
          0.27411167512690354,
          0.45685279187817257,
          0.005076142131979694,
          0.6142131979695431,
          0.06003089825645552,
          0.11476495254910615,
          0.19421761200617962,
          0.45685279187817257,
          0.11255793423085411,
          0.40101522842639586,
          0.25888324873096447,
          0.8578680203045684,
          0.25888324873096447,
          0.13683513573162656,
          0.19510041933348046,
          0.34142573383359076,
          0.1613330390642242,
          0.0913705583756345,
          0.06621054954756124,
          0.0375193114102847,
          0.3564334583977047,
          0.18009269476936657,
          0.02626351798719929,
          0.416243654822335,
          0.15758110792319577,
          0.18009269476936657,
          0.0812182741116751,
          0.7512690355329948,
          0.416243654822335,
          0.24873096446700507,
          0.056278967115427056,
          0.0812182741116751,
          0.4414036636504083,
          0.2401235930258221,
          0.6665195321121165,
          0.21628779518870006,
          0.8426395939086294,
          0.21187375855219598,
          0.07062458618406532,
          0.964467005076142,
          0.41491944383138374,
          0.7327300816596777,
          0.11630986537188258,
          0.11167512690355329,
          0.020304568527918777,
          0.13242109909512248,
          0.36393732067976164,
          0.7278746413595233,
          0.07614213197969542,
          0.004414036636504083,
          0.29640256014124916,
          0.4016773339218715,
          0.7203707790774663,
          0.42816155374089604,
          0.22842639593908629,
          0.14213197969543148,
          0.0812182741116751,
          0.4771573604060913,
          0.2560141249172368,
          0.4193334804678878,
          0,
          0.45398366806444485,
          0.24365482233502536,
          0.8254248510262634,
          0.36768925182079004,
          0.17258883248730963,
          0.34142573383359076,
          0.4652394614875303,
          0.8386669609357756,
          0.1613330390642242,
          0.07062458618406532,
          0.09379827852571175,
          0.09379827852571175,
          0.3864489075259324,
          0.07879055396159788,
          0.1388214522180534,
          0.37894504524387546,
          0.36018538953873314,
          0.6446700507614214,
          0.182741116751269,
          0.640035312293092,
          0.3707790774663429,
          0.39770470094901783,
          0.851909070845288,
          0.03553299492385787,
          0.3350253807106599,
          0.22842639593908629,
          0.1500772456411388,
          0.12359302582211432,
          0.6532774222026042,
          0.6790995365261531,
          0.46700507614213194,
          0.7391304347826085,
          0.06003089825645552,
          0.21628779518870006,
          0.05076142131979695,
          0.33016994041050535,
          0.3489295961156477,
          0.057382476274553076,
          0.22070183182520414,
          0.4923857868020304,
          0.4990068417567865,
          0.21187375855219598,
          0.24277201500772455,
          0.17258883248730963,
          0.45398366806444485,
          0.4771573604060913,
          0.11255793423085411,
          0.8274111675126903,
          0.0750386228205694,
          0.21628779518870006,
          0.7278746413595233,
          0.12690355329949238,
          0.48775104833370114,
          0.09710880600308981,
          0.18384462591039502,
          0.3864489075259324,
          0.35532994923857864,
          0.12690355329949238,
          0.052968439638048995,
          0.24873096446700507,
          0.5102626351798719,
          0.5208563231074818,
          0.9746192893401014,
          0.03045685279187817,
          0.033767380269256235,
          0.7353785036415801,
          0.3489295961156477,
          0.5532994923857867,
          0.3376738026925623,
          0.00375193114102847,
          0.31781063782829394,
          0.12756565879496798,
          0.3502538071065989,
          0.48775104833370114,
          0.033767380269256235,
          0.8274111675126903,
          0.23835797837122047,
          0.21628779518870006,
          0.08254248510262634,
          0.8254248510262634,
          0.03045685279187817,
          0.07128669167954092,
          0.7360406091370558,
          0.38578680203045684,
          0.06003089825645552,
          0.41116751269035534,
          0.07879055396159788,
          0.44162436548223344,
          0.7203707790774663,
          0.8563231074817921,
          0.09644670050761421,
          0.1986316486426837,
          0.38843522401235925,
          0.21187375855219598,
          0.2081218274111675,
          0.4325755903774001,
          0.8730964467005076,
          0.3487088942838225,
          0.5076142131979695,
          0.34142573383359076,
          0.7969543147208121,
          0.3089825645552858,
          0.21319796954314718,
          0.5552858088722136,
          0.34142573383359076,
          0.2780843080997572,
          0.5552858088722136,
          0.18980357536967554,
          0.781725888324873,
          0.7724564113882144,
          0.9035532994923856,
          0.17258883248730963,
          0.048775104833370116,
          0.375193114102847,
          0.21187375855219598,
          0.18384462591039502,
          0.6446700507614214,
          0.11035091591260207,
          0.09644670050761421,
          0.06003089825645552,
          0.45464577355992053,
          0.5076142131979695,
          0.3502538071065989,
          0.36393732067976164,
          0.35532994923857864,
          0.4414036636504083,
          0.2030456852791878,
          0.033767380269256235
         ],
         "xaxis": "x",
         "y": [
          0.6155461847793241,
          0.9228157898814048,
          1.0645647236053224,
          0.6190085231800094,
          0.6430318142201775,
          0.6233399000438051,
          0.8882236129856965,
          0.728138391448806,
          0.8760935661450563,
          0.8102835365186419,
          0.8129897046356815,
          0.9278203266142478,
          0.6653344675738562,
          1.128965568031659,
          0.7731533805104698,
          1.133549538614931,
          1.1226152958920288,
          0.6242516933354635,
          1.2782437811032332,
          1.2287670929496604,
          1.0572209630734675,
          0.6242516933354635,
          0.8129897046356815,
          0.7730818261925849,
          1.0645647236053224,
          1.2027013315295105,
          0.5784202469726393,
          0.6696920977492333,
          0.8129897046356815,
          0.8392130792670346,
          0.9256123096041577,
          1.2782437811032332,
          0.6696920977492333,
          0.7676881557232097,
          0.7721700329009266,
          0.7217572006155145,
          0.8938166108262955,
          0.9293178329944263,
          0.68219045576689,
          1.119824043671054,
          1.2309751099597506,
          2.360246451474126,
          0.6701516690943575,
          0.7266985970795709,
          1.2804517981133234,
          3.591573639021755,
          0.7676881557232097,
          0.6696920977492333,
          1.4729673042948777,
          1.4886589551276375,
          0.7782978617196726,
          0.9293178329944263,
          0.6248617041832439,
          1.1096958453802825,
          1.276241170050909,
          0.621949313417716,
          1.0935463154186575,
          3.895237054631072,
          0.8285196327006729,
          1.1211193324873832,
          0.6233399000438051,
          1.133549538614931,
          2.8591629098122877,
          1.056539325226626,
          1.2027013315295105,
          1.0574511185182842,
          1.120207539195725,
          1.2521780196830834,
          1.1991848909520428,
          0.6562025514118967,
          1.4786142666104083,
          1.5251660950109724,
          1.1255892495630684,
          1.4640497142422326,
          1.0892965686989118,
          0.9165131806369283,
          3.9046853515860533,
          0.7730818261925849,
          0.6242516933354635,
          1.2330168396694061,
          0.6233399000438051,
          3.870271473444342,
          0.6696920977492333,
          0.7430669058769287,
          1.0506376304314062,
          0.5911963450200739,
          2.3109227587551424,
          0.621949313417716,
          0.7650528690026629,
          0.7666096312910925,
          1.397462413230191,
          0.6154278230532134,
          0.8384840470231387,
          0.8081927443609856,
          0.8904847646483098,
          2.62320516970114,
          1.2027013315295105,
          0.5862250582671887,
          1.5266941912693366,
          1.6025866427170141,
          0.9290664968440012,
          0.8839050368193696,
          1.1255892495630684,
          2.2881226993534645,
          1.2804517981133234,
          0.6248617041832439,
          1.0574511185182842,
          1.1226152958920288,
          0.6233399000438051,
          1.2287670929496604,
          0.5911963450200739,
          2.942380202268797,
          0.7801954312796229,
          0.8807888220223115,
          0.8724947449620987,
          0.6026942182140014,
          0.5921081383117323,
          0.7650528690026629,
          1.1228999825529782,
          1.1211193324873832,
          0.7794057382825397,
          0.9293178329944263,
          0.5892987754601237,
          1.2027013315295105,
          0.8102835365186419,
          1.4413480196246113,
          2.306596952679573,
          3.0140336868063278,
          0.5862250582671887,
          2.0741253520509564,
          0.6951481779234018,
          0.5790243169174282,
          1.5266941912693366,
          1.1255892495630684,
          0.6242516933354635,
          1.0506376304314062,
          0.6430318142201775,
          0.8120779113440232,
          0.7650528690026629,
          1.2456677522270625,
          1.2486615791056157,
          0.5921081383117323,
          0.9228157898814048,
          1.4886589551276375,
          0.6859506544454586,
          0.8034124108737369,
          3.595780959294508,
          0.7801954312796229,
          0.7730818261925849,
          0.8925782326875918,
          1.2804517981133234,
          2.621296235259164,
          1.0892965686989118,
          0.5862250582671887,
          0.6233399000438051,
          1.2521780196830834,
          1.0574511185182842,
          1.0574511185182842,
          0.6026942182140014,
          1.064974896438853,
          0.7794057382825397,
          1.056539325226626,
          0.5911963450200739,
          0.9293178329944263,
          0.6951481779234018,
          0.6086429118919788,
          0.7730818261925849,
          0.6233399000438051,
          0.8201033097227195,
          0.5862250582671887,
          1.133549538614931,
          1.1211193324873832,
          1.1211193324873832,
          0.6242516933354635,
          0.8552857613061718,
          0.8120779113440232,
          1.1274709663957305,
          1.064974896438853,
          0.8267281945640218,
          1.7516359457113626,
          0.8839050368193696,
          0.6242516933354635,
          0.5921081383117323,
          1.1226152958920288,
          0.6491216341368565,
          1.2027013315295105,
          0.9610907565600675,
          0.6851950320640818,
          0.6430318142201775,
          1.889020857960701,
          0.7730818261925849,
          2.014235923667022,
          1.5226814356513285,
          1.0645647236053224,
          0.5921081383117323,
          1.0645647236053224,
          0.8081927443609856,
          0.7730818261925849,
          1.2287670929496604,
          0.8956361400188532,
          0.9298620563239034,
          0.8384840470231387,
          1.2027013315295105,
          0.5911963450200739,
          0.7801954312796229,
          0.6154278230532134,
          1.091504585709002,
          0.8446626326477781,
          0.7676881557232097,
          0.7247005589759677,
          0.6154278230532134,
          0.908400218661733,
          0.6430318142201775,
          1.0645647236053224,
          0.5921081383117323,
          1.120207539195725,
          1.1560361705812516,
          1.2521780196830834,
          1.2027013315295105,
          1.5226814356513285,
          3.895237054631072,
          0.9307636849747011,
          0.9256123096041577,
          0.8285196327006729,
          0.68219045576689,
          0.5891374490327167,
          0.6248617041832439,
          0.8315417347252156,
          2.103556520790529,
          0.7730818261925849,
          1.2521780196830834,
          0.8392130792670346,
          0.6242516933354635,
          1.5226814356513285,
          0.9307636849747011,
          0.6723360807821935,
          0.6242516933354635,
          0.7676881557232097,
          0.7282169995534352,
          0.8120779113440232,
          0.621949313417716,
          0.6233399000438051,
          1.2309751099597506,
          1.2027013315295105,
          0.7721700329009266,
          0.8691512538471332,
          0.7282169995534352,
          2.2448315857624745,
          2.9755757287979656,
          1.1225819852860845,
          1.0574511185182842,
          0.7676881557232097,
          0.775155991562794,
          1.1211193324873832,
          0.5862250582671887,
          0.6248617041832439,
          0.7801954312796229,
          3.016332072637105,
          0.8392130792670346,
          0.6430318142201775,
          0.8120779113440232,
          1.2782437811032332,
          2.9135296383416227,
          0.8065175097341891,
          1.1226152958920288,
          1.5226814356513285,
          1.7570592509984182,
          1.1171654108502163,
          0.9965971747967023,
          1.2027013315295105,
          0.7730818261925849,
          1.6817736680956321,
          0.8129897046356815,
          0.7676881557232097,
          1.2265412929563608,
          0.6233399000438051,
          0.8882236129856965,
          0.6365596193186851,
          0.7282169995534352,
          0.6696920977492333,
          0.6066403008396546,
          0.6242516933354635,
          1.4886589551276375,
          1.0413751998355418,
          1.040811280621594,
          0.6430318142201775,
          1.7937947982552551,
          0.6233399000438051,
          1.2521780196830834,
          0.5882047395457417,
          1.2027013315295105,
          1.1235048500977018,
          1.0645647236053224,
          1.2521780196830834,
          1.5226814356513285,
          0.7195491836054244,
          0.9256123096041577,
          1.1210281943744538,
          1.1847076349959733,
          1.3221881152819384,
          1.5472466522958683,
          0.5911963450200739,
          1.2330168396694061,
          1.3221881152819384,
          0.6086429118919788,
          0.5891374490327167,
          0.6233399000438051,
          0.6233399000438051,
          3.9046853515860533,
          0.6026942182140014,
          0.7666096312910925,
          1.1226152958920288,
          0.8129897046356815,
          0.7708194280772647,
          0.5891374490327167,
          0.9165131806369283,
          3.012124752364352,
          0.5862250582671887,
          0.8392130792670346,
          0.7676881557232097,
          1.1255892495630684,
          0.6155461847793241,
          0.6066403008396546,
          0.8926927816583999,
          1.1023968911329334,
          1.0356694002329658,
          0.7148539277281692,
          0.8082809254663177,
          1.1226152958920288,
          0.8392130792670346,
          1.0574511185182842,
          0.7721700329009266,
          0.7282169995534352,
          0.8839050368193696,
          2.2881226993534645,
          0.6696920977492333,
          0.7801954312796229,
          0.8812219997773592,
          0.7731533805104698,
          1.0574511185182842,
          0.68219045576689,
          0.8081927443609856,
          1.064974896438853,
          1.127363795727488,
          1.2027013315295105,
          1.0574511185182842,
          0.7676881557232097,
          1.4078930071335756,
          0.775155991562794,
          1.0892965686989118,
          1.5251660950109724,
          1.120207539195725,
          3.2828769106667948,
          1.091504585709002,
          0.8708968993542585,
          1.11503618978528,
          0.6242516933354635,
          0.686943600764599,
          0.7730818261925849,
          0.8882236129856965,
          1.056539325226626,
          0.8034124108737369,
          0.6026942182140014,
          0.7282169995534352,
          0.68219045576689,
          1.0935463154186575,
          0.7773640085728841,
          0.7730818261925849,
          1.1211193324873832,
          0.6430318142201775,
          0.9417797995331112,
          3.5455077719786394,
          0.7801954312796229,
          0.8760935661450563,
          1.0645647236053224,
          1.2027013315295105,
          0.775155991562794,
          0.7708194280772647,
          0.6562025514118967,
          1.0574511185182842,
          0.9293178329944263,
          0.7801954312796229,
          0.8691512538471332,
          1.1211193324873832,
          0.6430318142201775,
          1.2804517981133234,
          0.5891374490327167,
          2.9837602533485774,
          0.8201033097227195,
          1.0356694002329658,
          0.8458588402302021,
          1.0574511185182842,
          0.775155991562794,
          1.3221881152819384,
          1.056539325226626,
          0.5911963450200739,
          0.9290664968440012,
          0.9256123096041577,
          0.5911963450200739,
          1.0574511185182842,
          0.8129897046356815,
          0.5862250582671887,
          0.7148539277281692,
          3.916506272872338,
          0.9480851396171222,
          1.2027013315295105,
          0.6242516933354635,
          1.2521780196830834,
          1.5501661894116276,
          0.9293178329944263,
          1.091504585709002,
          0.7282169995534352,
          1.1211193324873832,
          3.595780959294508,
          0.5891374490327167,
          0.8713781908973701,
          0.9165131806369283,
          1.4886589551276375,
          0.7676881557232097,
          0.7398177613147785,
          1.0892965686989118,
          0.5891374490327167,
          0.5911963450200739,
          1.5472466522958683,
          0.6723360807821935,
          1.2287670929496604,
          0.6597740659640221,
          1.1202105780807279,
          0.7460637092693108,
          0.613338167769234,
          0.9165131806369283,
          0.7721700329009266,
          0.8941902880385784,
          0.8159829462426691,
          0.8211677929541877,
          0.5862250582671887,
          0.7801954312796229,
          0.6242516933354635,
          1.2521780196830834,
          0.6248617041832439,
          0.7801954312796229,
          0.7650528690026629,
          1.1211193324873832,
          1.1226152958920288,
          0.9293178329944263,
          1.0574511185182842,
          0.7801954312796229,
          0.6248617041832439,
          0.7676881557232097,
          0.621949313417716,
          1.3418923174923127,
          1.5472466522958683,
          1.056539325226626,
          0.8591564259544826,
          1.062667154045372,
          0.6242516933354635,
          0.7773640085728841,
          0.613338167769234,
          1.0892965686989118,
          0.8276399878556802,
          0.7730818261925849,
          0.7282169995534352,
          1.056539325226626,
          0.6233399000438051,
          0.8211677929541877,
          0.7650528690026629,
          3.916506272872338
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "variable"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "r"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "value"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"09aea5d2-b799-466c-9cb9-9a4cc8f9f87c\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"09aea5d2-b799-466c-9cb9-9a4cc8f9f87c\")) {                    Plotly.newPlot(                        \"09aea5d2-b799-466c-9cb9-9a4cc8f9f87c\",                        [{\"hovertemplate\": \"variable=Data<br>r=%{x}<br>value=%{y}<extra></extra>\", \"legendgroup\": \"Data\", \"marker\": {\"color\": \"#636efa\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"Data\", \"showlegend\": true, \"type\": \"scattergl\", \"x\": [0.12690355329949238, 0.7391304347826085, 0.22886779960273668, 0.9847715736040608, 0.49746192893401014, 0.2182741116751269, 0.4016773339218715, 0.8695652173913043, 0.06598984771573603, 0.03089825645552858, 0.45464577355992053, 0.13242109909512248, 0.42639593908629436, 0.1500772456411388, 0.004414036636504083, 0.09710880600308981, 0.21761200617965126, 0.5685279187817259, 0.033767380269256235, 0.056278967115427056, 0.7316265725005517, 0.598984771573604, 0.481129993378945, 0.5473405429265062, 0.21761200617965126, 0.24762745530787905, 0.9847715736040608, 0.4619289340101523, 0.5473405429265062, 0.3575369675568307, 0.07945265945707349, 0.07879055396159788, 0.47208121827411165, 0.8474950342087839, 0.4016773339218715, 0.13705583756345177, 0.7541381593467225, 0.1986316486426837, 0.3502538071065989, 0.02626351798719929, 0.12381372765393953, 0.09644670050761421, 0.11917898918561023, 0.8783932906643125, 0.11255793423085411, 0.04502317369234164, 0.7856985212977267, 0.47208121827411165, 0.15758110792319577, 0.10505407194879716, 0.1765614654601633, 0.26484219819024496, 0.7360406091370558, 0.7241227102184948, 0.01125579342308541, 0.8730964467005076, 0.1500772456411388, 0.06378282939748399, 0.020304568527918777, 0.5890531891414698, 0.4314720812182741, 0.044140366365040824, 0.08629441624365482, 0.4089604943721033, 0.2138600750386228, 0.7166188479364378, 0.36393732067976164, 0.28139483557713524, 0.18009269476936657, 0.49746192893401014, 0.12756565879496798, 0.09379827852571175, 0.10505407194879716, 0.16883690134628115, 0.02626351798719929, 0.22070183182520414, 0.06003089825645552, 0.6797616420216287, 0.4923857868020304, 0.14632531450011033, 0.2030456852791878, 0.07128669167954092, 0.4619289340101523, 0.2639593908629441, 0.18097550209666738, 0.416243654822335, 0.11675126903553298, 0.964467005076142, 0.2030456852791878, 0.4414036636504083, 0.16508497020525267, 1.0, 0.05076142131979695, 0.7724564113882144, 0.07945265945707349, 0.015228426395939085, 0.22511586846170822, 0.9340101522842639, 0.0375193114102847, 0.10593687927609799, 0.12690355329949238, 0.14213197969543148, 0.11630986537188258, 0.0812182741116751, 0.12381372765393953, 0.8426395939086294, 0.5440300154491282, 0.2401235930258221, 0.3096446700507614, 0.04502317369234164, 0.38578680203045684, 0.026484219819024497, 0.2030456852791878, 0.11167512690355329, 0.2780843080997572, 0.06598984771573603, 0.598984771573604, 0.2182741116751269, 0.08254248510262634, 0.4652394614875303, 0.16331935555065105, 0.3310527477378062, 0.17258883248730963, 0.3264180092694769, 0.09710880600308981, 0.13242109909512248, 0.1013021408077687, 0.013242109909512249, 0.964467005076142, 0.09710880600308981, 0.3959390862944162, 0.750386228205694, 0.0375193114102847, 0.1388214522180534, 0.6294416243654821, 0.1765614654601633, 0.4771573604060913, 0.375193114102847, 0.2081218274111675, 0.19885235047450892, 0.18009269476936657, 0.7208121827411167, 0.7391304347826085, 0.09379827852571175, 0.10659898477157359, 0.851909070845288, 0.02251158684617082, 0.26925623482674904, 0.5605826528360185, 0.37144118296181855, 0.13506952107702494, 0.0812182741116751, 0.048775104833370116, 0.949238578680203, 0.1878172588832487, 0.3264180092694769, 0.5440300154491282, 0.6565879496799822, 0.05076142131979695, 0.3564334583977047, 0.16331935555065105, 0.42021628779518866, 0.2182741116751269, 0.22511586846170822, 0.365482233502538, 0.05076142131979695, 0.45464577355992053, 0.3096446700507614, 0.322224674464798, 0.9187817258883249, 0.057382476274553076, 0.6903553299492385, 0.6115647759876407, 0.6903553299492385, 0.42374751710439196, 0.42816155374089604, 0.01765614654601633, 0.375193114102847, 0.42374751710439196, 0.11167512690355329, 0.12690355329949238, 0.7208121827411167, 0.4771573604060913, 0.34142573383359076, 0.38071065989847713, 0.35268152725667623, 0.026484219819024497, 0.06091370558375634, 0.46700507614213194, 0.13197969543147206, 0.6267932023835797, 0.1015228426395939, 0.1538291767821673, 0.3076583535643346, 0.6142131979695431, 0.2513793864489075, 0.7592143014787022, 0.6003089825645552, 0.06753476053851247, 0.17214742882365922, 0.15890531891414697, 0.03553299492385787, 0.28139483557713524, 0.2030456852791878, 0.2295299050982123, 1.0, 0.11630986537188258, 0.2030456852791878, 0.8386669609357756, 0.182741116751269, 0.9949238578680203, 0.23835797837122047, 0.5025380710659898, 0.29640256014124916, 0.6598984771573604, 0.39770470094901783, 0.1013021408077687, 0.30390642242330607, 0.22511586846170822, 0.12756565879496798, 0.06753476053851247, 0.17214742882365922, 0.09269476936658573, 0.005076142131979694, 0.1979695431472081, 0.7512690355329948, 0.766497461928934, 0.17258883248730963, 0.10659898477157359, 0.6135510924740675, 0.31516221584639154, 0.3972632972853674, 0.5532994923857867, 0.1500772456411388, 0.17214742882365922, 0.41116751269035534, 0.6142131979695431, 0.851909070845288, 0.31979695431472077, 0.38843522401235925, 0.8883248730964466, 0.3248730964467004, 0.11255793423085411, 0.30390642242330607, 0.375193114102847, 0.17258883248730963, 0.21319796954314718, 0.015228426395939085, 0.08629441624365482, 0.06003089825645552, 0.47649525491061573, 0.8474950342087839, 0.044140366365040824, 0.7166188479364378, 0.9695431472081217, 0.7969543147208121, 0.322224674464798, 0.026484219819024497, 0.38402118737585517, 0.4619289340101523, 0.4016773339218715, 0.056278967115427056, 0.06621054954756124, 0.4414036636504083, 0.35268152725667623, 0.1388214522180534, 0.12690355329949238, 0.18384462591039502, 0.1875965570514235, 0.19134848819245198, 0.7062458618406532, 0.10593687927609799, 0.4678878834694328, 0.7724564113882144, 0.16883690134628115, 0.2791878172588832, 0.38402118737585517, 0.44162436548223344, 0.27411167512690354, 0.45685279187817257, 0.005076142131979694, 0.6142131979695431, 0.06003089825645552, 0.11476495254910615, 0.19421761200617962, 0.45685279187817257, 0.11255793423085411, 0.40101522842639586, 0.25888324873096447, 0.8578680203045684, 0.25888324873096447, 0.13683513573162656, 0.19510041933348046, 0.34142573383359076, 0.1613330390642242, 0.0913705583756345, 0.06621054954756124, 0.0375193114102847, 0.3564334583977047, 0.18009269476936657, 0.02626351798719929, 0.416243654822335, 0.15758110792319577, 0.18009269476936657, 0.0812182741116751, 0.7512690355329948, 0.416243654822335, 0.24873096446700507, 0.056278967115427056, 0.0812182741116751, 0.4414036636504083, 0.2401235930258221, 0.6665195321121165, 0.21628779518870006, 0.8426395939086294, 0.21187375855219598, 0.07062458618406532, 0.964467005076142, 0.41491944383138374, 0.7327300816596777, 0.11630986537188258, 0.11167512690355329, 0.020304568527918777, 0.13242109909512248, 0.36393732067976164, 0.7278746413595233, 0.07614213197969542, 0.004414036636504083, 0.29640256014124916, 0.4016773339218715, 0.7203707790774663, 0.42816155374089604, 0.22842639593908629, 0.14213197969543148, 0.0812182741116751, 0.4771573604060913, 0.2560141249172368, 0.4193334804678878, 0.0, 0.45398366806444485, 0.24365482233502536, 0.8254248510262634, 0.36768925182079004, 0.17258883248730963, 0.34142573383359076, 0.4652394614875303, 0.8386669609357756, 0.1613330390642242, 0.07062458618406532, 0.09379827852571175, 0.09379827852571175, 0.3864489075259324, 0.07879055396159788, 0.1388214522180534, 0.37894504524387546, 0.36018538953873314, 0.6446700507614214, 0.182741116751269, 0.640035312293092, 0.3707790774663429, 0.39770470094901783, 0.851909070845288, 0.03553299492385787, 0.3350253807106599, 0.22842639593908629, 0.1500772456411388, 0.12359302582211432, 0.6532774222026042, 0.6790995365261531, 0.46700507614213194, 0.7391304347826085, 0.06003089825645552, 0.21628779518870006, 0.05076142131979695, 0.33016994041050535, 0.3489295961156477, 0.057382476274553076, 0.22070183182520414, 0.4923857868020304, 0.4990068417567865, 0.21187375855219598, 0.24277201500772455, 0.17258883248730963, 0.45398366806444485, 0.4771573604060913, 0.11255793423085411, 0.8274111675126903, 0.0750386228205694, 0.21628779518870006, 0.7278746413595233, 0.12690355329949238, 0.48775104833370114, 0.09710880600308981, 0.18384462591039502, 0.3864489075259324, 0.35532994923857864, 0.12690355329949238, 0.052968439638048995, 0.24873096446700507, 0.5102626351798719, 0.5208563231074818, 0.9746192893401014, 0.03045685279187817, 0.033767380269256235, 0.7353785036415801, 0.3489295961156477, 0.5532994923857867, 0.3376738026925623, 0.00375193114102847, 0.31781063782829394, 0.12756565879496798, 0.3502538071065989, 0.48775104833370114, 0.033767380269256235, 0.8274111675126903, 0.23835797837122047, 0.21628779518870006, 0.08254248510262634, 0.8254248510262634, 0.03045685279187817, 0.07128669167954092, 0.7360406091370558, 0.38578680203045684, 0.06003089825645552, 0.41116751269035534, 0.07879055396159788, 0.44162436548223344, 0.7203707790774663, 0.8563231074817921, 0.09644670050761421, 0.1986316486426837, 0.38843522401235925, 0.21187375855219598, 0.2081218274111675, 0.4325755903774001, 0.8730964467005076, 0.3487088942838225, 0.5076142131979695, 0.34142573383359076, 0.7969543147208121, 0.3089825645552858, 0.21319796954314718, 0.5552858088722136, 0.34142573383359076, 0.2780843080997572, 0.5552858088722136, 0.18980357536967554, 0.781725888324873, 0.7724564113882144, 0.9035532994923856, 0.17258883248730963, 0.048775104833370116, 0.375193114102847, 0.21187375855219598, 0.18384462591039502, 0.6446700507614214, 0.11035091591260207, 0.09644670050761421, 0.06003089825645552, 0.45464577355992053, 0.5076142131979695, 0.3502538071065989, 0.36393732067976164, 0.35532994923857864, 0.4414036636504083, 0.2030456852791878, 0.033767380269256235], \"xaxis\": \"x\", \"y\": [0.6061, 0.5017199999999999, 1.0674, 0.61416, 0.6765100000000001, 0.61165, 0.9469799999999999, 0.6339899999999999, 0.8710399999999999, 0.7925800000000001, 0.81788, 0.93976, 0.71763, 1.1096, 0.7520899999999999, 1.1566, 1.1199, 0.62097, 1.2945, 1.2389, 1.0221, 0.6221800000000001, 0.8186399999999999, 0.7819699999999999, 1.0676, 1.26, 0.50716, 0.71532, 0.82004, 0.90781, 0.93891, 1.3026, 0.71337, 0.75536, 0.77772, 0.7104699999999999, 0.85883, 0.9490700000000001, 0.68693, 1.0944, 1.2454, 2.3007, 0.6539699999999999, 0.7548699999999999, 1.304, 3.6409, 0.7864899999999999, 0.7125100000000001, 1.3705, 1.5278, 0.76707, 0.95321, 0.62507, 1.1282, 1.2924, 0.62537, 1.059, 4.041, 0.82965, 1.1388, 0.61888, 1.1538, 2.5568, 1.0779, 1.2554, 1.0834, 1.1275, 1.3205, 1.2487, 0.51078, 1.5303, 1.6032, 1.1025, 1.4478, 1.042, 0.98244, 4.0506, 0.786, 0.6197600000000001, 1.2499, 0.61002, 3.9577, 0.71529, 0.56183, 1.1025, 0.58934, 1.2188, 0.62227, 0.83265, 0.77761, 1.0894, 0.50512, 0.8342700000000001, 0.8252200000000001, 0.8936700000000001, 2.6169, 1.2589, 0.59665, 1.5312, 1.6153, 0.91669, 0.8723200000000001, 1.1024, 2.2049, 1.3052, 0.62634, 1.0841, 1.121, 0.6164, 1.232, 0.58841, 2.9172, 0.7659699999999999, 0.87284, 0.90636, 0.57326, 0.59299, 0.77994, 1.1013, 1.1321, 0.76658, 0.9528, 0.57953, 1.2507, 0.79771, 1.4552, 2.8378, 3.0621, 0.59442, 3.05, 0.7203, 0.60868, 1.5251, 1.1069, 0.6221399999999999, 1.0976, 0.68219, 0.8156, 0.8269200000000001, 0.92106, 1.3173, 0.5956600000000001, 1.0777, 1.5293, 0.67374, 0.81921, 3.6377, 0.7723, 0.7803800000000001, 0.7228, 1.3067, 2.5994, 1.0418, 0.59595, 0.60818, 1.3138, 1.0811, 1.0846, 0.5716399999999999, 1.2497, 0.76059, 1.0776, 0.58305, 0.95134, 0.7205600000000001, 0.6025, 0.77841, 0.6158100000000001, 0.81415, 0.5969399999999999, 1.154, 1.1347, 1.139, 0.62342, 0.84915, 0.81657, 1.0968, 1.1299, 0.9006200000000001, 1.806, 0.8726799999999999, 0.6248, 0.59102, 1.1268, 0.68791, 0.91755, 0.88476, 0.67375, 0.66804, 2.2026, 0.78492, 2.1216, 1.6006, 1.0735, 0.59285, 1.0693, 0.8242799999999999, 0.78452, 1.2393, 0.8959, 0.9436899999999999, 0.82934, 1.2585, 0.57904, 0.7702899999999999, 0.5269699999999999, 1.0492, 0.87255, 0.78492, 0.71281, 0.57772, 0.50015, 0.66274, 1.0686, 0.59334, 1.1301, 0.98345, 1.3183, 1.2562, 1.6001, 4.0259, 0.9451200000000001, 0.93855, 0.8302200000000001, 0.68219, 0.59531, 0.62513, 0.8318399999999999, 2.614, 0.78432, 1.3151, 0.904, 0.6209899999999999, 1.6005, 0.9451, 0.71973, 0.62211, 0.7089, 0.7205600000000001, 0.8154600000000001, 0.62513, 0.6167199999999999, 1.2429, 1.258, 0.77674, 0.8712, 0.71668, 2.1967, 1.7347, 1.0994, 1.0806, 0.7855, 0.75364, 1.1299, 0.59053, 0.62589, 0.77318, 3.0519, 0.90138, 0.67788, 0.8157399999999999, 1.2997, 2.9248, 0.81686, 1.1269, 1.6017, 2.394, 0.86634, 0.7747, 1.2566, 0.7857, 2.9538, 0.81892, 0.78601, 1.2475, 0.6150899999999999, 0.9476100000000001, 0.68533, 0.7197, 0.71503, 0.60028, 0.62231, 1.5263, 0.97016, 0.8627, 0.68455, 1.9573, 0.61843, 1.3212, 0.5965, 1.2607, 1.1536, 1.0603, 1.31, 1.5853, 0.71017, 0.93606, 1.0946, 1.2399, 1.5067, 1.5998, 0.58919, 1.2448, 0.5012, 0.60434, 0.5963, 0.61856, 0.6136, 4.0533, 0.57587, 0.77969, 1.1216, 0.82481, 0.64904, 0.59718, 1.0898, 3.0134, 0.59442, 0.72556, 0.7858, 1.1023, 0.60541, 0.6007600000000001, 0.89243, 0.95988, 1.0829, 0.7089300000000001, 0.79105, 1.1245, 0.90507, 0.99729, 0.77822, 0.71834, 0.87225, 2.2048, 0.70881, 0.77279, 0.89731, 0.75271, 1.0796, 0.68258, 0.82155, 1.2371, 1.1146, 1.2117, 1.0774, 0.78128, 1.2324, 0.75937, 1.0452, 1.603, 1.1301, 2.3861, 1.0578, 0.54188, 1.1485, 0.62248, 0.68083, 0.78364, 0.95011, 1.0774, 0.81386, 0.57213, 0.7205600000000001, 0.68555, 1.0513, 0.7593, 0.78561, 1.1354, 0.68376, 0.83091, 3.5357, 0.76764, 0.8712700000000001, 1.074, 1.0347, 0.75542, 0.52267, 0.5905, 1.0789, 0.9499, 0.7723, 0.8712700000000001, 1.132, 0.58855, 1.3034, 0.5967899999999999, 3.0553, 0.80937, 0.8058, 0.8345799999999999, 1.0787, 0.75942, 1.4611, 1.0765, 0.58885, 0.98703, 0.9362299999999999, 0.58368, 1.0823, 0.81886, 0.58208, 0.70508, 4.0465, 1.0207, 1.2534, 0.62138, 1.3111, 1.5966, 0.9538399999999999, 1.0546, 0.7209800000000001, 1.1337, 3.6437, 0.5967899999999999, 0.9067799999999999, 1.0598, 1.5335, 0.78432, 0.67237, 1.0469, 0.59595, 0.58905, 1.6052, 0.71917, 1.2395, 0.7168, 1.1324, 0.7842399999999999, 0.60493, 1.1008, 0.77685, 0.9057200000000001, 0.87277, 0.8921600000000001, 0.59665, 0.7762399999999999, 0.62051, 1.3105, 0.62502, 0.77496, 0.81481, 1.1364, 1.1266, 0.9538399999999999, 1.0846, 0.7648699999999999, 0.6258699999999999, 0.7858, 0.62621, 0.80479, 1.6016, 1.0732, 0.77644, 1.0649, 0.6230899999999999, 0.76017, 0.60482, 1.0433, 0.535, 0.78008, 0.72027, 1.0728, 0.61741, 0.82124, 0.8327700000000001, 4.0382], \"yaxis\": \"y\"}, {\"hovertemplate\": \"variable=ANN<br>r=%{x}<br>value=%{y}<extra></extra>\", \"legendgroup\": \"ANN\", \"marker\": {\"color\": \"#EF553B\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"ANN\", \"showlegend\": true, \"type\": \"scattergl\", \"x\": [0.12690355329949238, 0.7391304347826085, 0.22886779960273668, 0.9847715736040608, 0.49746192893401014, 0.2182741116751269, 0.4016773339218715, 0.8695652173913043, 0.06598984771573603, 0.03089825645552858, 0.45464577355992053, 0.13242109909512248, 0.42639593908629436, 0.1500772456411388, 0.004414036636504083, 0.09710880600308981, 0.21761200617965126, 0.5685279187817259, 0.033767380269256235, 0.056278967115427056, 0.7316265725005517, 0.598984771573604, 0.481129993378945, 0.5473405429265062, 0.21761200617965126, 0.24762745530787905, 0.9847715736040608, 0.4619289340101523, 0.5473405429265062, 0.3575369675568307, 0.07945265945707349, 0.07879055396159788, 0.47208121827411165, 0.8474950342087839, 0.4016773339218715, 0.13705583756345177, 0.7541381593467225, 0.1986316486426837, 0.3502538071065989, 0.02626351798719929, 0.12381372765393953, 0.09644670050761421, 0.11917898918561023, 0.8783932906643125, 0.11255793423085411, 0.04502317369234164, 0.7856985212977267, 0.47208121827411165, 0.15758110792319577, 0.10505407194879716, 0.1765614654601633, 0.26484219819024496, 0.7360406091370558, 0.7241227102184948, 0.01125579342308541, 0.8730964467005076, 0.1500772456411388, 0.06378282939748399, 0.020304568527918777, 0.5890531891414698, 0.4314720812182741, 0.044140366365040824, 0.08629441624365482, 0.4089604943721033, 0.2138600750386228, 0.7166188479364378, 0.36393732067976164, 0.28139483557713524, 0.18009269476936657, 0.49746192893401014, 0.12756565879496798, 0.09379827852571175, 0.10505407194879716, 0.16883690134628115, 0.02626351798719929, 0.22070183182520414, 0.06003089825645552, 0.6797616420216287, 0.4923857868020304, 0.14632531450011033, 0.2030456852791878, 0.07128669167954092, 0.4619289340101523, 0.2639593908629441, 0.18097550209666738, 0.416243654822335, 0.11675126903553298, 0.964467005076142, 0.2030456852791878, 0.4414036636504083, 0.16508497020525267, 1.0, 0.05076142131979695, 0.7724564113882144, 0.07945265945707349, 0.015228426395939085, 0.22511586846170822, 0.9340101522842639, 0.0375193114102847, 0.10593687927609799, 0.12690355329949238, 0.14213197969543148, 0.11630986537188258, 0.0812182741116751, 0.12381372765393953, 0.8426395939086294, 0.5440300154491282, 0.2401235930258221, 0.3096446700507614, 0.04502317369234164, 0.38578680203045684, 0.026484219819024497, 0.2030456852791878, 0.11167512690355329, 0.2780843080997572, 0.06598984771573603, 0.598984771573604, 0.2182741116751269, 0.08254248510262634, 0.4652394614875303, 0.16331935555065105, 0.3310527477378062, 0.17258883248730963, 0.3264180092694769, 0.09710880600308981, 0.13242109909512248, 0.1013021408077687, 0.013242109909512249, 0.964467005076142, 0.09710880600308981, 0.3959390862944162, 0.750386228205694, 0.0375193114102847, 0.1388214522180534, 0.6294416243654821, 0.1765614654601633, 0.4771573604060913, 0.375193114102847, 0.2081218274111675, 0.19885235047450892, 0.18009269476936657, 0.7208121827411167, 0.7391304347826085, 0.09379827852571175, 0.10659898477157359, 0.851909070845288, 0.02251158684617082, 0.26925623482674904, 0.5605826528360185, 0.37144118296181855, 0.13506952107702494, 0.0812182741116751, 0.048775104833370116, 0.949238578680203, 0.1878172588832487, 0.3264180092694769, 0.5440300154491282, 0.6565879496799822, 0.05076142131979695, 0.3564334583977047, 0.16331935555065105, 0.42021628779518866, 0.2182741116751269, 0.22511586846170822, 0.365482233502538, 0.05076142131979695, 0.45464577355992053, 0.3096446700507614, 0.322224674464798, 0.9187817258883249, 0.057382476274553076, 0.6903553299492385, 0.6115647759876407, 0.6903553299492385, 0.42374751710439196, 0.42816155374089604, 0.01765614654601633, 0.375193114102847, 0.42374751710439196, 0.11167512690355329, 0.12690355329949238, 0.7208121827411167, 0.4771573604060913, 0.34142573383359076, 0.38071065989847713, 0.35268152725667623, 0.026484219819024497, 0.06091370558375634, 0.46700507614213194, 0.13197969543147206, 0.6267932023835797, 0.1015228426395939, 0.1538291767821673, 0.3076583535643346, 0.6142131979695431, 0.2513793864489075, 0.7592143014787022, 0.6003089825645552, 0.06753476053851247, 0.17214742882365922, 0.15890531891414697, 0.03553299492385787, 0.28139483557713524, 0.2030456852791878, 0.2295299050982123, 1.0, 0.11630986537188258, 0.2030456852791878, 0.8386669609357756, 0.182741116751269, 0.9949238578680203, 0.23835797837122047, 0.5025380710659898, 0.29640256014124916, 0.6598984771573604, 0.39770470094901783, 0.1013021408077687, 0.30390642242330607, 0.22511586846170822, 0.12756565879496798, 0.06753476053851247, 0.17214742882365922, 0.09269476936658573, 0.005076142131979694, 0.1979695431472081, 0.7512690355329948, 0.766497461928934, 0.17258883248730963, 0.10659898477157359, 0.6135510924740675, 0.31516221584639154, 0.3972632972853674, 0.5532994923857867, 0.1500772456411388, 0.17214742882365922, 0.41116751269035534, 0.6142131979695431, 0.851909070845288, 0.31979695431472077, 0.38843522401235925, 0.8883248730964466, 0.3248730964467004, 0.11255793423085411, 0.30390642242330607, 0.375193114102847, 0.17258883248730963, 0.21319796954314718, 0.015228426395939085, 0.08629441624365482, 0.06003089825645552, 0.47649525491061573, 0.8474950342087839, 0.044140366365040824, 0.7166188479364378, 0.9695431472081217, 0.7969543147208121, 0.322224674464798, 0.026484219819024497, 0.38402118737585517, 0.4619289340101523, 0.4016773339218715, 0.056278967115427056, 0.06621054954756124, 0.4414036636504083, 0.35268152725667623, 0.1388214522180534, 0.12690355329949238, 0.18384462591039502, 0.1875965570514235, 0.19134848819245198, 0.7062458618406532, 0.10593687927609799, 0.4678878834694328, 0.7724564113882144, 0.16883690134628115, 0.2791878172588832, 0.38402118737585517, 0.44162436548223344, 0.27411167512690354, 0.45685279187817257, 0.005076142131979694, 0.6142131979695431, 0.06003089825645552, 0.11476495254910615, 0.19421761200617962, 0.45685279187817257, 0.11255793423085411, 0.40101522842639586, 0.25888324873096447, 0.8578680203045684, 0.25888324873096447, 0.13683513573162656, 0.19510041933348046, 0.34142573383359076, 0.1613330390642242, 0.0913705583756345, 0.06621054954756124, 0.0375193114102847, 0.3564334583977047, 0.18009269476936657, 0.02626351798719929, 0.416243654822335, 0.15758110792319577, 0.18009269476936657, 0.0812182741116751, 0.7512690355329948, 0.416243654822335, 0.24873096446700507, 0.056278967115427056, 0.0812182741116751, 0.4414036636504083, 0.2401235930258221, 0.6665195321121165, 0.21628779518870006, 0.8426395939086294, 0.21187375855219598, 0.07062458618406532, 0.964467005076142, 0.41491944383138374, 0.7327300816596777, 0.11630986537188258, 0.11167512690355329, 0.020304568527918777, 0.13242109909512248, 0.36393732067976164, 0.7278746413595233, 0.07614213197969542, 0.004414036636504083, 0.29640256014124916, 0.4016773339218715, 0.7203707790774663, 0.42816155374089604, 0.22842639593908629, 0.14213197969543148, 0.0812182741116751, 0.4771573604060913, 0.2560141249172368, 0.4193334804678878, 0.0, 0.45398366806444485, 0.24365482233502536, 0.8254248510262634, 0.36768925182079004, 0.17258883248730963, 0.34142573383359076, 0.4652394614875303, 0.8386669609357756, 0.1613330390642242, 0.07062458618406532, 0.09379827852571175, 0.09379827852571175, 0.3864489075259324, 0.07879055396159788, 0.1388214522180534, 0.37894504524387546, 0.36018538953873314, 0.6446700507614214, 0.182741116751269, 0.640035312293092, 0.3707790774663429, 0.39770470094901783, 0.851909070845288, 0.03553299492385787, 0.3350253807106599, 0.22842639593908629, 0.1500772456411388, 0.12359302582211432, 0.6532774222026042, 0.6790995365261531, 0.46700507614213194, 0.7391304347826085, 0.06003089825645552, 0.21628779518870006, 0.05076142131979695, 0.33016994041050535, 0.3489295961156477, 0.057382476274553076, 0.22070183182520414, 0.4923857868020304, 0.4990068417567865, 0.21187375855219598, 0.24277201500772455, 0.17258883248730963, 0.45398366806444485, 0.4771573604060913, 0.11255793423085411, 0.8274111675126903, 0.0750386228205694, 0.21628779518870006, 0.7278746413595233, 0.12690355329949238, 0.48775104833370114, 0.09710880600308981, 0.18384462591039502, 0.3864489075259324, 0.35532994923857864, 0.12690355329949238, 0.052968439638048995, 0.24873096446700507, 0.5102626351798719, 0.5208563231074818, 0.9746192893401014, 0.03045685279187817, 0.033767380269256235, 0.7353785036415801, 0.3489295961156477, 0.5532994923857867, 0.3376738026925623, 0.00375193114102847, 0.31781063782829394, 0.12756565879496798, 0.3502538071065989, 0.48775104833370114, 0.033767380269256235, 0.8274111675126903, 0.23835797837122047, 0.21628779518870006, 0.08254248510262634, 0.8254248510262634, 0.03045685279187817, 0.07128669167954092, 0.7360406091370558, 0.38578680203045684, 0.06003089825645552, 0.41116751269035534, 0.07879055396159788, 0.44162436548223344, 0.7203707790774663, 0.8563231074817921, 0.09644670050761421, 0.1986316486426837, 0.38843522401235925, 0.21187375855219598, 0.2081218274111675, 0.4325755903774001, 0.8730964467005076, 0.3487088942838225, 0.5076142131979695, 0.34142573383359076, 0.7969543147208121, 0.3089825645552858, 0.21319796954314718, 0.5552858088722136, 0.34142573383359076, 0.2780843080997572, 0.5552858088722136, 0.18980357536967554, 0.781725888324873, 0.7724564113882144, 0.9035532994923856, 0.17258883248730963, 0.048775104833370116, 0.375193114102847, 0.21187375855219598, 0.18384462591039502, 0.6446700507614214, 0.11035091591260207, 0.09644670050761421, 0.06003089825645552, 0.45464577355992053, 0.5076142131979695, 0.3502538071065989, 0.36393732067976164, 0.35532994923857864, 0.4414036636504083, 0.2030456852791878, 0.033767380269256235], \"xaxis\": \"x\", \"y\": [0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8554558753967285, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 2.2721903324127197, 2.500852584838867, 0.8553754091262817, 0.8553754091262817, 2.912498950958252, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 1.7137593030929565, 0.8654075264930725, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 1.998752474784851, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553962111473083, 1.665574312210083, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 1.8096202611923218, 0.8553754091262817, 0.8715038299560547, 0.8553963303565979, 0.8553754091262817, 2.131816864013672, 0.8553754091262817, 0.8650209307670593, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8653419613838196, 0.8553754091262817, 0.8553754091262817, 2.4828805923461914, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 2.5383050441741943, 1.8262050151824951, 0.8554547429084778, 0.8553754091262817, 1.9734184741973877, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 2.6598119735717773, 0.8553754091262817, 0.8554551601409912, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8649894595146179, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 2.513949394226074, 3.1611528396606445, 2.9752774238586426, 0.8553754091262817, 2.7636935710906982, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553963303565979, 0.8553754091262817, 0.8553754091262817, 0.8650104403495789, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 2.9170985221862793, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 2.368889093399048, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553968071937561, 0.8553754091262817, 0.8553754091262817, 1.8789384365081787, 0.8554549813270569, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 1.9948424100875854, 0.8553754091262817, 1.911723017692566, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.865374743938446, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8554537892341614, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8554031252861023, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 2.1829721927642822, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 1.7617793083190918, 0.8553754091262817, 0.8553754091262817, 0.8654404282569885, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8650843501091003, 2.202298641204834, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8554542660713196, 0.8553754091262817, 2.0571630001068115, 3.1328747272491455, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 2.9718809127807617, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 2.6301157474517822, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 2.046367883682251, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 2.709810733795166, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 2.51377272605896, 0.8553963303565979, 0.8553754091262817, 3.1412620544433594, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 1.6172422170639038, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553963303565979, 2.8999693393707275, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8554547429084778, 1.9734184741973877, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 3.0954668521881104, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 2.9550132751464844, 0.8553754091262817, 0.8554561138153076, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8554542660713196, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 2.8796472549438477, 0.8553754091262817, 0.8553754091262817, 0.8651801943778992, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 1.8262050151824951, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 1.6097148656845093, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 2.914799451828003, 0.8553754091262817, 0.8553754091262817, 0.8553962111473083, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553963303565979, 0.8553754091262817, 0.8553754091262817, 0.8554537892341614, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8649999499320984, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8553754091262817, 0.8650209307670593, 1.6097164154052734], \"yaxis\": \"y\"}, {\"hovertemplate\": \"variable=RFC<br>r=%{x}<br>value=%{y}<extra></extra>\", \"legendgroup\": \"RFC\", \"marker\": {\"color\": \"#00cc96\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"RFC\", \"showlegend\": true, \"type\": \"scattergl\", \"x\": [0.12690355329949238, 0.7391304347826085, 0.22886779960273668, 0.9847715736040608, 0.49746192893401014, 0.2182741116751269, 0.4016773339218715, 0.8695652173913043, 0.06598984771573603, 0.03089825645552858, 0.45464577355992053, 0.13242109909512248, 0.42639593908629436, 0.1500772456411388, 0.004414036636504083, 0.09710880600308981, 0.21761200617965126, 0.5685279187817259, 0.033767380269256235, 0.056278967115427056, 0.7316265725005517, 0.598984771573604, 0.481129993378945, 0.5473405429265062, 0.21761200617965126, 0.24762745530787905, 0.9847715736040608, 0.4619289340101523, 0.5473405429265062, 0.3575369675568307, 0.07945265945707349, 0.07879055396159788, 0.47208121827411165, 0.8474950342087839, 0.4016773339218715, 0.13705583756345177, 0.7541381593467225, 0.1986316486426837, 0.3502538071065989, 0.02626351798719929, 0.12381372765393953, 0.09644670050761421, 0.11917898918561023, 0.8783932906643125, 0.11255793423085411, 0.04502317369234164, 0.7856985212977267, 0.47208121827411165, 0.15758110792319577, 0.10505407194879716, 0.1765614654601633, 0.26484219819024496, 0.7360406091370558, 0.7241227102184948, 0.01125579342308541, 0.8730964467005076, 0.1500772456411388, 0.06378282939748399, 0.020304568527918777, 0.5890531891414698, 0.4314720812182741, 0.044140366365040824, 0.08629441624365482, 0.4089604943721033, 0.2138600750386228, 0.7166188479364378, 0.36393732067976164, 0.28139483557713524, 0.18009269476936657, 0.49746192893401014, 0.12756565879496798, 0.09379827852571175, 0.10505407194879716, 0.16883690134628115, 0.02626351798719929, 0.22070183182520414, 0.06003089825645552, 0.6797616420216287, 0.4923857868020304, 0.14632531450011033, 0.2030456852791878, 0.07128669167954092, 0.4619289340101523, 0.2639593908629441, 0.18097550209666738, 0.416243654822335, 0.11675126903553298, 0.964467005076142, 0.2030456852791878, 0.4414036636504083, 0.16508497020525267, 1.0, 0.05076142131979695, 0.7724564113882144, 0.07945265945707349, 0.015228426395939085, 0.22511586846170822, 0.9340101522842639, 0.0375193114102847, 0.10593687927609799, 0.12690355329949238, 0.14213197969543148, 0.11630986537188258, 0.0812182741116751, 0.12381372765393953, 0.8426395939086294, 0.5440300154491282, 0.2401235930258221, 0.3096446700507614, 0.04502317369234164, 0.38578680203045684, 0.026484219819024497, 0.2030456852791878, 0.11167512690355329, 0.2780843080997572, 0.06598984771573603, 0.598984771573604, 0.2182741116751269, 0.08254248510262634, 0.4652394614875303, 0.16331935555065105, 0.3310527477378062, 0.17258883248730963, 0.3264180092694769, 0.09710880600308981, 0.13242109909512248, 0.1013021408077687, 0.013242109909512249, 0.964467005076142, 0.09710880600308981, 0.3959390862944162, 0.750386228205694, 0.0375193114102847, 0.1388214522180534, 0.6294416243654821, 0.1765614654601633, 0.4771573604060913, 0.375193114102847, 0.2081218274111675, 0.19885235047450892, 0.18009269476936657, 0.7208121827411167, 0.7391304347826085, 0.09379827852571175, 0.10659898477157359, 0.851909070845288, 0.02251158684617082, 0.26925623482674904, 0.5605826528360185, 0.37144118296181855, 0.13506952107702494, 0.0812182741116751, 0.048775104833370116, 0.949238578680203, 0.1878172588832487, 0.3264180092694769, 0.5440300154491282, 0.6565879496799822, 0.05076142131979695, 0.3564334583977047, 0.16331935555065105, 0.42021628779518866, 0.2182741116751269, 0.22511586846170822, 0.365482233502538, 0.05076142131979695, 0.45464577355992053, 0.3096446700507614, 0.322224674464798, 0.9187817258883249, 0.057382476274553076, 0.6903553299492385, 0.6115647759876407, 0.6903553299492385, 0.42374751710439196, 0.42816155374089604, 0.01765614654601633, 0.375193114102847, 0.42374751710439196, 0.11167512690355329, 0.12690355329949238, 0.7208121827411167, 0.4771573604060913, 0.34142573383359076, 0.38071065989847713, 0.35268152725667623, 0.026484219819024497, 0.06091370558375634, 0.46700507614213194, 0.13197969543147206, 0.6267932023835797, 0.1015228426395939, 0.1538291767821673, 0.3076583535643346, 0.6142131979695431, 0.2513793864489075, 0.7592143014787022, 0.6003089825645552, 0.06753476053851247, 0.17214742882365922, 0.15890531891414697, 0.03553299492385787, 0.28139483557713524, 0.2030456852791878, 0.2295299050982123, 1.0, 0.11630986537188258, 0.2030456852791878, 0.8386669609357756, 0.182741116751269, 0.9949238578680203, 0.23835797837122047, 0.5025380710659898, 0.29640256014124916, 0.6598984771573604, 0.39770470094901783, 0.1013021408077687, 0.30390642242330607, 0.22511586846170822, 0.12756565879496798, 0.06753476053851247, 0.17214742882365922, 0.09269476936658573, 0.005076142131979694, 0.1979695431472081, 0.7512690355329948, 0.766497461928934, 0.17258883248730963, 0.10659898477157359, 0.6135510924740675, 0.31516221584639154, 0.3972632972853674, 0.5532994923857867, 0.1500772456411388, 0.17214742882365922, 0.41116751269035534, 0.6142131979695431, 0.851909070845288, 0.31979695431472077, 0.38843522401235925, 0.8883248730964466, 0.3248730964467004, 0.11255793423085411, 0.30390642242330607, 0.375193114102847, 0.17258883248730963, 0.21319796954314718, 0.015228426395939085, 0.08629441624365482, 0.06003089825645552, 0.47649525491061573, 0.8474950342087839, 0.044140366365040824, 0.7166188479364378, 0.9695431472081217, 0.7969543147208121, 0.322224674464798, 0.026484219819024497, 0.38402118737585517, 0.4619289340101523, 0.4016773339218715, 0.056278967115427056, 0.06621054954756124, 0.4414036636504083, 0.35268152725667623, 0.1388214522180534, 0.12690355329949238, 0.18384462591039502, 0.1875965570514235, 0.19134848819245198, 0.7062458618406532, 0.10593687927609799, 0.4678878834694328, 0.7724564113882144, 0.16883690134628115, 0.2791878172588832, 0.38402118737585517, 0.44162436548223344, 0.27411167512690354, 0.45685279187817257, 0.005076142131979694, 0.6142131979695431, 0.06003089825645552, 0.11476495254910615, 0.19421761200617962, 0.45685279187817257, 0.11255793423085411, 0.40101522842639586, 0.25888324873096447, 0.8578680203045684, 0.25888324873096447, 0.13683513573162656, 0.19510041933348046, 0.34142573383359076, 0.1613330390642242, 0.0913705583756345, 0.06621054954756124, 0.0375193114102847, 0.3564334583977047, 0.18009269476936657, 0.02626351798719929, 0.416243654822335, 0.15758110792319577, 0.18009269476936657, 0.0812182741116751, 0.7512690355329948, 0.416243654822335, 0.24873096446700507, 0.056278967115427056, 0.0812182741116751, 0.4414036636504083, 0.2401235930258221, 0.6665195321121165, 0.21628779518870006, 0.8426395939086294, 0.21187375855219598, 0.07062458618406532, 0.964467005076142, 0.41491944383138374, 0.7327300816596777, 0.11630986537188258, 0.11167512690355329, 0.020304568527918777, 0.13242109909512248, 0.36393732067976164, 0.7278746413595233, 0.07614213197969542, 0.004414036636504083, 0.29640256014124916, 0.4016773339218715, 0.7203707790774663, 0.42816155374089604, 0.22842639593908629, 0.14213197969543148, 0.0812182741116751, 0.4771573604060913, 0.2560141249172368, 0.4193334804678878, 0.0, 0.45398366806444485, 0.24365482233502536, 0.8254248510262634, 0.36768925182079004, 0.17258883248730963, 0.34142573383359076, 0.4652394614875303, 0.8386669609357756, 0.1613330390642242, 0.07062458618406532, 0.09379827852571175, 0.09379827852571175, 0.3864489075259324, 0.07879055396159788, 0.1388214522180534, 0.37894504524387546, 0.36018538953873314, 0.6446700507614214, 0.182741116751269, 0.640035312293092, 0.3707790774663429, 0.39770470094901783, 0.851909070845288, 0.03553299492385787, 0.3350253807106599, 0.22842639593908629, 0.1500772456411388, 0.12359302582211432, 0.6532774222026042, 0.6790995365261531, 0.46700507614213194, 0.7391304347826085, 0.06003089825645552, 0.21628779518870006, 0.05076142131979695, 0.33016994041050535, 0.3489295961156477, 0.057382476274553076, 0.22070183182520414, 0.4923857868020304, 0.4990068417567865, 0.21187375855219598, 0.24277201500772455, 0.17258883248730963, 0.45398366806444485, 0.4771573604060913, 0.11255793423085411, 0.8274111675126903, 0.0750386228205694, 0.21628779518870006, 0.7278746413595233, 0.12690355329949238, 0.48775104833370114, 0.09710880600308981, 0.18384462591039502, 0.3864489075259324, 0.35532994923857864, 0.12690355329949238, 0.052968439638048995, 0.24873096446700507, 0.5102626351798719, 0.5208563231074818, 0.9746192893401014, 0.03045685279187817, 0.033767380269256235, 0.7353785036415801, 0.3489295961156477, 0.5532994923857867, 0.3376738026925623, 0.00375193114102847, 0.31781063782829394, 0.12756565879496798, 0.3502538071065989, 0.48775104833370114, 0.033767380269256235, 0.8274111675126903, 0.23835797837122047, 0.21628779518870006, 0.08254248510262634, 0.8254248510262634, 0.03045685279187817, 0.07128669167954092, 0.7360406091370558, 0.38578680203045684, 0.06003089825645552, 0.41116751269035534, 0.07879055396159788, 0.44162436548223344, 0.7203707790774663, 0.8563231074817921, 0.09644670050761421, 0.1986316486426837, 0.38843522401235925, 0.21187375855219598, 0.2081218274111675, 0.4325755903774001, 0.8730964467005076, 0.3487088942838225, 0.5076142131979695, 0.34142573383359076, 0.7969543147208121, 0.3089825645552858, 0.21319796954314718, 0.5552858088722136, 0.34142573383359076, 0.2780843080997572, 0.5552858088722136, 0.18980357536967554, 0.781725888324873, 0.7724564113882144, 0.9035532994923856, 0.17258883248730963, 0.048775104833370116, 0.375193114102847, 0.21187375855219598, 0.18384462591039502, 0.6446700507614214, 0.11035091591260207, 0.09644670050761421, 0.06003089825645552, 0.45464577355992053, 0.5076142131979695, 0.3502538071065989, 0.36393732067976164, 0.35532994923857864, 0.4414036636504083, 0.2030456852791878, 0.033767380269256235], \"xaxis\": \"x\", \"y\": [0.6058129266666671, 0.9265517904761912, 1.0654115833333335, 0.6145601016666659, 0.6688154000000011, 0.6118688999999999, 0.9450864866666677, 0.6731523333333332, 0.8722395500000001, 0.7925087400000004, 0.817865999999999, 0.9401625416666675, 0.7182502999999995, 1.1093323333333354, 0.7535927383333327, 1.156805733333332, 1.1196458428571434, 0.6214094833333339, 1.2945815833333334, 1.2336880000000003, 1.0583391761904755, 0.6218037749999988, 0.8187005249999992, 0.7804345, 1.0646625333333324, 1.2577784000000007, 0.5258708999999996, 0.7149085283333336, 0.8203419266666664, 0.9064743383333336, 0.9382633999999986, 1.3015287000000009, 0.7096895533333333, 0.7770466099999996, 0.7766859899999994, 0.7108158616666675, 0.8417353000000005, 0.9492596499999992, 0.6865952533333334, 1.0933358333333318, 1.2401346666666646, 2.5079229666666696, 0.7821662466666668, 0.7198264250000002, 1.3035249666666673, 3.645047750000003, 0.7862216916666663, 0.7096895533333333, 1.5156119666666676, 1.5313479999999984, 0.7641212000000003, 0.9531389666666672, 0.6246905499999991, 1.1129959333333341, 1.293056449999999, 0.6261895500000002, 1.053290050000001, 4.031295000000003, 0.8305352000000005, 1.1370883166666674, 0.6187788999999991, 1.153605666666667, 2.7331356000000038, 1.075121666666668, 1.2578680666666655, 1.0593756333333324, 1.1278740500000006, 1.320838541666668, 1.2554359999999987, 0.5499560000000002, 1.5228249333333332, 1.6027640166666657, 1.102753666666667, 1.4708350000000003, 1.0397887904761915, 0.9391063333333327, 4.043130250000004, 0.7850656300000004, 0.620081724999999, 1.2442524333333322, 0.6095626999999992, 3.8346528833333315, 0.7149085283333336, 0.6937817000000002, 1.0904434999999995, 0.5890298966666669, 2.3763828833333314, 0.6234114500000006, 0.8324357333333342, 0.7780197350000002, 1.503885033333334, 0.578369703333334, 0.8309422166666668, 0.8239928249999996, 0.8917519333333337, 2.61418113333333, 1.2578597999999996, 0.5965164000000007, 1.527686666666666, 1.6451752166666649, 1.121331126666667, 0.872401686666667, 1.1039701999999991, 2.2038123142857136, 1.305305400000002, 0.626070530000001, 1.0820594999999995, 1.1201579999999995, 0.6160382857142871, 1.235279966666669, 0.5885207533333338, 2.917715333333333, 0.7698409000000009, 0.8734327666666658, 0.9073294399999995, 0.5745014666666655, 0.5918485999999998, 0.8304386333333335, 1.1010576190476176, 1.1325334166666647, 0.7617531466666679, 0.9527109916666664, 0.5778799749999998, 1.2531129976190492, 0.7980205266666672, 1.3883646333333328, 2.1614881499999994, 3.052083083333332, 0.5856000000000003, 1.9568187333333311, 0.7198140799999999, 0.5702841535714281, 1.527686666666666, 1.1065672000000009, 0.6225667349999995, 1.097766500000001, 0.6231608400000005, 0.8154731500000012, 0.8312960000000007, 1.0207272000000005, 1.3173693999999991, 0.5950757699999992, 0.9265517904761912, 1.5305289999999991, 0.676157166666667, 0.8127658602380952, 3.644430583333328, 0.7727551750000009, 0.781621916666667, 0.7760782883333328, 1.307132966666668, 2.60729316666667, 1.0453433333333348, 0.5959304000000006, 0.6083842333333324, 1.3135709999999994, 1.0820594999999995, 1.0850400333333317, 0.5742480733333326, 0.8886894299999989, 0.7617531466666679, 1.075929266666668, 0.5811629666666672, 0.9515847183333341, 0.7208824900000002, 0.6017410750000005, 0.7798373409523804, 0.6160382857142871, 0.8143527666666672, 0.5962787933333331, 1.1543090833333323, 1.1377691333333335, 1.1376647333333347, 0.6241215333333335, 0.832369333333333, 0.8164441666666675, 1.0981839999999996, 1.0499554333333336, 0.6264867500000003, 1.8145049666666662, 0.8730989916666667, 0.6245264083333324, 0.5895508899999994, 1.1265074000000006, 0.6869511533333323, 1.1257646049999988, 0.8895456900000004, 0.671392843333334, 0.6674191971428566, 1.9387523333333327, 0.7829601999999996, 2.1091922333333315, 1.6000406571428583, 1.0708925333333346, 0.5921883316666673, 1.0672495666666677, 0.8247220399999996, 0.7825152999999989, 1.2343078000000003, 0.9009021133333321, 0.942453146666667, 0.8311250666666676, 1.2589702666666653, 0.5811943250000003, 0.7716641999999996, 0.578369703333334, 1.0516167999999992, 0.8729272500000007, 0.7779711349999994, 0.7138054499999998, 0.578369703333334, 0.7051882999999995, 0.6123921999999999, 1.070854999999999, 0.5943239399999993, 1.1295136666666654, 1.0835076583333336, 1.3184001249999986, 1.2578597999999996, 1.6006233999999997, 3.976615999999999, 0.9456816825000003, 0.9393688333333337, 0.8295013999999997, 0.6782659933333339, 0.5960380833333333, 0.6256554333333326, 0.8327010166666668, 1.9366663333333343, 0.7827008999999994, 1.316333999999998, 0.8951747499999992, 0.6211833600000005, 1.6010776571428567, 0.9456816825000003, 0.7189386999999995, 0.6220351666666653, 0.7383405666666669, 0.7212110016666664, 0.8157941750000016, 0.6259462499999995, 0.6166686190476198, 1.2391794166666643, 1.2561335333333323, 0.7758760749999999, 0.8717873566666665, 0.7157228750000002, 2.2077505833333353, 3.2908274, 1.099053249999998, 1.0787679999999988, 0.7770466099999996, 0.7554341416666673, 1.132782250000001, 0.5835703000000002, 0.6254319149999995, 0.7748686499999997, 3.0537295833333333, 0.9043103333333338, 0.6765485999999992, 0.8158706366666683, 1.2991140333333342, 2.924791633333337, 0.8174115000000003, 1.1270031000000014, 1.6006489999999989, 1.845101213333333, 0.9434375666666678, 0.7168776500000001, 1.2542634999999982, 0.7858785142857146, 1.4615720533333325, 0.8180671999999996, 0.7859281804761906, 1.254387499999999, 0.6152565500000001, 0.948152983333334, 0.6833901500000001, 0.7202139699999995, 0.7156926750000007, 0.6000972499999992, 0.6220351666666653, 1.5312450333333334, 0.9906676383333323, 1.0096796999999993, 0.6781110999999999, 1.6252184833333325, 0.6184661416666661, 1.3213884666666673, 0.597178869999999, 1.2583162500000014, 1.153775499999999, 1.0644523333333347, 1.3090627500000007, 1.5881335000000023, 0.710074978095238, 0.9372569999999986, 1.0953357499999985, 1.1925244000000013, 1.1157638016666673, 1.598430483333335, 0.5890298966666669, 1.2519545000000007, 1.1157638016666673, 0.6041499599999995, 0.5960380833333333, 0.6187317999999994, 0.6136930100000002, 4.045095116666671, 0.5744861666666657, 0.7780197350000002, 1.1201579999999995, 0.8231963666666665, 0.6898173333333346, 0.5965081999999993, 1.0343014166666653, 3.0276020166666693, 0.5856000000000003, 0.8719689083333338, 0.7866266654761902, 1.1039701999999991, 0.6057926133333331, 0.6002558249999993, 0.8949748749999987, 0.9970522535714277, 0.9985699133333334, 0.7086632880952378, 0.7912421300000001, 1.124709166666666, 0.8892886, 1.059529173333332, 0.7775024100000003, 0.7179532000000002, 0.872401686666667, 2.2038123142857136, 0.7005480683333335, 0.7713992666666668, 0.9077034716666668, 0.7535927383333327, 1.0771035000000013, 0.6846994583333331, 0.8230394583333335, 0.9912268916666668, 1.1146912333333334, 1.2427637142857135, 1.0787955, 0.7779711349999994, 1.5204346333333345, 0.7564814599999999, 1.0488383499999998, 1.6027640166666657, 1.1291659333333335, 3.4539733238095254, 1.0502892500000014, 0.6731931295238092, 1.0982229999999984, 0.6229142100000004, 0.6776963999999999, 0.7853025999999997, 0.9497377116666657, 1.0743400000000012, 0.8127658602380952, 0.573210731666667, 0.7212306600000002, 0.6810213166666668, 1.053290050000001, 0.7599390066666661, 0.7849445249999997, 1.138229999999999, 0.6674191971428566, 0.9127755700000015, 3.6085940000000067, 0.7708093000000001, 0.8705002249999994, 1.071592416666666, 1.208501566666665, 0.7564992416666669, 0.6060355000000016, 0.6108144366666657, 1.0812480000000015, 0.9510090166666672, 0.7714011999999993, 0.8717873566666665, 1.132162199999998, 0.6231608400000005, 1.3035249666666673, 0.5962536749999998, 2.949945483333334, 0.8086613516666659, 0.9985699133333334, 0.8339493999999994, 1.0803600000000009, 0.7580594250000006, 1.2654167499999998, 1.074287, 0.5880689499999994, 1.121331126666667, 0.9344977, 0.5851931333333344, 1.0801880000000017, 0.8196514400000011, 0.5803592999999997, 0.7051280733333333, 4.043722583333337, 0.9556412428571442, 1.208501566666665, 0.6211833600000005, 1.3099432500000008, 1.596050499999999, 0.9527617499999992, 1.0500253333333343, 0.7211677500000004, 1.1330052833333328, 3.6474082499999962, 0.5962536749999998, 0.9049432199999988, 1.020426883333332, 1.5314410833333343, 0.7853557000000003, 0.6700823933333341, 1.0451925166666651, 0.5950438666666659, 0.5885207533333338, 1.602909999999998, 0.7189386999999995, 1.2357432999999989, 0.7175073999999997, 1.1273098333333325, 0.647220656666668, 0.6050415999999994, 0.8101564166666665, 0.776451164999999, 0.9038417000000006, 0.8731829166666658, 0.8430268999999995, 0.5971802466666665, 0.7753302899999999, 0.6202150209523808, 1.3090627500000007, 0.6254319149999995, 0.7739144999999997, 0.8305707000000006, 1.138019199999998, 1.1265074000000006, 0.9530506999999997, 1.0829854999999993, 0.7669323000000006, 0.6253817166666661, 0.7859281804761906, 0.6251465799999997, 1.3907183816666662, 1.602053233333331, 1.0751815000000002, 0.7746462000000008, 1.060822999999999, 0.6229142100000004, 0.7585618999999996, 0.6050415999999994, 1.0462959999999994, 0.6824032999999998, 0.7805580966666661, 0.7211677500000004, 1.0748578333333323, 0.6171853933333324, 0.7787504999999991, 0.8324357333333342, 4.043722583333337], \"yaxis\": \"y\"}, {\"hovertemplate\": \"variable=GBC<br>r=%{x}<br>value=%{y}<extra></extra>\", \"legendgroup\": \"GBC\", \"marker\": {\"color\": \"#ab63fa\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"GBC\", \"showlegend\": true, \"type\": \"scattergl\", \"x\": [0.12690355329949238, 0.7391304347826085, 0.22886779960273668, 0.9847715736040608, 0.49746192893401014, 0.2182741116751269, 0.4016773339218715, 0.8695652173913043, 0.06598984771573603, 0.03089825645552858, 0.45464577355992053, 0.13242109909512248, 0.42639593908629436, 0.1500772456411388, 0.004414036636504083, 0.09710880600308981, 0.21761200617965126, 0.5685279187817259, 0.033767380269256235, 0.056278967115427056, 0.7316265725005517, 0.598984771573604, 0.481129993378945, 0.5473405429265062, 0.21761200617965126, 0.24762745530787905, 0.9847715736040608, 0.4619289340101523, 0.5473405429265062, 0.3575369675568307, 0.07945265945707349, 0.07879055396159788, 0.47208121827411165, 0.8474950342087839, 0.4016773339218715, 0.13705583756345177, 0.7541381593467225, 0.1986316486426837, 0.3502538071065989, 0.02626351798719929, 0.12381372765393953, 0.09644670050761421, 0.11917898918561023, 0.8783932906643125, 0.11255793423085411, 0.04502317369234164, 0.7856985212977267, 0.47208121827411165, 0.15758110792319577, 0.10505407194879716, 0.1765614654601633, 0.26484219819024496, 0.7360406091370558, 0.7241227102184948, 0.01125579342308541, 0.8730964467005076, 0.1500772456411388, 0.06378282939748399, 0.020304568527918777, 0.5890531891414698, 0.4314720812182741, 0.044140366365040824, 0.08629441624365482, 0.4089604943721033, 0.2138600750386228, 0.7166188479364378, 0.36393732067976164, 0.28139483557713524, 0.18009269476936657, 0.49746192893401014, 0.12756565879496798, 0.09379827852571175, 0.10505407194879716, 0.16883690134628115, 0.02626351798719929, 0.22070183182520414, 0.06003089825645552, 0.6797616420216287, 0.4923857868020304, 0.14632531450011033, 0.2030456852791878, 0.07128669167954092, 0.4619289340101523, 0.2639593908629441, 0.18097550209666738, 0.416243654822335, 0.11675126903553298, 0.964467005076142, 0.2030456852791878, 0.4414036636504083, 0.16508497020525267, 1.0, 0.05076142131979695, 0.7724564113882144, 0.07945265945707349, 0.015228426395939085, 0.22511586846170822, 0.9340101522842639, 0.0375193114102847, 0.10593687927609799, 0.12690355329949238, 0.14213197969543148, 0.11630986537188258, 0.0812182741116751, 0.12381372765393953, 0.8426395939086294, 0.5440300154491282, 0.2401235930258221, 0.3096446700507614, 0.04502317369234164, 0.38578680203045684, 0.026484219819024497, 0.2030456852791878, 0.11167512690355329, 0.2780843080997572, 0.06598984771573603, 0.598984771573604, 0.2182741116751269, 0.08254248510262634, 0.4652394614875303, 0.16331935555065105, 0.3310527477378062, 0.17258883248730963, 0.3264180092694769, 0.09710880600308981, 0.13242109909512248, 0.1013021408077687, 0.013242109909512249, 0.964467005076142, 0.09710880600308981, 0.3959390862944162, 0.750386228205694, 0.0375193114102847, 0.1388214522180534, 0.6294416243654821, 0.1765614654601633, 0.4771573604060913, 0.375193114102847, 0.2081218274111675, 0.19885235047450892, 0.18009269476936657, 0.7208121827411167, 0.7391304347826085, 0.09379827852571175, 0.10659898477157359, 0.851909070845288, 0.02251158684617082, 0.26925623482674904, 0.5605826528360185, 0.37144118296181855, 0.13506952107702494, 0.0812182741116751, 0.048775104833370116, 0.949238578680203, 0.1878172588832487, 0.3264180092694769, 0.5440300154491282, 0.6565879496799822, 0.05076142131979695, 0.3564334583977047, 0.16331935555065105, 0.42021628779518866, 0.2182741116751269, 0.22511586846170822, 0.365482233502538, 0.05076142131979695, 0.45464577355992053, 0.3096446700507614, 0.322224674464798, 0.9187817258883249, 0.057382476274553076, 0.6903553299492385, 0.6115647759876407, 0.6903553299492385, 0.42374751710439196, 0.42816155374089604, 0.01765614654601633, 0.375193114102847, 0.42374751710439196, 0.11167512690355329, 0.12690355329949238, 0.7208121827411167, 0.4771573604060913, 0.34142573383359076, 0.38071065989847713, 0.35268152725667623, 0.026484219819024497, 0.06091370558375634, 0.46700507614213194, 0.13197969543147206, 0.6267932023835797, 0.1015228426395939, 0.1538291767821673, 0.3076583535643346, 0.6142131979695431, 0.2513793864489075, 0.7592143014787022, 0.6003089825645552, 0.06753476053851247, 0.17214742882365922, 0.15890531891414697, 0.03553299492385787, 0.28139483557713524, 0.2030456852791878, 0.2295299050982123, 1.0, 0.11630986537188258, 0.2030456852791878, 0.8386669609357756, 0.182741116751269, 0.9949238578680203, 0.23835797837122047, 0.5025380710659898, 0.29640256014124916, 0.6598984771573604, 0.39770470094901783, 0.1013021408077687, 0.30390642242330607, 0.22511586846170822, 0.12756565879496798, 0.06753476053851247, 0.17214742882365922, 0.09269476936658573, 0.005076142131979694, 0.1979695431472081, 0.7512690355329948, 0.766497461928934, 0.17258883248730963, 0.10659898477157359, 0.6135510924740675, 0.31516221584639154, 0.3972632972853674, 0.5532994923857867, 0.1500772456411388, 0.17214742882365922, 0.41116751269035534, 0.6142131979695431, 0.851909070845288, 0.31979695431472077, 0.38843522401235925, 0.8883248730964466, 0.3248730964467004, 0.11255793423085411, 0.30390642242330607, 0.375193114102847, 0.17258883248730963, 0.21319796954314718, 0.015228426395939085, 0.08629441624365482, 0.06003089825645552, 0.47649525491061573, 0.8474950342087839, 0.044140366365040824, 0.7166188479364378, 0.9695431472081217, 0.7969543147208121, 0.322224674464798, 0.026484219819024497, 0.38402118737585517, 0.4619289340101523, 0.4016773339218715, 0.056278967115427056, 0.06621054954756124, 0.4414036636504083, 0.35268152725667623, 0.1388214522180534, 0.12690355329949238, 0.18384462591039502, 0.1875965570514235, 0.19134848819245198, 0.7062458618406532, 0.10593687927609799, 0.4678878834694328, 0.7724564113882144, 0.16883690134628115, 0.2791878172588832, 0.38402118737585517, 0.44162436548223344, 0.27411167512690354, 0.45685279187817257, 0.005076142131979694, 0.6142131979695431, 0.06003089825645552, 0.11476495254910615, 0.19421761200617962, 0.45685279187817257, 0.11255793423085411, 0.40101522842639586, 0.25888324873096447, 0.8578680203045684, 0.25888324873096447, 0.13683513573162656, 0.19510041933348046, 0.34142573383359076, 0.1613330390642242, 0.0913705583756345, 0.06621054954756124, 0.0375193114102847, 0.3564334583977047, 0.18009269476936657, 0.02626351798719929, 0.416243654822335, 0.15758110792319577, 0.18009269476936657, 0.0812182741116751, 0.7512690355329948, 0.416243654822335, 0.24873096446700507, 0.056278967115427056, 0.0812182741116751, 0.4414036636504083, 0.2401235930258221, 0.6665195321121165, 0.21628779518870006, 0.8426395939086294, 0.21187375855219598, 0.07062458618406532, 0.964467005076142, 0.41491944383138374, 0.7327300816596777, 0.11630986537188258, 0.11167512690355329, 0.020304568527918777, 0.13242109909512248, 0.36393732067976164, 0.7278746413595233, 0.07614213197969542, 0.004414036636504083, 0.29640256014124916, 0.4016773339218715, 0.7203707790774663, 0.42816155374089604, 0.22842639593908629, 0.14213197969543148, 0.0812182741116751, 0.4771573604060913, 0.2560141249172368, 0.4193334804678878, 0.0, 0.45398366806444485, 0.24365482233502536, 0.8254248510262634, 0.36768925182079004, 0.17258883248730963, 0.34142573383359076, 0.4652394614875303, 0.8386669609357756, 0.1613330390642242, 0.07062458618406532, 0.09379827852571175, 0.09379827852571175, 0.3864489075259324, 0.07879055396159788, 0.1388214522180534, 0.37894504524387546, 0.36018538953873314, 0.6446700507614214, 0.182741116751269, 0.640035312293092, 0.3707790774663429, 0.39770470094901783, 0.851909070845288, 0.03553299492385787, 0.3350253807106599, 0.22842639593908629, 0.1500772456411388, 0.12359302582211432, 0.6532774222026042, 0.6790995365261531, 0.46700507614213194, 0.7391304347826085, 0.06003089825645552, 0.21628779518870006, 0.05076142131979695, 0.33016994041050535, 0.3489295961156477, 0.057382476274553076, 0.22070183182520414, 0.4923857868020304, 0.4990068417567865, 0.21187375855219598, 0.24277201500772455, 0.17258883248730963, 0.45398366806444485, 0.4771573604060913, 0.11255793423085411, 0.8274111675126903, 0.0750386228205694, 0.21628779518870006, 0.7278746413595233, 0.12690355329949238, 0.48775104833370114, 0.09710880600308981, 0.18384462591039502, 0.3864489075259324, 0.35532994923857864, 0.12690355329949238, 0.052968439638048995, 0.24873096446700507, 0.5102626351798719, 0.5208563231074818, 0.9746192893401014, 0.03045685279187817, 0.033767380269256235, 0.7353785036415801, 0.3489295961156477, 0.5532994923857867, 0.3376738026925623, 0.00375193114102847, 0.31781063782829394, 0.12756565879496798, 0.3502538071065989, 0.48775104833370114, 0.033767380269256235, 0.8274111675126903, 0.23835797837122047, 0.21628779518870006, 0.08254248510262634, 0.8254248510262634, 0.03045685279187817, 0.07128669167954092, 0.7360406091370558, 0.38578680203045684, 0.06003089825645552, 0.41116751269035534, 0.07879055396159788, 0.44162436548223344, 0.7203707790774663, 0.8563231074817921, 0.09644670050761421, 0.1986316486426837, 0.38843522401235925, 0.21187375855219598, 0.2081218274111675, 0.4325755903774001, 0.8730964467005076, 0.3487088942838225, 0.5076142131979695, 0.34142573383359076, 0.7969543147208121, 0.3089825645552858, 0.21319796954314718, 0.5552858088722136, 0.34142573383359076, 0.2780843080997572, 0.5552858088722136, 0.18980357536967554, 0.781725888324873, 0.7724564113882144, 0.9035532994923856, 0.17258883248730963, 0.048775104833370116, 0.375193114102847, 0.21187375855219598, 0.18384462591039502, 0.6446700507614214, 0.11035091591260207, 0.09644670050761421, 0.06003089825645552, 0.45464577355992053, 0.5076142131979695, 0.3502538071065989, 0.36393732067976164, 0.35532994923857864, 0.4414036636504083, 0.2030456852791878, 0.033767380269256235], \"xaxis\": \"x\", \"y\": [0.6155461847793241, 0.9228157898814048, 1.0645647236053224, 0.6190085231800094, 0.6430318142201775, 0.6233399000438051, 0.8882236129856965, 0.728138391448806, 0.8760935661450563, 0.8102835365186419, 0.8129897046356815, 0.9278203266142478, 0.6653344675738562, 1.128965568031659, 0.7731533805104698, 1.133549538614931, 1.1226152958920288, 0.6242516933354635, 1.2782437811032332, 1.2287670929496604, 1.0572209630734675, 0.6242516933354635, 0.8129897046356815, 0.7730818261925849, 1.0645647236053224, 1.2027013315295105, 0.5784202469726393, 0.6696920977492333, 0.8129897046356815, 0.8392130792670346, 0.9256123096041577, 1.2782437811032332, 0.6696920977492333, 0.7676881557232097, 0.7721700329009266, 0.7217572006155145, 0.8938166108262955, 0.9293178329944263, 0.68219045576689, 1.119824043671054, 1.2309751099597506, 2.360246451474126, 0.6701516690943575, 0.7266985970795709, 1.2804517981133234, 3.591573639021755, 0.7676881557232097, 0.6696920977492333, 1.4729673042948777, 1.4886589551276375, 0.7782978617196726, 0.9293178329944263, 0.6248617041832439, 1.1096958453802825, 1.276241170050909, 0.621949313417716, 1.0935463154186575, 3.895237054631072, 0.8285196327006729, 1.1211193324873832, 0.6233399000438051, 1.133549538614931, 2.8591629098122877, 1.056539325226626, 1.2027013315295105, 1.0574511185182842, 1.120207539195725, 1.2521780196830834, 1.1991848909520428, 0.6562025514118967, 1.4786142666104083, 1.5251660950109724, 1.1255892495630684, 1.4640497142422326, 1.0892965686989118, 0.9165131806369283, 3.9046853515860533, 0.7730818261925849, 0.6242516933354635, 1.2330168396694061, 0.6233399000438051, 3.870271473444342, 0.6696920977492333, 0.7430669058769287, 1.0506376304314062, 0.5911963450200739, 2.3109227587551424, 0.621949313417716, 0.7650528690026629, 0.7666096312910925, 1.397462413230191, 0.6154278230532134, 0.8384840470231387, 0.8081927443609856, 0.8904847646483098, 2.62320516970114, 1.2027013315295105, 0.5862250582671887, 1.5266941912693366, 1.6025866427170141, 0.9290664968440012, 0.8839050368193696, 1.1255892495630684, 2.2881226993534645, 1.2804517981133234, 0.6248617041832439, 1.0574511185182842, 1.1226152958920288, 0.6233399000438051, 1.2287670929496604, 0.5911963450200739, 2.942380202268797, 0.7801954312796229, 0.8807888220223115, 0.8724947449620987, 0.6026942182140014, 0.5921081383117323, 0.7650528690026629, 1.1228999825529782, 1.1211193324873832, 0.7794057382825397, 0.9293178329944263, 0.5892987754601237, 1.2027013315295105, 0.8102835365186419, 1.4413480196246113, 2.306596952679573, 3.0140336868063278, 0.5862250582671887, 2.0741253520509564, 0.6951481779234018, 0.5790243169174282, 1.5266941912693366, 1.1255892495630684, 0.6242516933354635, 1.0506376304314062, 0.6430318142201775, 0.8120779113440232, 0.7650528690026629, 1.2456677522270625, 1.2486615791056157, 0.5921081383117323, 0.9228157898814048, 1.4886589551276375, 0.6859506544454586, 0.8034124108737369, 3.595780959294508, 0.7801954312796229, 0.7730818261925849, 0.8925782326875918, 1.2804517981133234, 2.621296235259164, 1.0892965686989118, 0.5862250582671887, 0.6233399000438051, 1.2521780196830834, 1.0574511185182842, 1.0574511185182842, 0.6026942182140014, 1.064974896438853, 0.7794057382825397, 1.056539325226626, 0.5911963450200739, 0.9293178329944263, 0.6951481779234018, 0.6086429118919788, 0.7730818261925849, 0.6233399000438051, 0.8201033097227195, 0.5862250582671887, 1.133549538614931, 1.1211193324873832, 1.1211193324873832, 0.6242516933354635, 0.8552857613061718, 0.8120779113440232, 1.1274709663957305, 1.064974896438853, 0.8267281945640218, 1.7516359457113626, 0.8839050368193696, 0.6242516933354635, 0.5921081383117323, 1.1226152958920288, 0.6491216341368565, 1.2027013315295105, 0.9610907565600675, 0.6851950320640818, 0.6430318142201775, 1.889020857960701, 0.7730818261925849, 2.014235923667022, 1.5226814356513285, 1.0645647236053224, 0.5921081383117323, 1.0645647236053224, 0.8081927443609856, 0.7730818261925849, 1.2287670929496604, 0.8956361400188532, 0.9298620563239034, 0.8384840470231387, 1.2027013315295105, 0.5911963450200739, 0.7801954312796229, 0.6154278230532134, 1.091504585709002, 0.8446626326477781, 0.7676881557232097, 0.7247005589759677, 0.6154278230532134, 0.908400218661733, 0.6430318142201775, 1.0645647236053224, 0.5921081383117323, 1.120207539195725, 1.1560361705812516, 1.2521780196830834, 1.2027013315295105, 1.5226814356513285, 3.895237054631072, 0.9307636849747011, 0.9256123096041577, 0.8285196327006729, 0.68219045576689, 0.5891374490327167, 0.6248617041832439, 0.8315417347252156, 2.103556520790529, 0.7730818261925849, 1.2521780196830834, 0.8392130792670346, 0.6242516933354635, 1.5226814356513285, 0.9307636849747011, 0.6723360807821935, 0.6242516933354635, 0.7676881557232097, 0.7282169995534352, 0.8120779113440232, 0.621949313417716, 0.6233399000438051, 1.2309751099597506, 1.2027013315295105, 0.7721700329009266, 0.8691512538471332, 0.7282169995534352, 2.2448315857624745, 2.9755757287979656, 1.1225819852860845, 1.0574511185182842, 0.7676881557232097, 0.775155991562794, 1.1211193324873832, 0.5862250582671887, 0.6248617041832439, 0.7801954312796229, 3.016332072637105, 0.8392130792670346, 0.6430318142201775, 0.8120779113440232, 1.2782437811032332, 2.9135296383416227, 0.8065175097341891, 1.1226152958920288, 1.5226814356513285, 1.7570592509984182, 1.1171654108502163, 0.9965971747967023, 1.2027013315295105, 0.7730818261925849, 1.6817736680956321, 0.8129897046356815, 0.7676881557232097, 1.2265412929563608, 0.6233399000438051, 0.8882236129856965, 0.6365596193186851, 0.7282169995534352, 0.6696920977492333, 0.6066403008396546, 0.6242516933354635, 1.4886589551276375, 1.0413751998355418, 1.040811280621594, 0.6430318142201775, 1.7937947982552551, 0.6233399000438051, 1.2521780196830834, 0.5882047395457417, 1.2027013315295105, 1.1235048500977018, 1.0645647236053224, 1.2521780196830834, 1.5226814356513285, 0.7195491836054244, 0.9256123096041577, 1.1210281943744538, 1.1847076349959733, 1.3221881152819384, 1.5472466522958683, 0.5911963450200739, 1.2330168396694061, 1.3221881152819384, 0.6086429118919788, 0.5891374490327167, 0.6233399000438051, 0.6233399000438051, 3.9046853515860533, 0.6026942182140014, 0.7666096312910925, 1.1226152958920288, 0.8129897046356815, 0.7708194280772647, 0.5891374490327167, 0.9165131806369283, 3.012124752364352, 0.5862250582671887, 0.8392130792670346, 0.7676881557232097, 1.1255892495630684, 0.6155461847793241, 0.6066403008396546, 0.8926927816583999, 1.1023968911329334, 1.0356694002329658, 0.7148539277281692, 0.8082809254663177, 1.1226152958920288, 0.8392130792670346, 1.0574511185182842, 0.7721700329009266, 0.7282169995534352, 0.8839050368193696, 2.2881226993534645, 0.6696920977492333, 0.7801954312796229, 0.8812219997773592, 0.7731533805104698, 1.0574511185182842, 0.68219045576689, 0.8081927443609856, 1.064974896438853, 1.127363795727488, 1.2027013315295105, 1.0574511185182842, 0.7676881557232097, 1.4078930071335756, 0.775155991562794, 1.0892965686989118, 1.5251660950109724, 1.120207539195725, 3.2828769106667948, 1.091504585709002, 0.8708968993542585, 1.11503618978528, 0.6242516933354635, 0.686943600764599, 0.7730818261925849, 0.8882236129856965, 1.056539325226626, 0.8034124108737369, 0.6026942182140014, 0.7282169995534352, 0.68219045576689, 1.0935463154186575, 0.7773640085728841, 0.7730818261925849, 1.1211193324873832, 0.6430318142201775, 0.9417797995331112, 3.5455077719786394, 0.7801954312796229, 0.8760935661450563, 1.0645647236053224, 1.2027013315295105, 0.775155991562794, 0.7708194280772647, 0.6562025514118967, 1.0574511185182842, 0.9293178329944263, 0.7801954312796229, 0.8691512538471332, 1.1211193324873832, 0.6430318142201775, 1.2804517981133234, 0.5891374490327167, 2.9837602533485774, 0.8201033097227195, 1.0356694002329658, 0.8458588402302021, 1.0574511185182842, 0.775155991562794, 1.3221881152819384, 1.056539325226626, 0.5911963450200739, 0.9290664968440012, 0.9256123096041577, 0.5911963450200739, 1.0574511185182842, 0.8129897046356815, 0.5862250582671887, 0.7148539277281692, 3.916506272872338, 0.9480851396171222, 1.2027013315295105, 0.6242516933354635, 1.2521780196830834, 1.5501661894116276, 0.9293178329944263, 1.091504585709002, 0.7282169995534352, 1.1211193324873832, 3.595780959294508, 0.5891374490327167, 0.8713781908973701, 0.9165131806369283, 1.4886589551276375, 0.7676881557232097, 0.7398177613147785, 1.0892965686989118, 0.5891374490327167, 0.5911963450200739, 1.5472466522958683, 0.6723360807821935, 1.2287670929496604, 0.6597740659640221, 1.1202105780807279, 0.7460637092693108, 0.613338167769234, 0.9165131806369283, 0.7721700329009266, 0.8941902880385784, 0.8159829462426691, 0.8211677929541877, 0.5862250582671887, 0.7801954312796229, 0.6242516933354635, 1.2521780196830834, 0.6248617041832439, 0.7801954312796229, 0.7650528690026629, 1.1211193324873832, 1.1226152958920288, 0.9293178329944263, 1.0574511185182842, 0.7801954312796229, 0.6248617041832439, 0.7676881557232097, 0.621949313417716, 1.3418923174923127, 1.5472466522958683, 1.056539325226626, 0.8591564259544826, 1.062667154045372, 0.6242516933354635, 0.7773640085728841, 0.613338167769234, 1.0892965686989118, 0.8276399878556802, 0.7730818261925849, 0.7282169995534352, 1.056539325226626, 0.6233399000438051, 0.8211677929541877, 0.7650528690026629, 3.916506272872338], \"yaxis\": \"y\"}],                        {\"legend\": {\"title\": {\"text\": \"variable\"}, \"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"r\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"value\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('09aea5d2-b799-466c-9cb9-9a4cc8f9f87c');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torep = pd.DataFrame(y_test.values, columns = ['Data'])\n",
    "torep['ANN'] = DenseAdamaxPrediction5\n",
    "torep['RFC'] = RFC\n",
    "torep['GBC'] = GBC\n",
    "torep.index = x_test['r'] \n",
    "TOREP = torep.iloc[:, :]\n",
    "TOREP.plot.scatter(x=TOREP.index, y=['Data', 'ANN', 'RFC', 'GBC'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "variable=señal<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "señal",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "mode": "lines",
         "name": "señal",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39
         ],
         "xaxis": "x",
         "y": [
          0.6061,
          0.5017199999999999,
          1.0674,
          0.61416,
          0.6765100000000001,
          0.61165,
          0.9469799999999999,
          0.6339899999999999,
          0.8710399999999999,
          0.7925800000000001,
          0.81788,
          0.93976,
          0.71763,
          1.1096,
          0.7520899999999999,
          1.1566,
          1.1199,
          0.62097,
          1.2945,
          1.2389,
          1.0221,
          0.6221800000000001,
          0.8186399999999999,
          0.7819699999999999,
          1.0676,
          1.26,
          0.50716,
          0.71532,
          0.82004,
          0.90781,
          0.93891,
          1.3026,
          0.71337,
          0.75536,
          0.77772,
          0.7104699999999999,
          0.85883,
          0.9490700000000001,
          0.68693,
          1.0944
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "variable"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "index"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "value"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"dde44f17-1b1c-4274-a57b-1aa88c7504a7\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"dde44f17-1b1c-4274-a57b-1aa88c7504a7\")) {                    Plotly.newPlot(                        \"dde44f17-1b1c-4274-a57b-1aa88c7504a7\",                        [{\"hovertemplate\": \"variable=se\\u00f1al<br>index=%{x}<br>value=%{y}<extra></extra>\", \"legendgroup\": \"se\\u00f1al\", \"line\": {\"color\": \"#636efa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"se\\u00f1al\", \"orientation\": \"v\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], \"xaxis\": \"x\", \"y\": [0.6061, 0.5017199999999999, 1.0674, 0.61416, 0.6765100000000001, 0.61165, 0.9469799999999999, 0.6339899999999999, 0.8710399999999999, 0.7925800000000001, 0.81788, 0.93976, 0.71763, 1.1096, 0.7520899999999999, 1.1566, 1.1199, 0.62097, 1.2945, 1.2389, 1.0221, 0.6221800000000001, 0.8186399999999999, 0.7819699999999999, 1.0676, 1.26, 0.50716, 0.71532, 0.82004, 0.90781, 0.93891, 1.3026, 0.71337, 0.75536, 0.77772, 0.7104699999999999, 0.85883, 0.9490700000000001, 0.68693, 1.0944], \"yaxis\": \"y\"}],                        {\"legend\": {\"title\": {\"text\": \"variable\"}, \"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"index\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"value\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('dde44f17-1b1c-4274-a57b-1aa88c7504a7');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(y_test).reset_index(drop=True)[:40].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "variable=0<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "0",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "mode": "lines",
         "name": "0",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39
         ],
         "xaxis": "x",
         "y": [
          0.861886739730835,
          0.861886739730835,
          0.861886739730835,
          0.861886739730835,
          0.861886739730835,
          0.861886739730835,
          0.861886739730835,
          0.861886739730835,
          0.861886739730835,
          0.861886739730835,
          0.861886739730835,
          0.861886739730835,
          0.861886739730835,
          0.861886739730835,
          0.861886739730835,
          0.861886739730835,
          0.861886739730835,
          0.861886739730835,
          0.861886739730835,
          0.861886739730835,
          0.861886739730835,
          0.861886739730835,
          0.861886739730835,
          0.861886739730835,
          0.861886739730835,
          0.861886739730835,
          0.861886739730835,
          0.861886739730835,
          0.861886739730835,
          0.861886739730835,
          0.861886739730835,
          0.861886739730835,
          0.861886739730835,
          0.861886739730835,
          0.861886739730835,
          0.861886739730835,
          0.861886739730835,
          0.861886739730835,
          0.861886739730835,
          0.861886739730835
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "variable"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "index"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "value"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"bbe8532d-b19f-40f8-8282-7e8176a566a8\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"bbe8532d-b19f-40f8-8282-7e8176a566a8\")) {                    Plotly.newPlot(                        \"bbe8532d-b19f-40f8-8282-7e8176a566a8\",                        [{\"hovertemplate\": \"variable=0<br>index=%{x}<br>value=%{y}<extra></extra>\", \"legendgroup\": \"0\", \"line\": {\"color\": \"#636efa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"0\", \"orientation\": \"v\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], \"xaxis\": \"x\", \"y\": [0.861886739730835, 0.861886739730835, 0.861886739730835, 0.861886739730835, 0.861886739730835, 0.861886739730835, 0.861886739730835, 0.861886739730835, 0.861886739730835, 0.861886739730835, 0.861886739730835, 0.861886739730835, 0.861886739730835, 0.861886739730835, 0.861886739730835, 0.861886739730835, 0.861886739730835, 0.861886739730835, 0.861886739730835, 0.861886739730835, 0.861886739730835, 0.861886739730835, 0.861886739730835, 0.861886739730835, 0.861886739730835, 0.861886739730835, 0.861886739730835, 0.861886739730835, 0.861886739730835, 0.861886739730835, 0.861886739730835, 0.861886739730835, 0.861886739730835, 0.861886739730835, 0.861886739730835, 0.861886739730835, 0.861886739730835, 0.861886739730835, 0.861886739730835, 0.861886739730835], \"yaxis\": \"y\"}],                        {\"legend\": {\"title\": {\"text\": \"variable\"}, \"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"index\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"value\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('bbe8532d-b19f-40f8-8282-7e8176a566a8');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(DenseRMSpropPrediction4)[:40].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "426.667px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
