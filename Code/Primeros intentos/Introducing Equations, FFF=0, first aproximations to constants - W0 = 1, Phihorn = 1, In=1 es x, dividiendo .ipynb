{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "pd.options.plotting.backend = 'plotly'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cuadrados 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-103-5e7b1d497cad>:1: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n",
      "<ipython-input-103-5e7b1d497cad>:2: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n",
      "<ipython-input-103-5e7b1d497cad>:3: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n",
      "<ipython-input-103-5e7b1d497cad>:5: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n",
      "<ipython-input-103-5e7b1d497cad>:6: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n",
      "<ipython-input-103-5e7b1d497cad>:7: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pddopen5 = pd.read_csv('OPEN/txt/5x5/5x5pdd.txt', delimiter='\t\t', names=['r', 'señal open 5x5', 'error open 5x5'])\n",
    "pcropen5 = pd.read_csv('OPEN/txt/5x5/5x5pro-cr-100.txt', delimiter='\t\t', names=['r', 'señal open 5x5', 'error open 5x5'])\n",
    "pinopen5 = pd.read_csv('OPEN/txt/5x5/5x5pro-in-100.txt', delimiter='\t\t', names=['r', 'señal open 5x5', 'error open 5x5'])\n",
    "\n",
    "pddFFF5 = pd.read_csv('FFF/txt/5x5/5x5pdd.txt', delimiter='\t\t', names=['r', 'señal fff 5x5', 'error fff 5x5'])\n",
    "pcrFFF5 = pd.read_csv('FFF/txt/5x5/5x5pro-cr-100.txt', delimiter='\t\t', names=['r', 'señal fff 5x5', 'error fff 5x5'])\n",
    "pinFFF5 = pd.read_csv('FFF/txt/5x5/5x5pro-in-100.txt', delimiter='\t\t', names=['r', 'señal fff 5x5', 'error fff 5x5'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10x10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-104-5559e724810d>:1: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n",
      "<ipython-input-104-5559e724810d>:2: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n",
      "<ipython-input-104-5559e724810d>:3: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n",
      "<ipython-input-104-5559e724810d>:5: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n",
      "<ipython-input-104-5559e724810d>:6: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n",
      "<ipython-input-104-5559e724810d>:7: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pddopen10 = pd.read_csv('OPEN/txt/10x10/10x10pdd.txt', delimiter='\t\t', names=['r', 'señal open 10x10', 'error open 10x10'])\n",
    "pcropen10 = pd.read_csv('OPEN/txt/10x10/10x10pro-cr-100.txt', delimiter='\t\t', names=['r', 'señal open 10x10', 'error open 10x10'])\n",
    "pinopen10 = pd.read_csv('OPEN/txt/10x10/10x10pro-in-100.txt', delimiter='\t\t', names=['r', 'señal open 10x10', 'error open 10x10'])\n",
    "\n",
    "pddFFF10 = pd.read_csv('FFF/txt/10x10/10x10pdd.txt', delimiter='\t\t', names=['r', 'señal fff 10x10', 'error fff 10x10'])\n",
    "pcrFFF10 = pd.read_csv('FFF/txt/10x10/10x10pro-cr-100.txt', delimiter='\t\t', names=['r', 'señal fff 10x10', 'error fff 10x10'])\n",
    "pinFFF10 = pd.read_csv('FFF/txt/10x10/10x10pro-in-100.txt', delimiter='\t\t', names=['r', 'señal fff 10x10', 'error fff 10x10'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20x20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-105-fc4605168945>:1: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n",
      "<ipython-input-105-fc4605168945>:2: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n",
      "<ipython-input-105-fc4605168945>:3: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n",
      "<ipython-input-105-fc4605168945>:5: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n",
      "<ipython-input-105-fc4605168945>:6: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n",
      "<ipython-input-105-fc4605168945>:7: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pddopen20 = pd.read_csv('OPEN/txt/20x20/20x20pdd.txt', delimiter='\t\t', names=['r', 'señal open 20x20', 'error open 20x20'])\n",
    "pcropen20 = pd.read_csv('OPEN/txt/20x20/20x20pro-cr-100.txt', delimiter='\t\t', names=['r', 'señal open 20x20', 'error open 20x20'])\n",
    "pinopen20 = pd.read_csv('OPEN/txt/20x20/20x20pro-in-100.txt', delimiter='\t\t', names=['r', 'señal open 20x20', 'error open 20x20'])\n",
    "\n",
    "pddFFF20 = pd.read_csv('FFF/txt/20x20/20x20pdd.txt', delimiter='\t\t', names=['r', 'señal fff 20x20', 'error fff 20x20'])\n",
    "pcrFFF20 = pd.read_csv('FFF/txt/20x20/20x20pro-cr-100.txt', delimiter='\t\t', names=['r', 'señal fff 20x20', 'error fff 20x20'])\n",
    "pinFFF20 = pd.read_csv('FFF/txt/20x20/20x20pro-in-100.txt', delimiter='\t\t', names=['r', 'señal fff 20x20', 'error fff 20x20'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 40x40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-106-38d7031e937e>:1: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n",
      "<ipython-input-106-38d7031e937e>:2: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n",
      "<ipython-input-106-38d7031e937e>:3: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n",
      "<ipython-input-106-38d7031e937e>:5: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n",
      "<ipython-input-106-38d7031e937e>:6: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n",
      "<ipython-input-106-38d7031e937e>:7: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pddopen40 = pd.read_csv('OPEN/txt/40x40/40x40pdd.txt', delimiter='\t\t', names=['r', 'señal open 40x40', 'error open 40x40'])\n",
    "pcropen40 = pd.read_csv('OPEN/txt/40x40/40x40pro-cr-100.txt', delimiter='\t\t', names=['r', 'señal open 40x40', 'error open 40x40'])\n",
    "pinopen40 = pd.read_csv('OPEN/txt/40x40/40x40pro-in-100.txt', delimiter='\t\t', names=['r', 'señal open 40x40', 'error open 40x40'])\n",
    "\n",
    "pddFFF40 = pd.read_csv('FFF/txt/40x40/40x40pdd.txt', delimiter='\t\t', names=['r', 'señal fff 40x40', 'error fff 40x40'])\n",
    "pcrFFF40 = pd.read_csv('FFF/txt/40x40/40x40pro-cr-100.txt', delimiter='\t\t', names=['r', 'señal fff 40x40', 'error fff 40x40'])\n",
    "pinFFF40 = pd.read_csv('FFF/txt/40x40/40x40pro-in-100.txt', delimiter='\t\t', names=['r', 'señal fff 40x40', 'error fff 40x40'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cuadrados 115"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-107-b3dbae33331e>:1: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n",
      "<ipython-input-107-b3dbae33331e>:2: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n",
      "<ipython-input-107-b3dbae33331e>:3: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n",
      "<ipython-input-107-b3dbae33331e>:5: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n",
      "<ipython-input-107-b3dbae33331e>:6: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n",
      "<ipython-input-107-b3dbae33331e>:7: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pddopen5_115 = pd.read_csv('OPEN/txt/5x5/5x5pdd.txt', delimiter='\t\t', names=['r', 'señal open 5x5', 'error open 5x5'])\n",
    "pcropen5_115 = pd.read_csv('OPEN/txt/5x5/5x5pro-cr-115.txt', delimiter='\t\t', names=['r', 'señal open 5x5', 'error open 5x5'])\n",
    "pinopen5_115 = pd.read_csv('OPEN/txt/5x5/5x5pro-in-115.txt', delimiter='\t\t', names=['r', 'señal open 5x5', 'error open 5x5'])\n",
    "\n",
    "pddFFF5_115 = pd.read_csv('FFF/txt/5x5/5x5pdd.txt', delimiter='\t\t', names=['r', 'señal fff 5x5', 'error fff 5x5'])\n",
    "pcrFFF5_115 = pd.read_csv('FFF/txt/5x5/5x5pro-cr-115.txt', delimiter='\t\t', names=['r', 'señal fff 5x5', 'error fff 5x5'])\n",
    "pinFFF5_115 = pd.read_csv('FFF/txt/5x5/5x5pro-in-115.txt', delimiter='\t\t', names=['r', 'señal fff 5x5', 'error fff 5x5'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10x10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-108-876a524bf11a>:1: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n",
      "<ipython-input-108-876a524bf11a>:2: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n",
      "<ipython-input-108-876a524bf11a>:3: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n",
      "<ipython-input-108-876a524bf11a>:5: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n",
      "<ipython-input-108-876a524bf11a>:6: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n",
      "<ipython-input-108-876a524bf11a>:7: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pddopen10_115 = pd.read_csv('OPEN/txt/10x10/10x10pdd.txt', delimiter='\t\t', names=['r', 'señal open 10x10', 'error open 10x10'])\n",
    "pcropen10_115 = pd.read_csv('OPEN/txt/10x10/10x10pro-cr-115.txt', delimiter='\t\t', names=['r', 'señal open 10x10', 'error open 10x10'])\n",
    "pinopen10_115 = pd.read_csv('OPEN/txt/10x10/10x10pro-in-115.txt', delimiter='\t\t', names=['r', 'señal open 10x10', 'error open 10x10'])\n",
    "\n",
    "pddFFF10_115 = pd.read_csv('FFF/txt/10x10/10x10pdd.txt', delimiter='\t\t', names=['r', 'señal fff 10x10', 'error fff 10x10'])\n",
    "pcrFFF10_115 = pd.read_csv('FFF/txt/10x10/10x10pro-cr-115.txt', delimiter='\t\t', names=['r', 'señal fff 10x10', 'error fff 10x10'])\n",
    "pinFFF10_115 = pd.read_csv('FFF/txt/10x10/10x10pro-in-115.txt', delimiter='\t\t', names=['r', 'señal fff 10x10', 'error fff 10x10'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20x20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-109-97b1090201f8>:1: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n",
      "<ipython-input-109-97b1090201f8>:2: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n",
      "<ipython-input-109-97b1090201f8>:3: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n",
      "<ipython-input-109-97b1090201f8>:5: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n",
      "<ipython-input-109-97b1090201f8>:6: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n",
      "<ipython-input-109-97b1090201f8>:7: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pddopen20_115 = pd.read_csv('OPEN/txt/20x20/20x20pdd.txt', delimiter='\t\t', names=['r', 'señal open 20x20', 'error open 20x20'])\n",
    "pcropen20_115 = pd.read_csv('OPEN/txt/20x20/20x20pro-cr-115.txt', delimiter='\t\t', names=['r', 'señal open 20x20', 'error open 20x20'])\n",
    "pinopen20_115 = pd.read_csv('OPEN/txt/20x20/20x20pro-in-115.txt', delimiter='\t\t', names=['r', 'señal open 20x20', 'error open 20x20'])\n",
    "\n",
    "pddFFF20_115 = pd.read_csv('FFF/txt/20x20/20x20pdd.txt', delimiter='\t\t', names=['r', 'señal fff 20x20', 'error fff 20x20'])\n",
    "pcrFFF20_115 = pd.read_csv('FFF/txt/20x20/20x20pro-cr-115.txt', delimiter='\t\t', names=['r', 'señal fff 20x20', 'error fff 20x20'])\n",
    "pinFFF20_115 = pd.read_csv('FFF/txt/20x20/20x20pro-in-115.txt', delimiter='\t\t', names=['r', 'señal fff 20x20', 'error fff 20x20'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 40x40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-110-5deaefc013ea>:1: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n",
      "<ipython-input-110-5deaefc013ea>:2: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n",
      "<ipython-input-110-5deaefc013ea>:3: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n",
      "<ipython-input-110-5deaefc013ea>:5: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n",
      "<ipython-input-110-5deaefc013ea>:6: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n",
      "<ipython-input-110-5deaefc013ea>:7: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pddopen40_115 = pd.read_csv('OPEN/txt/40x40/40x40pdd.txt', delimiter='\t\t', names=['r', 'señal open 40x40', 'error open 40x40'])\n",
    "pcropen40_115 = pd.read_csv('OPEN/txt/40x40/40x40pro-cr-115.txt', delimiter='\t\t', names=['r', 'señal open 40x40', 'error open 40x40'])\n",
    "pinopen40_115 = pd.read_csv('OPEN/txt/40x40/40x40pro-in-115.txt', delimiter='\t\t', names=['r', 'señal open 40x40', 'error open 40x40'])\n",
    "\n",
    "pddFFF40_115 = pd.read_csv('FFF/txt/40x40/40x40pdd.txt', delimiter='\t\t', names=['r', 'señal fff 40x40', 'error fff 40x40'])\n",
    "pcrFFF40_115 = pd.read_csv('FFF/txt/40x40/40x40pro-cr-115.txt', delimiter='\t\t', names=['r', 'señal fff 40x40', 'error fff 40x40'])\n",
    "pinFFF40_115 = pd.read_csv('FFF/txt/40x40/40x40pro-in-115.txt', delimiter='\t\t', names=['r', 'señal fff 40x40', 'error fff 40x40'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 115"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-8e01dae11aa5>:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pddopen5_85 = pd.read_csv('OPEN/txt/5x5/5x5pdd.txt', delimiter='\t\t', names=['r', 'señal open 5x5', 'error open 5x5'])\n",
      "<ipython-input-10-8e01dae11aa5>:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pcropen5_85 = pd.read_csv('OPEN/txt/5x5/5x5pro-cr-85.txt', delimiter='\t\t', names=['r', 'señal open 5x5', 'error open 5x5'])\n",
      "<ipython-input-10-8e01dae11aa5>:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pinopen5_85 = pd.read_csv('OPEN/txt/5x5/5x5pro-in-85.txt', delimiter='\t\t', names=['r', 'señal open 5x5', 'error open 5x5'])\n",
      "<ipython-input-10-8e01dae11aa5>:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pddFFF5_85 = pd.read_csv('FFF/txt/5x5/5x5pdd.txt', delimiter='\t\t', names=['r', 'señal fff 5x5', 'error fff 5x5'])\n",
      "<ipython-input-10-8e01dae11aa5>:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pcrFFF5_85 = pd.read_csv('FFF/txt/5x5/5x5pro-cr-85.txt', delimiter='\t\t', names=['r', 'señal fff 5x5', 'error fff 5x5'])\n",
      "<ipython-input-10-8e01dae11aa5>:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pinFFF5_85 = pd.read_csv('FFF/txt/5x5/5x5pro-in-85.txt', delimiter='\t\t', names=['r', 'señal fff 5x5', 'error fff 5x5'])\n"
     ]
    }
   ],
   "source": [
    "pddopen5_85 = pd.read_csv('OPEN/txt/5x5/5x5pdd.txt', delimiter='\t\t', names=['r', 'señal open 5x5', 'error open 5x5'])\n",
    "pcropen5_85 = pd.read_csv('OPEN/txt/5x5/5x5pro-cr-85.txt', delimiter='\t\t', names=['r', 'señal open 5x5', 'error open 5x5'])\n",
    "pinopen5_85 = pd.read_csv('OPEN/txt/5x5/5x5pro-in-85.txt', delimiter='\t\t', names=['r', 'señal open 5x5', 'error open 5x5'])\n",
    "\n",
    "pddFFF5_85 = pd.read_csv('FFF/txt/5x5/5x5pdd.txt', delimiter='\t\t', names=['r', 'señal fff 5x5', 'error fff 5x5'])\n",
    "pcrFFF5_85 = pd.read_csv('FFF/txt/5x5/5x5pro-cr-85.txt', delimiter='\t\t', names=['r', 'señal fff 5x5', 'error fff 5x5'])\n",
    "pinFFF5_85 = pd.read_csv('FFF/txt/5x5/5x5pro-in-85.txt', delimiter='\t\t', names=['r', 'señal fff 5x5', 'error fff 5x5'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10x10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-efc97f103422>:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pddopen10_85 = pd.read_csv('OPEN/txt/10x10/10x10pdd.txt', delimiter='\t\t', names=['r', 'señal open 10x10', 'error open 10x10'])\n",
      "<ipython-input-11-efc97f103422>:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pcropen10_85 = pd.read_csv('OPEN/txt/10x10/10x10pro-cr-85.txt', delimiter='\t\t', names=['r', 'señal open 10x10', 'error open 10x10'])\n",
      "<ipython-input-11-efc97f103422>:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pinopen10_85 = pd.read_csv('OPEN/txt/10x10/10x10pro-in-85.txt', delimiter='\t\t', names=['r', 'señal open 10x10', 'error open 10x10'])\n",
      "<ipython-input-11-efc97f103422>:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pddFFF10_85 = pd.read_csv('FFF/txt/10x10/10x10pdd.txt', delimiter='\t\t', names=['r', 'señal fff 10x10', 'error fff 10x10'])\n",
      "<ipython-input-11-efc97f103422>:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pcrFFF10_85 = pd.read_csv('FFF/txt/10x10/10x10pro-cr-85.txt', delimiter='\t\t', names=['r', 'señal fff 10x10', 'error fff 10x10'])\n",
      "<ipython-input-11-efc97f103422>:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pinFFF10_85 = pd.read_csv('FFF/txt/10x10/10x10pro-in-85.txt', delimiter='\t\t', names=['r', 'señal fff 10x10', 'error fff 10x10'])\n"
     ]
    }
   ],
   "source": [
    "pddopen10_85 = pd.read_csv('OPEN/txt/10x10/10x10pdd.txt', delimiter='\t\t', names=['r', 'señal open 10x10', 'error open 10x10'])\n",
    "pcropen10_85 = pd.read_csv('OPEN/txt/10x10/10x10pro-cr-85.txt', delimiter='\t\t', names=['r', 'señal open 10x10', 'error open 10x10'])\n",
    "pinopen10_85 = pd.read_csv('OPEN/txt/10x10/10x10pro-in-85.txt', delimiter='\t\t', names=['r', 'señal open 10x10', 'error open 10x10'])\n",
    "\n",
    "pddFFF10_85 = pd.read_csv('FFF/txt/10x10/10x10pdd.txt', delimiter='\t\t', names=['r', 'señal fff 10x10', 'error fff 10x10'])\n",
    "pcrFFF10_85 = pd.read_csv('FFF/txt/10x10/10x10pro-cr-85.txt', delimiter='\t\t', names=['r', 'señal fff 10x10', 'error fff 10x10'])\n",
    "pinFFF10_85 = pd.read_csv('FFF/txt/10x10/10x10pro-in-85.txt', delimiter='\t\t', names=['r', 'señal fff 10x10', 'error fff 10x10'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20x20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-a68ef06485ef>:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pddopen20_85 = pd.read_csv('OPEN/txt/20x20/20x20pdd.txt', delimiter='\t\t', names=['r', 'señal open 20x20', 'error open 20x20'])\n",
      "<ipython-input-12-a68ef06485ef>:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pcropen20_85 = pd.read_csv('OPEN/txt/20x20/20x20pro-cr-85.txt', delimiter='\t\t', names=['r', 'señal open 20x20', 'error open 20x20'])\n",
      "<ipython-input-12-a68ef06485ef>:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pinopen20_85 = pd.read_csv('OPEN/txt/20x20/20x20pro-in-85.txt', delimiter='\t\t', names=['r', 'señal open 20x20', 'error open 20x20'])\n",
      "<ipython-input-12-a68ef06485ef>:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pddFFF20_85 = pd.read_csv('FFF/txt/20x20/20x20pdd.txt', delimiter='\t\t', names=['r', 'señal fff 20x20', 'error fff 20x20'])\n",
      "<ipython-input-12-a68ef06485ef>:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pcrFFF20_85 = pd.read_csv('FFF/txt/20x20/20x20pro-cr-85.txt', delimiter='\t\t', names=['r', 'señal fff 20x20', 'error fff 20x20'])\n",
      "<ipython-input-12-a68ef06485ef>:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pinFFF20_85 = pd.read_csv('FFF/txt/20x20/20x20pro-in-85.txt', delimiter='\t\t', names=['r', 'señal fff 20x20', 'error fff 20x20'])\n"
     ]
    }
   ],
   "source": [
    "pddopen20_85 = pd.read_csv('OPEN/txt/20x20/20x20pdd.txt', delimiter='\t\t', names=['r', 'señal open 20x20', 'error open 20x20'])\n",
    "pcropen20_85 = pd.read_csv('OPEN/txt/20x20/20x20pro-cr-85.txt', delimiter='\t\t', names=['r', 'señal open 20x20', 'error open 20x20'])\n",
    "pinopen20_85 = pd.read_csv('OPEN/txt/20x20/20x20pro-in-85.txt', delimiter='\t\t', names=['r', 'señal open 20x20', 'error open 20x20'])\n",
    "\n",
    "pddFFF20_85 = pd.read_csv('FFF/txt/20x20/20x20pdd.txt', delimiter='\t\t', names=['r', 'señal fff 20x20', 'error fff 20x20'])\n",
    "pcrFFF20_85 = pd.read_csv('FFF/txt/20x20/20x20pro-cr-85.txt', delimiter='\t\t', names=['r', 'señal fff 20x20', 'error fff 20x20'])\n",
    "pinFFF20_85 = pd.read_csv('FFF/txt/20x20/20x20pro-in-85.txt', delimiter='\t\t', names=['r', 'señal fff 20x20', 'error fff 20x20'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 40x40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-a1eb063f6d0d>:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pddopen40_85 = pd.read_csv('OPEN/txt/40x40/40x40pdd.txt', delimiter='\t\t', names=['r', 'señal open 40x40', 'error open 40x40'])\n",
      "<ipython-input-13-a1eb063f6d0d>:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pcropen40_85 = pd.read_csv('OPEN/txt/40x40/40x40pro-cr-85.txt', delimiter='\t\t', names=['r', 'señal open 40x40', 'error open 40x40'])\n",
      "<ipython-input-13-a1eb063f6d0d>:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pinopen40_85 = pd.read_csv('OPEN/txt/40x40/40x40pro-in-85.txt', delimiter='\t\t', names=['r', 'señal open 40x40', 'error open 40x40'])\n",
      "<ipython-input-13-a1eb063f6d0d>:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pddFFF40_85 = pd.read_csv('FFF/txt/40x40/40x40pdd.txt', delimiter='\t\t', names=['r', 'señal fff 40x40', 'error fff 40x40'])\n",
      "<ipython-input-13-a1eb063f6d0d>:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pcrFFF40_85 = pd.read_csv('FFF/txt/40x40/40x40pro-cr-85.txt', delimiter='\t\t', names=['r', 'señal fff 40x40', 'error fff 40x40'])\n",
      "<ipython-input-13-a1eb063f6d0d>:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pinFFF40_85 = pd.read_csv('FFF/txt/40x40/40x40pro-in-85.txt', delimiter='\t\t', names=['r', 'señal fff 40x40', 'error fff 40x40'])\n"
     ]
    }
   ],
   "source": [
    "pddopen40_85 = pd.read_csv('OPEN/txt/40x40/40x40pdd.txt', delimiter='\t\t', names=['r', 'señal open 40x40', 'error open 40x40'])\n",
    "pcropen40_85 = pd.read_csv('OPEN/txt/40x40/40x40pro-cr-85.txt', delimiter='\t\t', names=['r', 'señal open 40x40', 'error open 40x40'])\n",
    "pinopen40_85 = pd.read_csv('OPEN/txt/40x40/40x40pro-in-85.txt', delimiter='\t\t', names=['r', 'señal open 40x40', 'error open 40x40'])\n",
    "\n",
    "pddFFF40_85 = pd.read_csv('FFF/txt/40x40/40x40pdd.txt', delimiter='\t\t', names=['r', 'señal fff 40x40', 'error fff 40x40'])\n",
    "pcrFFF40_85 = pd.read_csv('FFF/txt/40x40/40x40pro-cr-85.txt', delimiter='\t\t', names=['r', 'señal fff 40x40', 'error fff 40x40'])\n",
    "pinFFF40_85 = pd.read_csv('FFF/txt/40x40/40x40pro-in-85.txt', delimiter='\t\t', names=['r', 'señal fff 40x40', 'error fff 40x40'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Z = 100; In = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFF5 = pinFFF5.iloc[:,:2]\n",
    "FFF5['FFF'] = 1\n",
    "FFF5.columns = ['r', 'señal', 'FFF']\n",
    "FFF5['Area'] = 25\n",
    "FFF5['Limit'] = 25\n",
    "\n",
    "Open5 = pinopen5.iloc[:,:2]\n",
    "Open5['FFF'] = 0\n",
    "Open5.columns = ['r', 'señal', 'FFF']\n",
    "Open5['Area'] = 25\n",
    "Open5['Limit'] = 25\n",
    "\n",
    "FFF10 = pinFFF10.iloc[:,:2]\n",
    "FFF10['FFF'] = 1\n",
    "FFF10.columns = ['r', 'señal', 'FFF']\n",
    "FFF10['Area'] = 100\n",
    "FFF10['Limit'] = 50\n",
    "\n",
    "Open10 = pinopen10.iloc[:,:2]\n",
    "Open10['FFF'] = 0\n",
    "Open10.columns = ['r', 'señal', 'FFF']\n",
    "Open10['Area'] = 100\n",
    "Open10['Limit'] = 50\n",
    "\n",
    "FFF20 = pinFFF20.iloc[:,:2]\n",
    "FFF20['FFF'] = 1\n",
    "FFF20.columns = ['r', 'señal', 'FFF']\n",
    "FFF20['Area'] = 400\n",
    "FFF20['Limit'] = 100\n",
    "\n",
    "Open20 = pinopen20.iloc[:,:2]\n",
    "Open20['FFF'] = 0\n",
    "Open20.columns = ['r', 'señal', 'FFF']\n",
    "Open20['Area'] = 400\n",
    "Open20['Limit'] = 100\n",
    "\n",
    "FFF40 = pinFFF40.iloc[:,:2]\n",
    "FFF40['FFF'] = 1\n",
    "FFF40.columns = ['r', 'señal', 'FFF']\n",
    "FFF40['Area'] = 1600\n",
    "FFF40['Limit'] = 200\n",
    "\n",
    "Open40 = pinopen40.iloc[:,:2]\n",
    "Open40['FFF'] = 0\n",
    "Open40.columns = ['r', 'señal', 'FFF']\n",
    "Open40['Area'] = 1600\n",
    "Open40['Limit'] = 200\n",
    "\n",
    "MixedIN = pd.concat([Open5, FFF5, Open10, FFF10, Open20, FFF20, Open40, FFF40]) \n",
    "MixedIN['In'] = 1\n",
    "MixedIN['Z'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Z = 100; In = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFF5 = pcrFFF5.iloc[:,:2]\n",
    "FFF5['FFF'] = 1\n",
    "FFF5.columns = ['r', 'señal', 'FFF']\n",
    "FFF5['Area'] = 25\n",
    "FFF5['Limit'] = 25\n",
    "\n",
    "Open5 = pcropen5.iloc[:,:2]\n",
    "Open5['FFF'] = 0\n",
    "Open5.columns = ['r', 'señal', 'FFF']\n",
    "Open5['Area'] = 25\n",
    "Open5['Limit'] = 25\n",
    "\n",
    "FFF10 = pcrFFF10.iloc[:,:2]\n",
    "FFF10['FFF'] = 1\n",
    "FFF10.columns = ['r', 'señal', 'FFF']\n",
    "FFF10['Area'] = 100\n",
    "FFF10['Limit'] = 50\n",
    "\n",
    "Open10 = pcropen10.iloc[:,:2]\n",
    "Open10['FFF'] = 0\n",
    "Open10.columns = ['r', 'señal', 'FFF']\n",
    "Open10['Area'] = 100\n",
    "Open10['Limit'] = 50\n",
    "\n",
    "FFF20 = pcrFFF20.iloc[:,:2]\n",
    "FFF20['FFF'] = 1\n",
    "FFF20.columns = ['r', 'señal', 'FFF']\n",
    "FFF20['Area'] = 400\n",
    "FFF20['Limit'] = 100\n",
    "\n",
    "Open20 = pcropen20.iloc[:,:2]\n",
    "Open20['FFF'] = 0\n",
    "Open20.columns = ['r', 'señal', 'FFF']\n",
    "Open20['Area'] = 400\n",
    "Open20['Limit'] = 100\n",
    "\n",
    "FFF40 = pcrFFF40.iloc[:,:2]\n",
    "FFF40['FFF'] = 1\n",
    "FFF40.columns = ['r', 'señal', 'FFF']\n",
    "FFF40['Area'] = 1600\n",
    "FFF40['Limit'] = 200\n",
    "\n",
    "Open40 = pcropen40.iloc[:,:2]\n",
    "Open40['FFF'] = 0\n",
    "Open40.columns = ['r', 'señal', 'FFF']\n",
    "Open40['Area'] = 1600\n",
    "Open40['Limit'] = 200\n",
    "\n",
    "MixedCR = pd.concat([Open5, FFF5, Open10, FFF10, Open20, FFF20, Open40, FFF40]) \n",
    "MixedCR['In'] = 0\n",
    "MixedCR['Z'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Z = 85; In = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFF5 = pinFFF5_85.iloc[:,:2]\n",
    "FFF5['FFF'] = 1\n",
    "FFF5.columns = ['r', 'señal', 'FFF']\n",
    "FFF5['Area'] = 25\n",
    "FFF5['Limit'] = 25\n",
    "\n",
    "Open5 = pinopen5_85.iloc[:,:2]\n",
    "Open5['FFF'] = 0\n",
    "Open5.columns = ['r', 'señal', 'FFF']\n",
    "Open5['Area'] = 25\n",
    "Open5['Limit'] = 25\n",
    "\n",
    "FFF10 = pinFFF10_85.iloc[:,:2]\n",
    "FFF10['FFF'] = 1\n",
    "FFF10.columns = ['r', 'señal', 'FFF']\n",
    "FFF10['Area'] = 100\n",
    "FFF10['Limit'] = 50\n",
    "\n",
    "Open10 = pinopen10_85.iloc[:,:2]\n",
    "Open10['FFF'] = 0\n",
    "Open10.columns = ['r', 'señal', 'FFF']\n",
    "Open10['Area'] = 100\n",
    "Open10['Limit'] = 50\n",
    "\n",
    "FFF20 = pinFFF20_85.iloc[:,:2]\n",
    "FFF20['FFF'] = 1\n",
    "FFF20.columns = ['r', 'señal', 'FFF']\n",
    "FFF20['Area'] = 400\n",
    "FFF20['Limit'] = 100\n",
    "\n",
    "Open20 = pinopen20_85.iloc[:,:2]\n",
    "Open20['FFF'] = 0\n",
    "Open20.columns = ['r', 'señal', 'FFF']\n",
    "Open20['Area'] = 400\n",
    "Open20['Limit'] = 100\n",
    "\n",
    "FFF40 = pinFFF40_85.iloc[:,:2]\n",
    "FFF40['FFF'] = 1\n",
    "FFF40.columns = ['r', 'señal', 'FFF']\n",
    "FFF40['Area'] = 1600\n",
    "FFF40['Limit'] = 200\n",
    "\n",
    "Open40 = pinopen40_85.iloc[:,:2]\n",
    "Open40['FFF'] = 0\n",
    "Open40.columns = ['r', 'señal', 'FFF']\n",
    "Open40['Area'] = 1600\n",
    "Open40['Limit'] = 200\n",
    "\n",
    "MixedIN85 = pd.concat([Open5, FFF5, Open10, FFF10, Open20, FFF20, Open40, FFF40]) \n",
    "MixedIN85['In'] = 1\n",
    "MixedIN85['Z'] = 85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFF5 = pcrFFF5_85.iloc[:,:2]\n",
    "FFF5['FFF'] = 1\n",
    "FFF5.columns = ['r', 'señal', 'FFF']\n",
    "FFF5['Area'] = 25\n",
    "FFF5['Limit'] = 25\n",
    "\n",
    "Open5 = pcropen5_85.iloc[:,:2]\n",
    "Open5['FFF'] = 0\n",
    "Open5.columns = ['r', 'señal', 'FFF']\n",
    "Open5['Area'] = 25\n",
    "Open5['Limit'] = 25\n",
    "\n",
    "FFF10 = pcrFFF10_85.iloc[:,:2]\n",
    "FFF10['FFF'] = 1\n",
    "FFF10.columns = ['r', 'señal', 'FFF']\n",
    "FFF10['Area'] = 100\n",
    "FFF10['Limit'] = 50\n",
    "\n",
    "Open10 = pcropen10_85.iloc[:,:2]\n",
    "Open10['FFF'] = 0\n",
    "Open10.columns = ['r', 'señal', 'FFF']\n",
    "Open10['Area'] = 100\n",
    "Open10['Limit'] = 50\n",
    "\n",
    "FFF20 = pcrFFF20_85.iloc[:,:2]\n",
    "FFF20['FFF'] = 1\n",
    "FFF20.columns = ['r', 'señal', 'FFF']\n",
    "FFF20['Area'] = 400\n",
    "FFF20['Limit'] = 100\n",
    "\n",
    "Open20 = pcropen20_85.iloc[:,:2]\n",
    "Open20['FFF'] = 0\n",
    "Open20.columns = ['r', 'señal', 'FFF']\n",
    "Open20['Area'] = 400\n",
    "Open20['Limit'] = 100\n",
    "\n",
    "FFF40 = pcrFFF40_85.iloc[:,:2]\n",
    "FFF40['FFF'] = 1\n",
    "FFF40.columns = ['r', 'señal', 'FFF']\n",
    "FFF40['Area'] = 1600\n",
    "FFF40['Limit'] = 200\n",
    "\n",
    "Open40 = pcropen40_85.iloc[:,:2]\n",
    "Open40['FFF'] = 0\n",
    "Open40.columns = ['r', 'señal', 'FFF']\n",
    "Open40['Area'] = 1600\n",
    "Open40['Limit'] = 200\n",
    "\n",
    "MixedCR85 = pd.concat([Open5, FFF5, Open10, FFF10, Open20, FFF20, Open40, FFF40]) \n",
    "MixedCR85['In'] = 0\n",
    "MixedCR85['Z'] = 85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFF5 = pinFFF5_115.iloc[:,:2]\n",
    "FFF5['FFF'] = 1\n",
    "FFF5.columns = ['r', 'señal', 'FFF']\n",
    "FFF5['Area'] = 25\n",
    "FFF5['Limit'] = 25\n",
    "\n",
    "Open5 = pinopen5_115.iloc[:,:2]\n",
    "Open5['FFF'] = 0\n",
    "Open5.columns = ['r', 'señal', 'FFF']\n",
    "Open5['Area'] = 25\n",
    "Open5['Limit'] = 25\n",
    "\n",
    "FFF10 = pinFFF10_115.iloc[:,:2]\n",
    "FFF10['FFF'] = 1\n",
    "FFF10.columns = ['r', 'señal', 'FFF']\n",
    "FFF10['Area'] = 100\n",
    "FFF10['Limit'] = 50\n",
    "\n",
    "Open10 = pinopen10_115.iloc[:,:2]\n",
    "Open10['FFF'] = 0\n",
    "Open10.columns = ['r', 'señal', 'FFF']\n",
    "Open10['Area'] = 100\n",
    "Open10['Limit'] = 50\n",
    "\n",
    "FFF20 = pinFFF20_115.iloc[:,:2]\n",
    "FFF20['FFF'] = 1\n",
    "FFF20.columns = ['r', 'señal', 'FFF']\n",
    "FFF20['Area'] = 400\n",
    "FFF20['Limit'] = 100\n",
    "\n",
    "Open20 = pinopen20_115.iloc[:,:2]\n",
    "Open20['FFF'] = 0\n",
    "Open20.columns = ['r', 'señal', 'FFF']\n",
    "Open20['Area'] = 400\n",
    "Open20['Limit'] = 100\n",
    "\n",
    "FFF40 = pinFFF40_115.iloc[:,:2]\n",
    "FFF40['FFF'] = 1\n",
    "FFF40.columns = ['r', 'señal', 'FFF']\n",
    "FFF40['Area'] = 1600\n",
    "FFF40['Limit'] = 200\n",
    "\n",
    "Open40 = pinopen40_115.iloc[:,:2]\n",
    "Open40['FFF'] = 0\n",
    "Open40.columns = ['r', 'señal', 'FFF']\n",
    "Open40['Area'] = 1600\n",
    "Open40['Limit'] = 200\n",
    "\n",
    "MixedIN115 = pd.concat([Open5, FFF5, Open10, FFF10, Open20, FFF20, Open40, FFF40]) \n",
    "MixedIN115['In'] = 1\n",
    "MixedIN115['Z'] = 115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFF5 = pcrFFF5_115.iloc[:,:2]\n",
    "FFF5['FFF'] = 1\n",
    "FFF5.columns = ['r', 'señal', 'FFF']\n",
    "FFF5['Area'] = 25\n",
    "FFF5['Limit'] = 25\n",
    "\n",
    "Open5 = pcropen5_115.iloc[:,:2]\n",
    "Open5['FFF'] = 0\n",
    "Open5.columns = ['r', 'señal', 'FFF']\n",
    "Open5['Area'] = 25\n",
    "Open5['Limit'] = 25\n",
    "\n",
    "FFF10 = pcrFFF10_115.iloc[:,:2]\n",
    "FFF10['FFF'] = 1\n",
    "FFF10.columns = ['r', 'señal', 'FFF']\n",
    "FFF10['Area'] = 100\n",
    "FFF10['Limit'] = 50\n",
    "\n",
    "Open10 = pcropen10_115.iloc[:,:2]\n",
    "Open10['FFF'] = 0\n",
    "Open10.columns = ['r', 'señal', 'FFF']\n",
    "Open10['Area'] = 100\n",
    "Open10['Limit'] = 50\n",
    "\n",
    "FFF20 = pcrFFF20_115.iloc[:,:2]\n",
    "FFF20['FFF'] = 1\n",
    "FFF20.columns = ['r', 'señal', 'FFF']\n",
    "FFF20['Area'] = 400\n",
    "FFF20['Limit'] = 100\n",
    "\n",
    "Open20 = pcropen20_115.iloc[:,:2]\n",
    "Open20['FFF'] = 0\n",
    "Open20.columns = ['r', 'señal', 'FFF']\n",
    "Open20['Area'] = 400\n",
    "Open20['Limit'] = 100\n",
    "\n",
    "FFF40 = pcrFFF40_115.iloc[:,:2]\n",
    "FFF40['FFF'] = 1\n",
    "FFF40.columns = ['r', 'señal', 'FFF']\n",
    "FFF40['Area'] = 1600\n",
    "FFF40['Limit'] = 200\n",
    "\n",
    "Open40 = pcropen40_115.iloc[:,:2]\n",
    "Open40['FFF'] = 0\n",
    "Open40.columns = ['r', 'señal', 'FFF']\n",
    "Open40['Area'] = 1600\n",
    "Open40['Limit'] = 200\n",
    "\n",
    "MixedCR115 = pd.concat([Open5, FFF5, Open10, FFF10, Open20, FFF20, Open40, FFF40]) \n",
    "MixedCR115['In'] = 0\n",
    "MixedCR115['Z'] = 115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mixed = pd.concat([MixedCR, MixedIN, MixedCR85, MixedIN85, MixedCR115, MixedIN115]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Mixed['r'] = np.abs(Mixed.loc[:,['r']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mixed = Mixed.loc[:,['r', 'señal', 'FFF', 'Area', 'In', 'Z']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ecuaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fluencia de los fotones se define como \n",
    "\n",
    "\\\\[ \\phi_\\gamma(x,y,z) = w_0\\phi_0 (x,y,z) \\phi^\\gamma_{horn}(x,y,z) + (1-w_0)\\phi_s(x,y,z)\\\\]\n",
    "\n",
    "donde \\\\(\\phi_o \\\\) es la fluencia producida por la fuente primaria y \\\\( \\phi_s \\\\) la producida por la fuente secundaria (\"scatter source\"). \n",
    "\n",
    "\n",
    "Todas las fluencias son tal que:\n",
    "\n",
    "\\\\[ \\phi_\\alpha (x,y,z) = Z(z; z_D^x, z_D^y, z_\\alpha) T_\\alpha(x^+_\\alpha x^-_\\alpha)T_\\alpha(y^+_\\alpha, y^-_\\alpha)\\\\]\n",
    "\n",
    "Donde\n",
    "\n",
    "\\\\[ Z(z; z_D^x, z_D^y, z_\\alpha) = \\frac{1}{4} \\frac{ (z^x_D - z_\\alpha) (z^y_D - z_\\alpha)}{ (z-z_\\alpha)^2} \\\\]\n",
    "\n",
    "y \\\\(T_0\\\\) es la fuente primaria y \\\\(T_\\alpha\\\\)\n",
    "\n",
    "\\\\[ T_0(t_0^+, t_0^-) = Q_0(\\frac{t_0^+}{\\delta_0}) + Q_0(\\frac{t^-_0}{\\delta_0}) \\\\]\n",
    "\n",
    "donde \\\\(Q_0(v) = \\frac{v}{\\sqrt{1+v^2}}\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para los casos 'free' usamos únicamente las fluencias primarias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\\[ \\phi_\\gamma(x,y,z) = w_0\\phi_0 (x,y,z) \\phi^\\gamma_{horn}(x,y,z)\\\\]\n",
    "\n",
    "Por tanto \n",
    "\n",
    "\\\\[ \\phi_\\gamma(x,y,z) = w_0\\phi_0 (x,y,z) ( 1 + \\rho^2 \\sum^4_{j=0} h^{\\gamma}_j \\rho^j )\\\\]\n",
    "\n",
    "donde \\\\[ \\rho = \\frac{\\sqrt{x^2 + y^2}}{z-z_0} \\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\\[ \\Phi_{horn}=0 \\\\]\n",
    "Por lo que nos queda \\\\[\\phi_\\gamma (x, y, z) = \\omega_0 \\phi_0(x,y,z) \\\\]\n",
    "\n",
    "Estamos suponiendo \\\\(\\omega_0\\\\) como 0\n",
    "\n",
    "\\\\[ \\phi_\\gamma (x,y,z) = \\phi_0(x,y,z) = \\frac{1}{4} \\frac{(z^x_D-z_0)(z^y_D - z_0)}{(z-z_0)^2}*Q_0(\\frac{t^+_0}{\\delta_0} + \\frac{t_0^-}{\\delta_0})  \\\\]\n",
    "\n",
    "Conocemos: \\\\( z^x_D = 50.9 ; z^y_D=42.6 \\\\), y sabiendo que hay componente x e y.\n",
    "\n",
    "\\\\[ \\phi_\\gamma (x,y,z) = \\phi_0(x,y,z) = \\frac{542.085}{z^2}*Q_0(\\frac{x^+_0}{\\delta_0} + \\frac{x_0^-}{\\delta_0}) * Q_0(\\frac{y^+_0}{\\delta_0} + \\frac{y_0^-}{\\delta_0})  \\\\]\n",
    "\n",
    "\\\\( Q_0 (v) = \\frac{v}{\\sqrt{1+v^2}} \\\\) por lo que, siendo fotones, van a la velocidad de la luz, siendo \\\\(Q_0 \\approx 1\\\\) \n",
    "\n",
    "\\\\[ \\phi_\\gamma (x,y,z) = \\phi_0(x,y,z) = \\frac{542.085}{z^2}*(\\frac{t^+_0 + t_0^-}{\\delta_0})  \\\\]\n",
    "\n",
    "t significa x o y, dependiendo del caso.\n",
    "\n",
    "\\\\[t_0^{\\pm} = min[\\frac{w_I^tz_U^t(z-z_0) \\pm 2tz_I(z_0 - z_U^t)}{2z_I(z-z_U^t)} , \\frac{w_I^tz_D^t(z-z_0) \\pm 2 t z_I(z_0-z_D^t)}{2z_I(z-z^t_D)}   ] \\\\]\n",
    "\n",
    "Conocemos: \\\\(  z_U^x = 43.1; z_U^y = 29.8 ; z_0 = 0 ; z^x_D = 50.9 ; z^y_D=42.6 \\\\)\n",
    "\n",
    "Para x:\n",
    "\n",
    "\\\\[x_0^{\\pm} = min[\\frac{w_I^x * 43.1*(z) \\pm 2xz_I*43.1}{2z_I(z-43.1)} , \\frac{w_I^x *50.9 *z \\pm 2 x z_I(-50.9)}{2z_I(z-50.9)}  ] \\\\]\n",
    "\n",
    "Para y:\n",
    "\n",
    "\\\\[y_0^{\\pm} = min[\\frac{w_I^y * 29.8*(z) \\pm 2yz_I*29.8}{2z_I(z-29.8)} , \\frac{w_I^y *42.6* z \\pm 2 y z_I(-42.6)}{2z_I(z-42.6)}  ] \\\\]\n",
    "\n",
    "\n",
    "\\\\( w_I \\\\) indican el área inicialmente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MixedF0 = Mixed[Mixed['FFF']==0].reset_index(drop=True)\n",
    "MixedF00 = MixedF0[MixedF0['señal']>0.5]\n",
    "MixedF00.reset_index(drop=True, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining T plus and T minus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z_I es lo mismo que Z?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\\\\[t_0^{\\pm} = min[\\frac{w_I^tz_U^t(z-z_0) \\pm 2tz_I(z_0 - z_U^t)}{2z_I(z-z_U^t)} , \\frac{w_I^tz_D^t(z-z_0) \\pm 2 t z_I(z_0-z_D^t)}{2z_I(z-z^t_D)}   ] \\\\]\n",
    "\n",
    "Suponemos \\\\(Z_I = Z; z_0=0\\\\)\n",
    "\n",
    "\\\\[t_0^{\\pm} = min[\\frac{w_I^tz_U^t \\pm 2t(z_0 - z_U^t)}{2(z-z_U^t)} , \\frac{w_I^tz_D^t \\pm 2 t (z_0-z_D^t)}{2z_I(z-z^t_D)}   ] \\\\]\n",
    "\n",
    "Separando en x, y, +, - y cada ima de ñas dos ecuaciones\n",
    "\n",
    "\n",
    "\n",
    "\\\\[x_0^{+} (1) = \\frac{ w_I^x * (43.1) + 2x*(-43.1)}{2(z-43.1)}   \\\\]\n",
    "\n",
    "\\\\[x_0^{+} (2) = \\frac{w_I^x *(50.9) + 2 x (-50.9)}{2(z-50.9)}  \\\\]\n",
    "\n",
    "\n",
    "\\\\[y_0^{+} (1) = \\frac{ w_I^y * (29.8) + 2y*(-29.8)}{2(z-29.8)}   \\\\]\n",
    "\n",
    "\\\\[y_0^{+} (2) = \\frac{w_I^y * (42.6) + 2 y (-42.6)}{2(z-42.6)}  \\\\]\n",
    "\n",
    "\n",
    "\\\\[x_0^{-} (1) = \\frac{ w_I^x * (43.1) - 2x*(-43.1)}{2(z-43.1)}   \\\\]\n",
    "\n",
    "\\\\[x_0^{-} (2) = \\frac{w_I^x *(50.9) - 2 x (-50.9)}{2(z-50.9)}  \\\\]\n",
    "\n",
    "\n",
    "\\\\[y_0^{-} (1) = \\frac{ w_I^y * (29.8) - 2y*(-29.8)}{2(z-29.8)}   \\\\]\n",
    "\n",
    "\\\\[y_0^{-} (2) = \\frac{w_I^y * (42.6) - 2 y (-42.6)}{2(z-42.6)}  \\\\]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tplus_1 = []\n",
    "tplus_2 = []\n",
    "\n",
    "tplus_ = []\n",
    "\n",
    "\n",
    "\n",
    "tminus_1 = []\n",
    "tminus_2 = []\n",
    "\n",
    "tminus_ = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-25-b658f6e40212>:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  MixedF00['tplus'] = tplus\n",
      "<ipython-input-25-b658f6e40212>:79: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  MixedF00['tminus'] = tminus\n",
      "<ipython-input-25-b658f6e40212>:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  MixedF00['tplus_'] = tplus_\n",
      "<ipython-input-25-b658f6e40212>:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  MixedF00['tminus_'] = tminus_\n"
     ]
    }
   ],
   "source": [
    "tplus1 = []\n",
    "tplus2 = []\n",
    "\n",
    "tplus = []\n",
    "\n",
    "\n",
    "\n",
    "tminus1 = []\n",
    "tminus2 = []\n",
    "\n",
    "tminus = []\n",
    "\n",
    "\n",
    "tplus_1 = []\n",
    "tplus_2 = []\n",
    "\n",
    "tplus_ = []\n",
    "\n",
    "\n",
    "\n",
    "tminus_1 = []\n",
    "tminus_2 = []\n",
    "\n",
    "tminus_ = []\n",
    "\n",
    "\n",
    "for k in np.arange(0, MixedF00.shape[0]):\n",
    "    \n",
    "    if MixedF00['In'][k]==1:\n",
    "        tplus1.append(((MixedF00['Area'][k] * 43.1 + 2*np.abs(MixedF00['r'][k]) * -43.1)/(2*((MixedF00['Z'][k]) - 43.1))))\n",
    "        tplus2.append(((MixedF00['Area'][k] * 50.9 + 2*np.abs(MixedF00['r'][k]) * -50.9)/(2*((MixedF00['Z'][k]) - 50.9))))\n",
    "        tminus1.append(((MixedF00['Area'][k] * 43.1 - 2*np.abs(MixedF00['r'][k]) * -43.1)/(2*((MixedF00['Z'][k]) - 43.1))))\n",
    "        tminus2.append(((MixedF00['Area'][k] * 50.9 - 2*np.abs(MixedF00['r'][k]) * -50.9)/(2*((MixedF00['Z'][k]) - 50.9))))\n",
    "        \n",
    "        tplus_1.append(((MixedF00['Area'][k] * 43.1)/(2*((MixedF00['Z'][k]) - 43.1))))\n",
    "        tplus_2.append(((MixedF00['Area'][k] * 50.9)/(2*((MixedF00['Z'][k]) - 50.9))))\n",
    "        tminus_1.append(((MixedF00['Area'][k] * 43.1)/(2*((MixedF00['Z'][k]) - 43.1))))\n",
    "        tminus_2.append(((MixedF00['Area'][k] * 50.9)/(2*((MixedF00['Z'][k]) - 50.9))))\n",
    "    else :\n",
    "        tplus1.append(((MixedF00['Area'][k] * 29.8 + 2*np.abs(MixedF00['r'][k]) * -29.8)/(2*((MixedF00['Z'][k]) - 29.8))))\n",
    "        tplus2.append(((MixedF00['Area'][k] * 42.6 + 2*np.abs(MixedF00['r'][k]) * -42.6)/(2*((MixedF00['Z'][k]) - 42.6))))\n",
    "        tminus1.append(((MixedF00['Area'][k] * 29.8 - 2*np.abs(MixedF00['r'][k]) * -29.8)/(2*((MixedF00['Z'][k]) - 29.8))))\n",
    "        tminus2.append(((MixedF00['Area'][k] * 42.6 - 2*np.abs(MixedF00['r'][k]) * -42.6)/(2*((MixedF00['Z'][k]) - 42.6))))\n",
    "\n",
    "        tplus_1.append(((MixedF00['Area'][k] * 43.1)/(2*((MixedF00['Z'][k]) - 43.1))))\n",
    "        tplus_2.append(((MixedF00['Area'][k] * 50.9)/(2*((MixedF00['Z'][k]) - 50.9))))\n",
    "        tminus_1.append(((MixedF00['Area'][k] * 43.1)/(2*((MixedF00['Z'][k]) - 43.1))))\n",
    "        tminus_2.append(((MixedF00['Area'][k] * 50.9)/(2*((MixedF00['Z'][k]) - 50.9))))\n",
    "\n",
    "        \n",
    "                \n",
    "                \n",
    "for i, j in zip(tplus1, tplus2):\n",
    "    if i <= j :\n",
    "        tplus.append(i)\n",
    "    else:\n",
    "        tplus.append(j)\n",
    "        \n",
    "for i, j in zip(tminus1, tminus2):\n",
    "    if i <= j :\n",
    "        tminus.append(i)\n",
    "    else:\n",
    "        tminus.append(j)\n",
    "        \n",
    "for i, j in zip(tplus_1, tplus_2):\n",
    "    if i <= j :\n",
    "        tplus_.append(i)\n",
    "    else:\n",
    "        tplus_.append(j)\n",
    "        \n",
    "for i, j in zip(tminus_1, tminus_2):\n",
    "    if i <= j :\n",
    "        tminus_.append(i)\n",
    "    else:\n",
    "        tminus_.append(j)\n",
    "\n",
    "        \n",
    "MixedF00['tplus'] = tplus \n",
    "MixedF00['tminus'] = tminus \n",
    "MixedF00['tplus_'] = tplus_\n",
    "MixedF00['tminus_'] = tminus_  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining Phi0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\\[ \\phi_\\gamma (x,y,z) = \\phi_0(x,y,z) = \\frac{542.085}{z^2}*(\\frac{x^+_0 + x_0^-}{\\delta_0})*(\\frac{y^+_0 + y_0^-}{\\delta_0})  \\\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-8b12198e1642>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  MixedF00['phi0 minus delta'] = ((542.085/(MixedF00['Z'].to_numpy())**2))*(MixedF00['tplus'].to_numpy()+MixedF00['tminus'].to_numpy())*(MixedF00['tplus_'].to_numpy()+MixedF00['tminus_'].to_numpy())\n"
     ]
    }
   ],
   "source": [
    "MixedF00['phi0 minus delta'] = ((542.085/(MixedF00['Z'].to_numpy())**2))*(MixedF00['tplus'].to_numpy()+MixedF00['tminus'].to_numpy())*(MixedF00['tplus_'].to_numpy()+MixedF00['tminus_'].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-38677b307b4c>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  MixedF00['Rho'] = np.ravel(Rho)\n"
     ]
    }
   ],
   "source": [
    "Rho = []\n",
    "Rho.append(MixedF00['r']/MixedF00['Z'])\n",
    "\n",
    "MixedF00['Rho'] = np.ravel(Rho)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate through possible values of phi_0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1441\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1520\n",
      "1521\n",
      "1522\n",
      "1523\n",
      "1524\n",
      "1525\n",
      "1526\n",
      "1527\n",
      "1528\n",
      "1529\n",
      "1530\n",
      "1531\n",
      "1532\n",
      "1533\n",
      "1534\n",
      "1535\n",
      "1536\n",
      "1537\n",
      "1538\n",
      "1539\n",
      "1540\n",
      "1541\n",
      "1542\n",
      "1543\n",
      "1544\n",
      "1545\n",
      "1546\n",
      "1547\n",
      "1548\n",
      "1549\n",
      "1550\n",
      "1551\n",
      "1552\n",
      "1553\n",
      "1554\n",
      "1555\n",
      "1556\n",
      "1557\n",
      "1558\n",
      "1559\n",
      "1560\n",
      "1561\n",
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1566\n",
      "1567\n",
      "1568\n",
      "1569\n",
      "1570\n",
      "1571\n",
      "1572\n",
      "1573\n",
      "1574\n",
      "1575\n",
      "1576\n",
      "1577\n",
      "1578\n",
      "1579\n",
      "1580\n",
      "1581\n",
      "1582\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1586\n",
      "1587\n",
      "1588\n",
      "1589\n",
      "1590\n",
      "1591\n",
      "1592\n",
      "1593\n",
      "1594\n",
      "1595\n",
      "1596\n",
      "1597\n",
      "1598\n",
      "1599\n",
      "1600\n",
      "1601\n",
      "1602\n",
      "1603\n",
      "1604\n",
      "1605\n",
      "1606\n",
      "1607\n",
      "1608\n",
      "1609\n",
      "1610\n",
      "1611\n",
      "1612\n",
      "1613\n",
      "1614\n",
      "1615\n",
      "1616\n",
      "1617\n",
      "1618\n",
      "1619\n",
      "1620\n",
      "1621\n",
      "1622\n",
      "1623\n",
      "1624\n",
      "1625\n",
      "1626\n",
      "1627\n",
      "1628\n",
      "1629\n",
      "1630\n",
      "1631\n",
      "1632\n",
      "1633\n",
      "1634\n",
      "1635\n",
      "1636\n",
      "1637\n",
      "1638\n",
      "1639\n",
      "1640\n",
      "1641\n",
      "1642\n",
      "1643\n",
      "1644\n",
      "1645\n",
      "1646\n",
      "1647\n",
      "1648\n",
      "1649\n",
      "1650\n",
      "1651\n",
      "1652\n",
      "1653\n",
      "1654\n",
      "1655\n",
      "1656\n",
      "1657\n",
      "1658\n",
      "1659\n",
      "1660\n",
      "1661\n",
      "1662\n",
      "1663\n",
      "1664\n",
      "1665\n",
      "1666\n",
      "1667\n",
      "1668\n",
      "1669\n",
      "1670\n",
      "1671\n",
      "1672\n",
      "1673\n",
      "1674\n",
      "1675\n",
      "1676\n",
      "1677\n",
      "1678\n",
      "1679\n",
      "1680\n",
      "1681\n",
      "1682\n",
      "1683\n",
      "1684\n",
      "1685\n",
      "1686\n",
      "1687\n",
      "1688\n",
      "1689\n",
      "1690\n",
      "1691\n",
      "1692\n",
      "1693\n",
      "1694\n",
      "1695\n",
      "1696\n",
      "1697\n",
      "1698\n",
      "1699\n",
      "1700\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1704\n",
      "1705\n",
      "1706\n",
      "1707\n",
      "1708\n",
      "1709\n",
      "1710\n",
      "1711\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n",
      "1719\n",
      "1720\n",
      "1721\n",
      "1722\n",
      "1723\n",
      "1724\n",
      "1725\n",
      "1726\n",
      "1727\n",
      "1728\n",
      "1729\n",
      "1730\n",
      "1731\n",
      "1732\n",
      "1733\n",
      "1734\n",
      "1735\n",
      "1736\n",
      "1737\n",
      "1738\n",
      "1739\n",
      "1740\n",
      "1741\n",
      "1742\n",
      "1743\n",
      "1744\n",
      "1745\n",
      "1746\n",
      "1747\n",
      "1748\n",
      "1749\n",
      "1750\n",
      "1751\n",
      "1752\n",
      "1753\n",
      "1754\n",
      "1755\n",
      "1756\n",
      "1757\n",
      "1758\n",
      "1759\n",
      "1760\n",
      "1761\n",
      "1762\n",
      "1763\n",
      "1764\n",
      "1765\n",
      "1766\n",
      "1767\n",
      "1768\n",
      "1769\n",
      "1770\n",
      "1771\n",
      "1772\n",
      "1773\n",
      "1774\n",
      "1775\n",
      "1776\n",
      "1777\n",
      "1778\n",
      "1779\n",
      "1780\n",
      "1781\n",
      "1782\n",
      "1783\n",
      "1784\n",
      "1785\n",
      "1786\n",
      "1787\n",
      "1788\n",
      "1789\n",
      "1790\n",
      "1791\n",
      "1792\n",
      "1793\n",
      "1794\n",
      "1795\n",
      "1796\n",
      "1797\n",
      "1798\n",
      "1799\n",
      "1800\n",
      "1801\n",
      "1802\n",
      "1803\n",
      "1804\n",
      "1805\n",
      "1806\n",
      "1807\n",
      "1808\n",
      "1809\n",
      "1810\n",
      "1811\n",
      "1812\n",
      "1813\n",
      "1814\n",
      "1815\n",
      "1816\n",
      "1817\n",
      "1818\n",
      "1819\n",
      "1820\n",
      "1821\n",
      "1822\n",
      "1823\n",
      "1824\n",
      "1825\n",
      "1826\n",
      "1827\n",
      "1828\n",
      "1829\n",
      "1830\n",
      "1831\n",
      "1832\n",
      "1833\n",
      "1834\n",
      "1835\n",
      "1836\n",
      "1837\n",
      "1838\n",
      "1839\n",
      "1840\n",
      "1841\n",
      "1842\n",
      "1843\n",
      "1844\n",
      "1845\n",
      "1846\n",
      "1847\n",
      "1848\n",
      "1849\n",
      "1850\n",
      "1851\n",
      "1852\n",
      "1853\n",
      "1854\n",
      "1855\n",
      "1856\n",
      "1857\n",
      "1858\n",
      "1859\n",
      "1860\n",
      "1861\n",
      "1862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1863\n",
      "1864\n",
      "1865\n",
      "1866\n",
      "1867\n",
      "1868\n",
      "1869\n",
      "1870\n",
      "1871\n",
      "1872\n",
      "1873\n",
      "1874\n",
      "1875\n",
      "1876\n",
      "1877\n",
      "1878\n",
      "1879\n",
      "1880\n",
      "1881\n",
      "1882\n",
      "1883\n",
      "1884\n",
      "1885\n",
      "1886\n",
      "1887\n",
      "1888\n",
      "1889\n",
      "1890\n",
      "1891\n",
      "1892\n",
      "1893\n",
      "1894\n",
      "1895\n",
      "1896\n",
      "1897\n",
      "1898\n",
      "1899\n",
      "1900\n",
      "1901\n",
      "1902\n",
      "1903\n",
      "1904\n",
      "1905\n",
      "1906\n",
      "1907\n",
      "1908\n",
      "1909\n",
      "1910\n",
      "1911\n",
      "1912\n",
      "1913\n",
      "1914\n",
      "1915\n",
      "1916\n",
      "1917\n",
      "1918\n",
      "1919\n",
      "1920\n",
      "1921\n",
      "1922\n",
      "1923\n",
      "1924\n",
      "1925\n",
      "1926\n",
      "1927\n",
      "1928\n",
      "1929\n",
      "1930\n",
      "1931\n",
      "1932\n",
      "1933\n",
      "1934\n",
      "1935\n",
      "1936\n",
      "1937\n",
      "1938\n",
      "1939\n",
      "1940\n",
      "1941\n",
      "1942\n",
      "1943\n",
      "1944\n",
      "1945\n",
      "1946\n",
      "1947\n",
      "1948\n",
      "1949\n",
      "1950\n",
      "1951\n",
      "1952\n",
      "1953\n",
      "1954\n",
      "1955\n",
      "1956\n",
      "1957\n",
      "1958\n",
      "1959\n",
      "1960\n",
      "1961\n",
      "1962\n",
      "1963\n",
      "1964\n",
      "1965\n",
      "1966\n",
      "1967\n",
      "1968\n",
      "1969\n",
      "1970\n",
      "1971\n",
      "1972\n",
      "1973\n",
      "1974\n",
      "1975\n",
      "1976\n",
      "1977\n",
      "1978\n",
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n",
      "2024\n",
      "2025\n",
      "2026\n",
      "2027\n",
      "2028\n",
      "2029\n",
      "2030\n",
      "2031\n",
      "2032\n",
      "2033\n",
      "2034\n",
      "2035\n",
      "2036\n",
      "2037\n",
      "2038\n",
      "2039\n",
      "2040\n",
      "2041\n",
      "2042\n",
      "2043\n",
      "2044\n",
      "2045\n",
      "2046\n",
      "2047\n",
      "2048\n",
      "2049\n",
      "2050\n",
      "2051\n",
      "2052\n",
      "2053\n",
      "2054\n",
      "2055\n",
      "2056\n",
      "2057\n",
      "2058\n",
      "2059\n",
      "2060\n",
      "2061\n",
      "2062\n",
      "2063\n",
      "2064\n",
      "2065\n",
      "2066\n",
      "2067\n",
      "2068\n",
      "2069\n",
      "2070\n",
      "2071\n",
      "2072\n",
      "2073\n",
      "2074\n",
      "2075\n",
      "2076\n",
      "2077\n",
      "2078\n",
      "2079\n",
      "2080\n",
      "2081\n",
      "2082\n",
      "2083\n",
      "2084\n",
      "2085\n",
      "2086\n",
      "2087\n",
      "2088\n",
      "2089\n",
      "2090\n",
      "2091\n",
      "2092\n",
      "2093\n",
      "2094\n",
      "2095\n",
      "2096\n",
      "2097\n",
      "2098\n",
      "2099\n",
      "2100\n",
      "2101\n",
      "2102\n",
      "2103\n",
      "2104\n",
      "2105\n",
      "2106\n",
      "2107\n",
      "2108\n",
      "2109\n",
      "2110\n",
      "2111\n",
      "2112\n",
      "2113\n",
      "2114\n",
      "2115\n",
      "2116\n",
      "2117\n",
      "2118\n",
      "2119\n",
      "2120\n",
      "2121\n",
      "2122\n",
      "2123\n",
      "2124\n",
      "2125\n",
      "2126\n",
      "2127\n",
      "2128\n",
      "2129\n",
      "2130\n",
      "2131\n",
      "2132\n",
      "2133\n",
      "2134\n",
      "2135\n",
      "2136\n",
      "2137\n",
      "2138\n",
      "2139\n",
      "2140\n",
      "2141\n",
      "2142\n",
      "2143\n",
      "2144\n",
      "2145\n",
      "2146\n",
      "2147\n",
      "2148\n",
      "2149\n",
      "2150\n",
      "2151\n",
      "2152\n",
      "2153\n",
      "2154\n",
      "2155\n",
      "2156\n",
      "2157\n",
      "2158\n",
      "2159\n",
      "2160\n",
      "2161\n",
      "2162\n",
      "2163\n",
      "2164\n",
      "2165\n",
      "2166\n",
      "2167\n",
      "2168\n",
      "2169\n",
      "2170\n",
      "2171\n",
      "2172\n",
      "2173\n",
      "2174\n",
      "2175\n",
      "2176\n",
      "2177\n",
      "2178\n",
      "2179\n",
      "2180\n",
      "2181\n",
      "2182\n",
      "2183\n",
      "2184\n",
      "2185\n",
      "2186\n",
      "2187\n",
      "2188\n",
      "2189\n",
      "2190\n",
      "2191\n",
      "2192\n",
      "2193\n",
      "2194\n",
      "2195\n",
      "2196\n",
      "2197\n",
      "2198\n",
      "2199\n",
      "2200\n",
      "2201\n",
      "2202\n",
      "2203\n",
      "2204\n",
      "2205\n",
      "2206\n",
      "2207\n",
      "2208\n",
      "2209\n",
      "2210\n",
      "2211\n",
      "2212\n",
      "2213\n",
      "2214\n",
      "2215\n",
      "2216\n",
      "2217\n",
      "2218\n",
      "2219\n",
      "2220\n",
      "2221\n",
      "2222\n",
      "2223\n",
      "2224\n",
      "2225\n",
      "2226\n",
      "2227\n",
      "2228\n",
      "2229\n",
      "2230\n",
      "2231\n",
      "2232\n",
      "2233\n",
      "2234\n",
      "2235\n",
      "2236\n",
      "2237\n",
      "2238\n",
      "2239\n",
      "2240\n",
      "2241\n",
      "2242\n",
      "2243\n",
      "2244\n",
      "2245\n",
      "2246\n",
      "2247\n",
      "2248\n",
      "2249\n",
      "2250\n",
      "2251\n",
      "2252\n",
      "2253\n",
      "2254\n",
      "2255\n",
      "2256\n",
      "2257\n",
      "2258\n",
      "2259\n",
      "2260\n",
      "2261\n",
      "2262\n",
      "2263\n",
      "2264\n",
      "2265\n",
      "2266\n",
      "2267\n",
      "2268\n",
      "2269\n",
      "2270\n",
      "2271\n",
      "2272\n",
      "2273\n",
      "2274\n",
      "2275\n",
      "2276\n",
      "2277\n",
      "2278\n",
      "2279\n",
      "2280\n",
      "2281\n",
      "2282\n",
      "2283\n",
      "2284\n",
      "2285\n",
      "2286\n",
      "2287\n",
      "2288\n",
      "2289\n",
      "2290\n",
      "2291\n",
      "2292\n",
      "2293\n",
      "2294\n",
      "2295\n",
      "2296\n",
      "2297\n",
      "2298\n",
      "2299\n",
      "2300\n",
      "2301\n",
      "2302\n",
      "2303\n",
      "2304\n",
      "2305\n",
      "2306\n",
      "2307\n",
      "2308\n",
      "2309\n",
      "2310\n",
      "2311\n",
      "2312\n",
      "2313\n",
      "2314\n",
      "2315\n",
      "2316\n",
      "2317\n",
      "2318\n",
      "2319\n",
      "2320\n",
      "2321\n",
      "2322\n",
      "2323\n",
      "2324\n",
      "2325\n",
      "2326\n",
      "2327\n",
      "2328\n",
      "2329\n",
      "2330\n",
      "2331\n",
      "2332\n",
      "2333\n",
      "2334\n",
      "2335\n",
      "2336\n",
      "2337\n",
      "2338\n",
      "2339\n",
      "2340\n",
      "2341\n",
      "2342\n",
      "2343\n",
      "2344\n",
      "2345\n",
      "2346\n",
      "2347\n",
      "2348\n",
      "2349\n",
      "2350\n",
      "2351\n",
      "2352\n",
      "2353\n",
      "2354\n",
      "2355\n",
      "2356\n",
      "2357\n",
      "2358\n",
      "2359\n",
      "2360\n",
      "2361\n",
      "2362\n",
      "2363\n",
      "2364\n",
      "2365\n",
      "2366\n",
      "2367\n",
      "2368\n",
      "2369\n",
      "2370\n",
      "2371\n",
      "2372\n",
      "2373\n",
      "2374\n",
      "2375\n",
      "2376\n",
      "2377\n",
      "2378\n",
      "2379\n",
      "2380\n",
      "2381\n",
      "2382\n",
      "2383\n",
      "2384\n",
      "2385\n",
      "2386\n",
      "2387\n",
      "2388\n",
      "2389\n",
      "2390\n",
      "2391\n",
      "2392\n",
      "2393\n",
      "2394\n",
      "2395\n",
      "2396\n",
      "2397\n",
      "2398\n",
      "2399\n",
      "2400\n",
      "2401\n",
      "2402\n",
      "2403\n",
      "2404\n",
      "2405\n",
      "2406\n",
      "2407\n",
      "2408\n",
      "2409\n",
      "2410\n",
      "2411\n",
      "2412\n",
      "2413\n",
      "2414\n",
      "2415\n",
      "2416\n",
      "2417\n",
      "2418\n",
      "2419\n",
      "2420\n",
      "2421\n",
      "2422\n",
      "2423\n",
      "2424\n",
      "2425\n",
      "2426\n",
      "2427\n",
      "2428\n",
      "2429\n",
      "2430\n",
      "2431\n",
      "2432\n",
      "2433\n",
      "2434\n",
      "2435\n",
      "2436\n",
      "2437\n",
      "2438\n",
      "2439\n",
      "2440\n",
      "2441\n",
      "2442\n",
      "2443\n",
      "2444\n",
      "2445\n",
      "2446\n",
      "2447\n",
      "2448\n",
      "2449\n",
      "2450\n",
      "2451\n",
      "2452\n",
      "2453\n",
      "2454\n",
      "2455\n",
      "2456\n",
      "2457\n",
      "2458\n",
      "2459\n",
      "2460\n",
      "2461\n",
      "2462\n",
      "2463\n",
      "2464\n",
      "2465\n",
      "2466\n",
      "2467\n",
      "2468\n",
      "2469\n",
      "2470\n",
      "2471\n",
      "2472\n",
      "2473\n",
      "2474\n",
      "2475\n",
      "2476\n",
      "2477\n",
      "2478\n",
      "2479\n",
      "2480\n",
      "2481\n",
      "2482\n",
      "2483\n",
      "2484\n",
      "2485\n",
      "2486\n",
      "2487\n",
      "2488\n",
      "2489\n",
      "2490\n",
      "2491\n",
      "2492\n",
      "2493\n",
      "2494\n",
      "2495\n",
      "2496\n",
      "2497\n",
      "2498\n",
      "2499\n",
      "2500\n",
      "2501\n",
      "2502\n",
      "2503\n",
      "2504\n",
      "2505\n",
      "2506\n",
      "2507\n",
      "2508\n",
      "2509\n",
      "2510\n",
      "2511\n",
      "2512\n",
      "2513\n",
      "2514\n",
      "2515\n",
      "2516\n",
      "2517\n",
      "2518\n",
      "2519\n",
      "2520\n",
      "2521\n",
      "2522\n",
      "2523\n",
      "2524\n",
      "2525\n",
      "2526\n",
      "2527\n",
      "2528\n",
      "2529\n",
      "2530\n",
      "2531\n",
      "2532\n",
      "2533\n",
      "2534\n",
      "2535\n",
      "2536\n",
      "2537\n",
      "2538\n",
      "2539\n",
      "2540\n",
      "2541\n",
      "2542\n",
      "2543\n",
      "2544\n",
      "2545\n",
      "2546\n",
      "2547\n",
      "2548\n",
      "2549\n",
      "2550\n",
      "2551\n",
      "2552\n",
      "2553\n",
      "2554\n",
      "2555\n",
      "2556\n",
      "2557\n",
      "2558\n",
      "2559\n",
      "2560\n",
      "2561\n",
      "2562\n",
      "2563\n",
      "2564\n",
      "2565\n",
      "2566\n",
      "2567\n",
      "2568\n",
      "2569\n",
      "2570\n",
      "2571\n",
      "2572\n",
      "2573\n",
      "2574\n",
      "2575\n",
      "2576\n",
      "2577\n",
      "2578\n",
      "2579\n",
      "2580\n",
      "2581\n",
      "2582\n",
      "2583\n",
      "2584\n",
      "2585\n",
      "2586\n",
      "2587\n",
      "2588\n",
      "2589\n",
      "2590\n",
      "2591\n",
      "2592\n",
      "2593\n",
      "2594\n",
      "2595\n",
      "2596\n",
      "2597\n",
      "2598\n",
      "2599\n",
      "2600\n",
      "2601\n",
      "2602\n",
      "2603\n",
      "2604\n",
      "2605\n",
      "2606\n",
      "2607\n",
      "2608\n",
      "2609\n",
      "2610\n",
      "2611\n",
      "2612\n",
      "2613\n",
      "2614\n",
      "2615\n",
      "2616\n",
      "2617\n",
      "2618\n",
      "2619\n",
      "2620\n",
      "2621\n",
      "2622\n",
      "2623\n",
      "2624\n",
      "2625\n",
      "2626\n",
      "2627\n",
      "2628\n",
      "2629\n",
      "2630\n",
      "2631\n",
      "2632\n",
      "2633\n",
      "2634\n",
      "2635\n",
      "2636\n",
      "2637\n",
      "2638\n",
      "2639\n",
      "2640\n",
      "2641\n",
      "2642\n",
      "2643\n",
      "2644\n",
      "2645\n",
      "2646\n",
      "2647\n",
      "2648\n",
      "2649\n",
      "2650\n",
      "2651\n",
      "2652\n",
      "2653\n",
      "2654\n",
      "2655\n",
      "2656\n",
      "2657\n",
      "2658\n",
      "2659\n",
      "2660\n",
      "2661\n",
      "2662\n",
      "2663\n",
      "2664\n",
      "2665\n",
      "2666\n",
      "2667\n",
      "2668\n",
      "2669\n",
      "2670\n",
      "2671\n",
      "2672\n",
      "2673\n",
      "2674\n",
      "2675\n",
      "2676\n",
      "2677\n",
      "2678\n",
      "2679\n",
      "2680\n",
      "2681\n",
      "2682\n",
      "2683\n",
      "2684\n",
      "2685\n",
      "2686\n",
      "2687\n",
      "2688\n",
      "2689\n",
      "2690\n",
      "2691\n",
      "2692\n",
      "2693\n",
      "2694\n",
      "2695\n",
      "2696\n",
      "2697\n",
      "2698\n",
      "2699\n",
      "2700\n",
      "2701\n",
      "2702\n",
      "2703\n",
      "2704\n",
      "2705\n",
      "2706\n",
      "2707\n",
      "2708\n",
      "2709\n",
      "2710\n",
      "2711\n",
      "2712\n",
      "2713\n",
      "2714\n",
      "2715\n",
      "2716\n",
      "2717\n",
      "2718\n",
      "2719\n",
      "2720\n",
      "2721\n",
      "2722\n",
      "2723\n",
      "2724\n",
      "2725\n",
      "2726\n",
      "2727\n",
      "2728\n",
      "2729\n",
      "2730\n",
      "2731\n",
      "2732\n",
      "2733\n",
      "2734\n",
      "2735\n",
      "2736\n",
      "2737\n",
      "2738\n",
      "2739\n",
      "2740\n",
      "2741\n",
      "2742\n",
      "2743\n",
      "2744\n",
      "2745\n",
      "2746\n",
      "2747\n",
      "2748\n",
      "2749\n",
      "2750\n",
      "2751\n",
      "2752\n",
      "2753\n",
      "2754\n",
      "2755\n",
      "2756\n",
      "2757\n",
      "2758\n",
      "2759\n",
      "2760\n",
      "2761\n",
      "2762\n",
      "2763\n",
      "2764\n",
      "2765\n",
      "2766\n",
      "2767\n",
      "2768\n",
      "2769\n",
      "2770\n",
      "2771\n",
      "2772\n",
      "2773\n",
      "2774\n",
      "2775\n",
      "2776\n",
      "2777\n",
      "2778\n",
      "2779\n",
      "2780\n",
      "2781\n",
      "2782\n",
      "2783\n",
      "2784\n",
      "2785\n",
      "2786\n",
      "2787\n",
      "2788\n",
      "2789\n",
      "2790\n",
      "2791\n",
      "2792\n",
      "2793\n",
      "2794\n",
      "2795\n",
      "2796\n",
      "2797\n",
      "2798\n",
      "2799\n",
      "2800\n",
      "2801\n",
      "2802\n",
      "2803\n",
      "2804\n",
      "2805\n",
      "2806\n",
      "2807\n",
      "2808\n",
      "2809\n",
      "2810\n",
      "2811\n",
      "2812\n",
      "2813\n",
      "2814\n",
      "2815\n",
      "2816\n",
      "2817\n",
      "2818\n",
      "2819\n",
      "2820\n",
      "2821\n",
      "2822\n",
      "2823\n",
      "2824\n",
      "2825\n",
      "2826\n",
      "2827\n",
      "2828\n",
      "2829\n",
      "2830\n",
      "2831\n",
      "2832\n",
      "2833\n",
      "2834\n",
      "2835\n",
      "2836\n",
      "2837\n",
      "2838\n",
      "2839\n",
      "2840\n",
      "2841\n",
      "2842\n",
      "2843\n",
      "2844\n",
      "2845\n",
      "2846\n",
      "2847\n",
      "2848\n",
      "2849\n",
      "2850\n",
      "2851\n",
      "2852\n",
      "2853\n",
      "2854\n",
      "2855\n",
      "2856\n",
      "2857\n",
      "2858\n",
      "2859\n",
      "2860\n",
      "2861\n",
      "2862\n",
      "2863\n",
      "2864\n",
      "2865\n",
      "2866\n",
      "2867\n",
      "2868\n",
      "2869\n",
      "2870\n",
      "2871\n",
      "2872\n",
      "2873\n",
      "2874\n",
      "2875\n",
      "2876\n",
      "2877\n",
      "2878\n",
      "2879\n",
      "2880\n",
      "2881\n",
      "2882\n",
      "2883\n",
      "2884\n",
      "2885\n",
      "2886\n",
      "2887\n",
      "2888\n",
      "2889\n",
      "2890\n",
      "2891\n",
      "2892\n",
      "2893\n",
      "2894\n",
      "2895\n",
      "2896\n",
      "2897\n",
      "2898\n",
      "2899\n",
      "2900\n",
      "2901\n",
      "2902\n",
      "2903\n",
      "2904\n",
      "2905\n",
      "2906\n",
      "2907\n",
      "2908\n",
      "2909\n",
      "2910\n",
      "2911\n",
      "2912\n",
      "2913\n",
      "2914\n",
      "2915\n",
      "2916\n",
      "2917\n",
      "2918\n",
      "2919\n",
      "2920\n",
      "2921\n",
      "2922\n",
      "2923\n",
      "2924\n",
      "2925\n",
      "2926\n",
      "2927\n",
      "2928\n",
      "2929\n",
      "2930\n",
      "2931\n",
      "2932\n",
      "2933\n",
      "2934\n",
      "2935\n",
      "2936\n",
      "2937\n",
      "2938\n",
      "2939\n",
      "2940\n",
      "2941\n",
      "2942\n",
      "2943\n",
      "2944\n",
      "2945\n",
      "2946\n",
      "2947\n",
      "2948\n",
      "2949\n",
      "2950\n",
      "2951\n",
      "2952\n",
      "2953\n",
      "2954\n",
      "2955\n",
      "2956\n",
      "2957\n",
      "2958\n",
      "2959\n",
      "2960\n",
      "2961\n",
      "2962\n",
      "2963\n",
      "2964\n",
      "2965\n",
      "2966\n",
      "2967\n",
      "2968\n",
      "2969\n",
      "2970\n",
      "2971\n",
      "2972\n",
      "2973\n",
      "2974\n",
      "2975\n",
      "2976\n",
      "2977\n",
      "2978\n",
      "2979\n",
      "2980\n",
      "2981\n",
      "2982\n",
      "2983\n",
      "2984\n",
      "2985\n",
      "2986\n",
      "2987\n",
      "2988\n",
      "2989\n",
      "2990\n",
      "2991\n",
      "2992\n",
      "2993\n",
      "2994\n",
      "2995\n",
      "2996\n",
      "2997\n",
      "2998\n",
      "2999\n",
      "3000\n",
      "3001\n",
      "3002\n",
      "3003\n",
      "3004\n",
      "3005\n",
      "3006\n",
      "3007\n",
      "3008\n",
      "3009\n",
      "3010\n",
      "3011\n",
      "3012\n",
      "3013\n",
      "3014\n",
      "3015\n",
      "3016\n",
      "3017\n",
      "3018\n",
      "3019\n",
      "3020\n",
      "3021\n",
      "3022\n",
      "3023\n",
      "3024\n",
      "3025\n",
      "3026\n",
      "3027\n",
      "3028\n",
      "3029\n",
      "3030\n",
      "3031\n",
      "3032\n",
      "3033\n",
      "3034\n",
      "3035\n",
      "3036\n",
      "3037\n",
      "3038\n",
      "3039\n",
      "3040\n",
      "3041\n",
      "3042\n",
      "3043\n",
      "3044\n",
      "3045\n",
      "3046\n",
      "3047\n",
      "3048\n",
      "3049\n",
      "3050\n",
      "3051\n",
      "3052\n",
      "3053\n",
      "3054\n",
      "3055\n",
      "3056\n",
      "3057\n",
      "3058\n",
      "3059\n",
      "3060\n",
      "3061\n",
      "3062\n",
      "3063\n",
      "3064\n",
      "3065\n",
      "3066\n",
      "3067\n",
      "3068\n",
      "3069\n",
      "3070\n",
      "3071\n",
      "3072\n",
      "3073\n",
      "3074\n",
      "3075\n",
      "3076\n",
      "3077\n",
      "3078\n",
      "3079\n",
      "3080\n",
      "3081\n",
      "3082\n",
      "3083\n",
      "3084\n",
      "3085\n",
      "3086\n",
      "3087\n",
      "3088\n",
      "3089\n",
      "3090\n",
      "3091\n",
      "3092\n",
      "3093\n",
      "3094\n",
      "3095\n",
      "3096\n",
      "3097\n",
      "3098\n",
      "3099\n",
      "3100\n",
      "3101\n",
      "3102\n",
      "3103\n",
      "3104\n",
      "3105\n",
      "3106\n",
      "3107\n",
      "3108\n",
      "3109\n",
      "3110\n",
      "3111\n",
      "3112\n",
      "3113\n",
      "3114\n",
      "3115\n",
      "3116\n",
      "3117\n",
      "3118\n",
      "3119\n",
      "3120\n",
      "3121\n",
      "3122\n",
      "3123\n",
      "3124\n",
      "3125\n",
      "3126\n",
      "3127\n",
      "3128\n",
      "3129\n",
      "3130\n",
      "3131\n",
      "3132\n",
      "3133\n",
      "3134\n",
      "3135\n",
      "3136\n",
      "3137\n",
      "3138\n",
      "3139\n",
      "3140\n",
      "3141\n",
      "3142\n",
      "3143\n",
      "3144\n",
      "3145\n",
      "3146\n",
      "3147\n",
      "3148\n",
      "3149\n",
      "3150\n",
      "3151\n",
      "3152\n",
      "3153\n",
      "3154\n",
      "3155\n",
      "3156\n",
      "3157\n",
      "3158\n",
      "3159\n",
      "3160\n",
      "3161\n",
      "3162\n",
      "3163\n",
      "3164\n",
      "3165\n",
      "3166\n",
      "3167\n",
      "3168\n",
      "3169\n",
      "3170\n",
      "3171\n",
      "3172\n",
      "3173\n",
      "3174\n",
      "3175\n",
      "3176\n",
      "3177\n",
      "3178\n",
      "3179\n",
      "3180\n",
      "3181\n",
      "3182\n",
      "3183\n",
      "3184\n",
      "3185\n",
      "3186\n",
      "3187\n",
      "3188\n",
      "3189\n",
      "3190\n",
      "3191\n",
      "3192\n",
      "3193\n",
      "3194\n",
      "3195\n",
      "3196\n",
      "3197\n",
      "3198\n",
      "3199\n",
      "3200\n",
      "3201\n",
      "3202\n",
      "3203\n",
      "3204\n",
      "3205\n",
      "3206\n",
      "3207\n",
      "3208\n",
      "3209\n",
      "3210\n",
      "3211\n",
      "3212\n",
      "3213\n",
      "3214\n",
      "3215\n",
      "3216\n",
      "3217\n",
      "3218\n",
      "3219\n",
      "3220\n",
      "3221\n",
      "3222\n",
      "3223\n",
      "3224\n",
      "3225\n",
      "3226\n",
      "3227\n",
      "3228\n",
      "3229\n",
      "3230\n",
      "3231\n",
      "3232\n",
      "3233\n",
      "3234\n",
      "3235\n",
      "3236\n",
      "3237\n",
      "3238\n",
      "3239\n",
      "3240\n",
      "3241\n",
      "3242\n",
      "3243\n",
      "3244\n",
      "3245\n",
      "3246\n",
      "3247\n",
      "3248\n",
      "3249\n",
      "3250\n",
      "3251\n",
      "3252\n",
      "3253\n",
      "3254\n",
      "3255\n",
      "3256\n",
      "3257\n",
      "3258\n",
      "3259\n",
      "3260\n",
      "3261\n",
      "3262\n",
      "3263\n",
      "3264\n",
      "3265\n",
      "3266\n",
      "3267\n",
      "3268\n",
      "3269\n",
      "3270\n",
      "3271\n",
      "3272\n",
      "3273\n",
      "3274\n",
      "3275\n",
      "3276\n",
      "3277\n",
      "3278\n",
      "3279\n",
      "3280\n",
      "3281\n",
      "3282\n",
      "3283\n",
      "3284\n",
      "3285\n",
      "3286\n",
      "3287\n",
      "3288\n",
      "3289\n",
      "3290\n",
      "3291\n",
      "3292\n",
      "3293\n",
      "3294\n",
      "3295\n",
      "3296\n",
      "3297\n",
      "3298\n",
      "3299\n",
      "3300\n",
      "3301\n",
      "3302\n",
      "3303\n",
      "3304\n",
      "3305\n",
      "3306\n",
      "3307\n",
      "3308\n",
      "3309\n",
      "3310\n",
      "3311\n",
      "3312\n",
      "3313\n",
      "3314\n",
      "3315\n",
      "3316\n",
      "3317\n",
      "3318\n",
      "3319\n",
      "3320\n",
      "3321\n",
      "3322\n",
      "3323\n",
      "3324\n",
      "3325\n",
      "3326\n",
      "3327\n",
      "3328\n",
      "3329\n",
      "3330\n",
      "3331\n",
      "3332\n",
      "3333\n",
      "3334\n",
      "3335\n",
      "3336\n",
      "3337\n",
      "3338\n",
      "3339\n",
      "3340\n",
      "3341\n",
      "3342\n",
      "3343\n",
      "3344\n",
      "3345\n",
      "3346\n",
      "3347\n",
      "3348\n",
      "3349\n",
      "3350\n",
      "3351\n",
      "3352\n",
      "3353\n",
      "3354\n",
      "3355\n",
      "3356\n",
      "3357\n",
      "3358\n",
      "3359\n",
      "3360\n",
      "3361\n",
      "3362\n",
      "3363\n",
      "3364\n",
      "3365\n",
      "3366\n",
      "3367\n",
      "3368\n",
      "3369\n",
      "3370\n",
      "3371\n",
      "3372\n",
      "3373\n",
      "3374\n",
      "3375\n",
      "3376\n",
      "3377\n",
      "3378\n",
      "3379\n",
      "3380\n",
      "3381\n",
      "3382\n",
      "3383\n",
      "3384\n",
      "3385\n",
      "3386\n",
      "3387\n",
      "3388\n",
      "3389\n",
      "3390\n",
      "3391\n",
      "3392\n",
      "3393\n",
      "3394\n",
      "3395\n",
      "3396\n",
      "3397\n",
      "3398\n",
      "3399\n",
      "3400\n",
      "3401\n",
      "3402\n",
      "3403\n",
      "3404\n",
      "3405\n",
      "3406\n",
      "3407\n",
      "3408\n",
      "3409\n",
      "3410\n",
      "3411\n",
      "3412\n",
      "3413\n",
      "3414\n",
      "3415\n",
      "3416\n",
      "3417\n",
      "3418\n",
      "3419\n",
      "3420\n",
      "3421\n",
      "3422\n",
      "3423\n",
      "3424\n",
      "3425\n",
      "3426\n",
      "3427\n",
      "3428\n",
      "3429\n",
      "3430\n",
      "3431\n",
      "3432\n",
      "3433\n",
      "3434\n",
      "3435\n",
      "3436\n",
      "3437\n",
      "3438\n",
      "3439\n",
      "3440\n",
      "3441\n",
      "3442\n",
      "3443\n",
      "3444\n",
      "3445\n",
      "3446\n",
      "3447\n",
      "3448\n",
      "3449\n",
      "3450\n",
      "3451\n",
      "3452\n",
      "3453\n",
      "3454\n",
      "3455\n",
      "3456\n",
      "3457\n",
      "3458\n",
      "3459\n",
      "3460\n",
      "3461\n",
      "3462\n",
      "3463\n",
      "3464\n",
      "3465\n",
      "3466\n",
      "3467\n",
      "3468\n",
      "3469\n",
      "3470\n",
      "3471\n",
      "3472\n",
      "3473\n",
      "3474\n",
      "3475\n",
      "3476\n",
      "3477\n",
      "3478\n",
      "3479\n",
      "3480\n",
      "3481\n",
      "3482\n",
      "3483\n",
      "3484\n",
      "3485\n",
      "3486\n",
      "3487\n",
      "3488\n",
      "3489\n",
      "3490\n",
      "3491\n",
      "3492\n",
      "3493\n",
      "3494\n",
      "3495\n",
      "3496\n",
      "3497\n",
      "3498\n",
      "3499\n",
      "3500\n",
      "3501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3502\n",
      "3503\n",
      "3504\n",
      "3505\n",
      "3506\n",
      "3507\n",
      "3508\n",
      "3509\n",
      "3510\n",
      "3511\n",
      "3512\n",
      "3513\n",
      "3514\n",
      "3515\n",
      "3516\n",
      "3517\n",
      "3518\n",
      "3519\n",
      "3520\n",
      "3521\n",
      "3522\n",
      "3523\n",
      "3524\n",
      "3525\n",
      "3526\n",
      "3527\n",
      "3528\n",
      "3529\n",
      "3530\n",
      "3531\n",
      "3532\n",
      "3533\n",
      "3534\n",
      "3535\n",
      "3536\n",
      "3537\n",
      "3538\n",
      "3539\n",
      "3540\n",
      "3541\n",
      "3542\n",
      "3543\n",
      "3544\n",
      "3545\n",
      "3546\n",
      "3547\n",
      "3548\n",
      "3549\n",
      "3550\n",
      "3551\n",
      "3552\n",
      "3553\n",
      "3554\n",
      "3555\n",
      "3556\n",
      "3557\n",
      "3558\n",
      "3559\n",
      "3560\n",
      "3561\n",
      "3562\n",
      "3563\n",
      "3564\n",
      "3565\n",
      "3566\n",
      "3567\n",
      "3568\n",
      "3569\n",
      "3570\n",
      "3571\n",
      "3572\n",
      "3573\n",
      "3574\n",
      "3575\n",
      "3576\n",
      "3577\n",
      "3578\n",
      "3579\n",
      "3580\n",
      "3581\n",
      "3582\n",
      "3583\n",
      "3584\n",
      "3585\n",
      "3586\n",
      "3587\n",
      "3588\n",
      "3589\n",
      "3590\n",
      "3591\n",
      "3592\n",
      "3593\n",
      "3594\n",
      "3595\n",
      "3596\n",
      "3597\n",
      "3598\n",
      "3599\n",
      "3600\n",
      "3601\n",
      "3602\n",
      "3603\n",
      "3604\n",
      "3605\n",
      "3606\n",
      "3607\n",
      "3608\n",
      "3609\n",
      "3610\n",
      "3611\n",
      "3612\n",
      "3613\n",
      "3614\n",
      "3615\n",
      "3616\n",
      "3617\n",
      "3618\n",
      "3619\n",
      "3620\n",
      "3621\n",
      "3622\n",
      "3623\n",
      "3624\n",
      "3625\n",
      "3626\n",
      "3627\n",
      "3628\n",
      "3629\n",
      "3630\n",
      "3631\n",
      "3632\n",
      "3633\n",
      "3634\n",
      "3635\n",
      "3636\n",
      "3637\n",
      "3638\n",
      "3639\n",
      "3640\n",
      "3641\n",
      "3642\n",
      "3643\n",
      "3644\n",
      "3645\n",
      "3646\n",
      "3647\n",
      "3648\n",
      "3649\n",
      "3650\n",
      "3651\n",
      "3652\n",
      "3653\n",
      "3654\n",
      "3655\n",
      "3656\n",
      "3657\n",
      "3658\n",
      "3659\n",
      "3660\n",
      "3661\n",
      "3662\n",
      "3663\n",
      "3664\n",
      "3665\n",
      "3666\n",
      "3667\n",
      "3668\n",
      "3669\n",
      "3670\n",
      "3671\n",
      "3672\n",
      "3673\n",
      "3674\n",
      "3675\n",
      "3676\n",
      "3677\n",
      "3678\n",
      "3679\n",
      "3680\n",
      "3681\n",
      "3682\n",
      "3683\n",
      "3684\n",
      "3685\n",
      "3686\n",
      "3687\n",
      "3688\n",
      "3689\n",
      "3690\n",
      "3691\n",
      "3692\n",
      "3693\n",
      "3694\n",
      "3695\n",
      "3696\n",
      "3697\n",
      "3698\n",
      "3699\n",
      "3700\n",
      "3701\n",
      "3702\n",
      "3703\n",
      "3704\n",
      "3705\n",
      "3706\n",
      "3707\n",
      "3708\n",
      "3709\n",
      "3710\n",
      "3711\n",
      "3712\n",
      "3713\n",
      "3714\n",
      "3715\n",
      "3716\n",
      "3717\n",
      "3718\n",
      "3719\n",
      "3720\n",
      "3721\n",
      "3722\n",
      "3723\n",
      "3724\n",
      "3725\n",
      "3726\n",
      "3727\n",
      "3728\n",
      "3729\n",
      "3730\n",
      "3731\n",
      "3732\n",
      "3733\n",
      "3734\n",
      "3735\n",
      "3736\n",
      "3737\n",
      "3738\n",
      "3739\n",
      "3740\n",
      "3741\n",
      "3742\n",
      "3743\n",
      "3744\n",
      "3745\n",
      "3746\n",
      "3747\n",
      "3748\n",
      "3749\n",
      "3750\n",
      "3751\n",
      "3752\n",
      "3753\n",
      "3754\n",
      "3755\n",
      "3756\n",
      "3757\n",
      "3758\n",
      "3759\n",
      "3760\n",
      "3761\n",
      "3762\n",
      "3763\n",
      "3764\n",
      "3765\n",
      "3766\n",
      "3767\n",
      "3768\n",
      "3769\n",
      "3770\n",
      "3771\n",
      "3772\n",
      "3773\n",
      "3774\n",
      "3775\n",
      "3776\n",
      "3777\n",
      "3778\n",
      "3779\n",
      "3780\n",
      "3781\n",
      "3782\n",
      "3783\n",
      "3784\n",
      "3785\n",
      "3786\n",
      "3787\n",
      "3788\n",
      "3789\n",
      "3790\n",
      "3791\n",
      "3792\n",
      "3793\n",
      "3794\n",
      "3795\n",
      "3796\n",
      "3797\n",
      "3798\n",
      "3799\n",
      "3800\n",
      "3801\n",
      "3802\n",
      "3803\n",
      "3804\n",
      "3805\n",
      "3806\n",
      "3807\n",
      "3808\n",
      "3809\n",
      "3810\n",
      "3811\n",
      "3812\n",
      "3813\n",
      "3814\n",
      "3815\n",
      "3816\n",
      "3817\n",
      "3818\n",
      "3819\n",
      "3820\n",
      "3821\n",
      "3822\n",
      "3823\n",
      "3824\n",
      "3825\n",
      "3826\n",
      "3827\n",
      "3828\n",
      "3829\n",
      "3830\n",
      "3831\n",
      "3832\n",
      "3833\n",
      "3834\n",
      "3835\n",
      "3836\n",
      "3837\n",
      "3838\n",
      "3839\n",
      "3840\n",
      "3841\n",
      "3842\n",
      "3843\n",
      "3844\n",
      "3845\n",
      "3846\n",
      "3847\n",
      "3848\n",
      "3849\n",
      "3850\n",
      "3851\n",
      "3852\n",
      "3853\n",
      "3854\n",
      "3855\n",
      "3856\n",
      "3857\n",
      "3858\n",
      "3859\n",
      "3860\n",
      "3861\n",
      "3862\n",
      "3863\n",
      "3864\n",
      "3865\n",
      "3866\n",
      "3867\n",
      "3868\n",
      "3869\n",
      "3870\n",
      "3871\n",
      "3872\n",
      "3873\n",
      "3874\n",
      "3875\n",
      "3876\n",
      "3877\n",
      "3878\n",
      "3879\n",
      "3880\n",
      "3881\n",
      "3882\n",
      "3883\n",
      "3884\n",
      "3885\n",
      "3886\n",
      "3887\n",
      "3888\n",
      "3889\n",
      "3890\n",
      "3891\n",
      "3892\n",
      "3893\n",
      "3894\n",
      "3895\n",
      "3896\n",
      "3897\n",
      "3898\n",
      "3899\n",
      "3900\n",
      "3901\n",
      "3902\n",
      "3903\n",
      "3904\n",
      "3905\n",
      "3906\n",
      "3907\n",
      "3908\n",
      "3909\n",
      "3910\n",
      "3911\n",
      "3912\n",
      "3913\n",
      "3914\n",
      "3915\n",
      "3916\n",
      "3917\n",
      "3918\n",
      "3919\n",
      "3920\n",
      "3921\n",
      "3922\n",
      "3923\n",
      "3924\n",
      "3925\n",
      "3926\n",
      "3927\n",
      "3928\n",
      "3929\n",
      "3930\n",
      "3931\n",
      "3932\n",
      "3933\n",
      "3934\n",
      "3935\n",
      "3936\n",
      "3937\n",
      "3938\n",
      "3939\n",
      "3940\n",
      "3941\n",
      "3942\n",
      "3943\n",
      "3944\n",
      "3945\n",
      "3946\n",
      "3947\n",
      "3948\n",
      "3949\n",
      "3950\n",
      "3951\n",
      "3952\n",
      "3953\n",
      "3954\n",
      "3955\n",
      "3956\n",
      "3957\n",
      "3958\n",
      "3959\n",
      "3960\n",
      "3961\n",
      "3962\n",
      "3963\n",
      "3964\n",
      "3965\n",
      "3966\n",
      "3967\n",
      "3968\n",
      "3969\n",
      "3970\n",
      "3971\n",
      "3972\n",
      "3973\n",
      "3974\n",
      "3975\n",
      "3976\n",
      "3977\n",
      "3978\n",
      "3979\n",
      "3980\n",
      "3981\n",
      "3982\n",
      "3983\n",
      "3984\n",
      "3985\n",
      "3986\n",
      "3987\n",
      "3988\n",
      "3989\n",
      "3990\n",
      "3991\n",
      "3992\n",
      "3993\n",
      "3994\n",
      "3995\n",
      "3996\n",
      "3997\n",
      "3998\n",
      "3999\n",
      "4000\n",
      "4001\n",
      "4002\n",
      "4003\n",
      "4004\n",
      "4005\n",
      "4006\n",
      "4007\n",
      "4008\n",
      "4009\n",
      "4010\n",
      "4011\n",
      "4012\n",
      "4013\n",
      "4014\n",
      "4015\n",
      "4016\n",
      "4017\n",
      "4018\n",
      "4019\n",
      "4020\n",
      "4021\n",
      "4022\n",
      "4023\n",
      "4024\n",
      "4025\n",
      "4026\n",
      "4027\n",
      "4028\n",
      "4029\n",
      "4030\n",
      "4031\n",
      "4032\n",
      "4033\n",
      "4034\n",
      "4035\n",
      "4036\n",
      "4037\n",
      "4038\n",
      "4039\n",
      "4040\n",
      "4041\n",
      "4042\n",
      "4043\n",
      "4044\n",
      "4045\n",
      "4046\n",
      "4047\n",
      "4048\n",
      "4049\n",
      "4050\n",
      "4051\n",
      "4052\n",
      "4053\n",
      "4054\n",
      "4055\n",
      "4056\n",
      "4057\n",
      "4058\n",
      "4059\n",
      "4060\n",
      "4061\n",
      "4062\n",
      "4063\n",
      "4064\n",
      "4065\n",
      "4066\n",
      "4067\n",
      "4068\n",
      "4069\n",
      "4070\n",
      "4071\n",
      "4072\n",
      "4073\n",
      "4074\n",
      "4075\n",
      "4076\n",
      "4077\n",
      "4078\n",
      "4079\n",
      "4080\n",
      "4081\n",
      "4082\n",
      "4083\n",
      "4084\n",
      "4085\n",
      "4086\n",
      "4087\n",
      "4088\n",
      "4089\n",
      "4090\n",
      "4091\n",
      "4092\n",
      "4093\n",
      "4094\n",
      "4095\n",
      "4096\n",
      "4097\n",
      "4098\n",
      "4099\n",
      "4100\n",
      "4101\n",
      "4102\n",
      "4103\n",
      "4104\n",
      "4105\n",
      "4106\n",
      "4107\n",
      "4108\n",
      "4109\n",
      "4110\n",
      "4111\n",
      "4112\n",
      "4113\n",
      "4114\n",
      "4115\n",
      "4116\n",
      "4117\n",
      "4118\n",
      "4119\n",
      "4120\n",
      "4121\n",
      "4122\n",
      "4123\n",
      "4124\n",
      "4125\n",
      "4126\n",
      "4127\n",
      "4128\n",
      "4129\n",
      "4130\n",
      "4131\n",
      "4132\n",
      "4133\n",
      "4134\n",
      "4135\n",
      "4136\n",
      "4137\n",
      "4138\n",
      "4139\n",
      "4140\n",
      "4141\n",
      "4142\n",
      "4143\n",
      "4144\n",
      "4145\n",
      "4146\n",
      "4147\n",
      "4148\n",
      "4149\n",
      "4150\n",
      "4151\n",
      "4152\n",
      "4153\n",
      "4154\n",
      "4155\n",
      "4156\n",
      "4157\n",
      "4158\n",
      "4159\n",
      "4160\n",
      "4161\n",
      "4162\n",
      "4163\n",
      "4164\n",
      "4165\n",
      "4166\n",
      "4167\n",
      "4168\n",
      "4169\n",
      "4170\n",
      "4171\n",
      "4172\n",
      "4173\n",
      "4174\n",
      "4175\n",
      "4176\n",
      "4177\n",
      "4178\n",
      "4179\n",
      "4180\n",
      "4181\n",
      "4182\n",
      "4183\n",
      "4184\n",
      "4185\n",
      "4186\n",
      "4187\n",
      "4188\n",
      "4189\n",
      "4190\n",
      "4191\n",
      "4192\n",
      "4193\n",
      "4194\n",
      "4195\n",
      "4196\n",
      "4197\n",
      "4198\n",
      "4199\n",
      "4200\n",
      "4201\n",
      "4202\n",
      "4203\n",
      "4204\n",
      "4205\n",
      "4206\n",
      "4207\n",
      "4208\n",
      "4209\n",
      "4210\n",
      "4211\n",
      "4212\n",
      "4213\n",
      "4214\n",
      "4215\n",
      "4216\n",
      "4217\n",
      "4218\n",
      "4219\n",
      "4220\n",
      "4221\n",
      "4222\n",
      "4223\n",
      "4224\n",
      "4225\n",
      "4226\n",
      "4227\n",
      "4228\n",
      "4229\n",
      "4230\n",
      "4231\n",
      "4232\n",
      "4233\n",
      "4234\n",
      "4235\n",
      "4236\n",
      "4237\n",
      "4238\n",
      "4239\n",
      "4240\n",
      "4241\n",
      "4242\n",
      "4243\n",
      "4244\n",
      "4245\n",
      "4246\n",
      "4247\n",
      "4248\n",
      "4249\n",
      "4250\n",
      "4251\n",
      "4252\n",
      "4253\n",
      "4254\n",
      "4255\n",
      "4256\n",
      "4257\n",
      "4258\n",
      "4259\n",
      "4260\n",
      "4261\n",
      "4262\n",
      "4263\n",
      "4264\n",
      "4265\n",
      "4266\n",
      "4267\n",
      "4268\n",
      "4269\n",
      "4270\n",
      "4271\n",
      "4272\n",
      "4273\n",
      "4274\n",
      "4275\n",
      "4276\n",
      "4277\n",
      "4278\n",
      "4279\n",
      "4280\n",
      "4281\n",
      "4282\n",
      "4283\n",
      "4284\n",
      "4285\n",
      "4286\n",
      "4287\n",
      "4288\n",
      "4289\n",
      "4290\n",
      "4291\n",
      "4292\n",
      "4293\n",
      "4294\n",
      "4295\n",
      "4296\n",
      "4297\n",
      "4298\n",
      "4299\n",
      "4300\n",
      "4301\n",
      "4302\n",
      "4303\n",
      "4304\n",
      "4305\n",
      "4306\n",
      "4307\n",
      "4308\n",
      "4309\n",
      "4310\n",
      "4311\n",
      "4312\n",
      "4313\n",
      "4314\n",
      "4315\n",
      "4316\n",
      "4317\n",
      "4318\n",
      "4319\n",
      "4320\n",
      "4321\n",
      "4322\n",
      "4323\n",
      "4324\n",
      "4325\n",
      "4326\n",
      "4327\n",
      "4328\n",
      "4329\n",
      "4330\n",
      "4331\n",
      "4332\n",
      "4333\n",
      "4334\n",
      "4335\n",
      "4336\n",
      "4337\n",
      "4338\n",
      "4339\n",
      "4340\n",
      "4341\n",
      "4342\n",
      "4343\n",
      "4344\n",
      "4345\n",
      "4346\n",
      "4347\n",
      "4348\n",
      "4349\n",
      "4350\n",
      "4351\n",
      "4352\n",
      "4353\n",
      "4354\n",
      "4355\n",
      "4356\n",
      "4357\n",
      "4358\n",
      "4359\n",
      "4360\n",
      "4361\n",
      "4362\n",
      "4363\n",
      "4364\n",
      "4365\n",
      "4366\n",
      "4367\n",
      "4368\n",
      "4369\n",
      "4370\n",
      "4371\n",
      "4372\n",
      "4373\n",
      "4374\n",
      "4375\n",
      "4376\n",
      "4377\n",
      "4378\n",
      "4379\n",
      "4380\n",
      "4381\n",
      "4382\n",
      "4383\n",
      "4384\n",
      "4385\n",
      "4386\n",
      "4387\n",
      "4388\n",
      "4389\n",
      "4390\n",
      "4391\n",
      "4392\n",
      "4393\n",
      "4394\n",
      "4395\n",
      "4396\n",
      "4397\n",
      "4398\n",
      "4399\n",
      "4400\n",
      "4401\n",
      "4402\n",
      "4403\n",
      "4404\n",
      "4405\n",
      "4406\n",
      "4407\n",
      "4408\n",
      "4409\n",
      "4410\n",
      "4411\n",
      "4412\n",
      "4413\n",
      "4414\n",
      "4415\n",
      "4416\n",
      "4417\n",
      "4418\n",
      "4419\n",
      "4420\n",
      "4421\n",
      "4422\n",
      "4423\n",
      "4424\n",
      "4425\n",
      "4426\n",
      "4427\n",
      "4428\n",
      "4429\n",
      "4430\n",
      "4431\n",
      "4432\n",
      "4433\n",
      "4434\n",
      "4435\n",
      "4436\n",
      "4437\n",
      "4438\n",
      "4439\n",
      "4440\n",
      "4441\n",
      "4442\n",
      "4443\n",
      "4444\n",
      "4445\n",
      "4446\n",
      "4447\n",
      "4448\n",
      "4449\n",
      "4450\n",
      "4451\n",
      "4452\n",
      "4453\n",
      "4454\n",
      "4455\n",
      "4456\n",
      "4457\n",
      "4458\n",
      "4459\n",
      "4460\n",
      "4461\n",
      "4462\n",
      "4463\n",
      "4464\n",
      "4465\n",
      "4466\n",
      "4467\n",
      "4468\n",
      "4469\n",
      "4470\n",
      "4471\n",
      "4472\n",
      "4473\n",
      "4474\n",
      "4475\n",
      "4476\n",
      "4477\n",
      "4478\n",
      "4479\n",
      "4480\n",
      "4481\n",
      "4482\n",
      "4483\n",
      "4484\n",
      "4485\n",
      "4486\n",
      "4487\n",
      "4488\n",
      "4489\n",
      "4490\n",
      "4491\n",
      "4492\n",
      "4493\n",
      "4494\n",
      "4495\n",
      "4496\n",
      "4497\n",
      "4498\n",
      "4499\n",
      "4500\n",
      "4501\n",
      "4502\n",
      "4503\n",
      "4504\n",
      "4505\n",
      "4506\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "RF = RandomForestRegressor()\n",
    "\n",
    "\n",
    "ScoresRF = []\n",
    "for i in np.arange(6, NormX.shape[1]):\n",
    "\n",
    "    x = NormX.iloc[:,[0, 1, 2, 3, 4, 5, 6, i]]\n",
    "    print(i)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "\n",
    "    RF.fit(x_train, y_train)\n",
    "    ScoresRF.append((mean_absolute_error(RF.predict(x_test), y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3687</th>\n",
       "      <td>0.024230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1527</th>\n",
       "      <td>0.024336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0.024851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.024976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2160</th>\n",
       "      <td>0.025586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.068846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3541</th>\n",
       "      <td>0.069190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304</th>\n",
       "      <td>0.071054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>0.073336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>0.073455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4501 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0\n",
       "3687  0.024230\n",
       "1527  0.024336\n",
       "169   0.024851\n",
       "499   0.024976\n",
       "2160  0.025586\n",
       "...        ...\n",
       "55    0.068846\n",
       "3541  0.069190\n",
       "1304  0.071054\n",
       "380   0.073336\n",
       "1259  0.073455\n",
       "\n",
       "[4501 rows x 1 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(ScoresRF).sort_values(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>r</th>\n",
       "      <td>0.117868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Area</th>\n",
       "      <td>0.124614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>In</th>\n",
       "      <td>0.001572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z</th>\n",
       "      <td>0.091142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tplus</th>\n",
       "      <td>0.069607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tminus</th>\n",
       "      <td>0.036081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tminus_</th>\n",
       "      <td>0.156825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analytic 0.4999000000000129</th>\n",
       "      <td>0.402291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    0\n",
       "r                            0.117868\n",
       "Area                         0.124614\n",
       "In                           0.001572\n",
       "Z                            0.091142\n",
       "tplus                        0.069607\n",
       "tminus                       0.036081\n",
       "tminus_                      0.156825\n",
       "analytic 0.4999000000000129  0.402291"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(RF.feature_importances_, index=x_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1441\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1520\n",
      "1521\n",
      "1522\n",
      "1523\n",
      "1524\n",
      "1525\n",
      "1526\n",
      "1527\n",
      "1528\n",
      "1529\n",
      "1530\n",
      "1531\n",
      "1532\n",
      "1533\n",
      "1534\n",
      "1535\n",
      "1536\n",
      "1537\n",
      "1538\n",
      "1539\n",
      "1540\n",
      "1541\n",
      "1542\n",
      "1543\n",
      "1544\n",
      "1545\n",
      "1546\n",
      "1547\n",
      "1548\n",
      "1549\n",
      "1550\n",
      "1551\n",
      "1552\n",
      "1553\n",
      "1554\n",
      "1555\n",
      "1556\n",
      "1557\n",
      "1558\n",
      "1559\n",
      "1560\n",
      "1561\n",
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1566\n",
      "1567\n",
      "1568\n",
      "1569\n",
      "1570\n",
      "1571\n",
      "1572\n",
      "1573\n",
      "1574\n",
      "1575\n",
      "1576\n",
      "1577\n",
      "1578\n",
      "1579\n",
      "1580\n",
      "1581\n",
      "1582\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1586\n",
      "1587\n",
      "1588\n",
      "1589\n",
      "1590\n",
      "1591\n",
      "1592\n",
      "1593\n",
      "1594\n",
      "1595\n",
      "1596\n",
      "1597\n",
      "1598\n",
      "1599\n",
      "1600\n",
      "1601\n",
      "1602\n",
      "1603\n",
      "1604\n",
      "1605\n",
      "1606\n",
      "1607\n",
      "1608\n",
      "1609\n",
      "1610\n",
      "1611\n",
      "1612\n",
      "1613\n",
      "1614\n",
      "1615\n",
      "1616\n",
      "1617\n",
      "1618\n",
      "1619\n",
      "1620\n",
      "1621\n",
      "1622\n",
      "1623\n",
      "1624\n",
      "1625\n",
      "1626\n",
      "1627\n",
      "1628\n",
      "1629\n",
      "1630\n",
      "1631\n",
      "1632\n",
      "1633\n",
      "1634\n",
      "1635\n",
      "1636\n",
      "1637\n",
      "1638\n",
      "1639\n",
      "1640\n",
      "1641\n",
      "1642\n",
      "1643\n",
      "1644\n",
      "1645\n",
      "1646\n",
      "1647\n",
      "1648\n",
      "1649\n",
      "1650\n",
      "1651\n",
      "1652\n",
      "1653\n",
      "1654\n",
      "1655\n",
      "1656\n",
      "1657\n",
      "1658\n",
      "1659\n",
      "1660\n",
      "1661\n",
      "1662\n",
      "1663\n",
      "1664\n",
      "1665\n",
      "1666\n",
      "1667\n",
      "1668\n",
      "1669\n",
      "1670\n",
      "1671\n",
      "1672\n",
      "1673\n",
      "1674\n",
      "1675\n",
      "1676\n",
      "1677\n",
      "1678\n",
      "1679\n",
      "1680\n",
      "1681\n",
      "1682\n",
      "1683\n",
      "1684\n",
      "1685\n",
      "1686\n",
      "1687\n",
      "1688\n",
      "1689\n",
      "1690\n",
      "1691\n",
      "1692\n",
      "1693\n",
      "1694\n",
      "1695\n",
      "1696\n",
      "1697\n",
      "1698\n",
      "1699\n",
      "1700\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1704\n",
      "1705\n",
      "1706\n",
      "1707\n",
      "1708\n",
      "1709\n",
      "1710\n",
      "1711\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n",
      "1719\n",
      "1720\n",
      "1721\n",
      "1722\n",
      "1723\n",
      "1724\n",
      "1725\n",
      "1726\n",
      "1727\n",
      "1728\n",
      "1729\n",
      "1730\n",
      "1731\n",
      "1732\n",
      "1733\n",
      "1734\n",
      "1735\n",
      "1736\n",
      "1737\n",
      "1738\n",
      "1739\n",
      "1740\n",
      "1741\n",
      "1742\n",
      "1743\n",
      "1744\n",
      "1745\n",
      "1746\n",
      "1747\n",
      "1748\n",
      "1749\n",
      "1750\n",
      "1751\n",
      "1752\n",
      "1753\n",
      "1754\n",
      "1755\n",
      "1756\n",
      "1757\n",
      "1758\n",
      "1759\n",
      "1760\n",
      "1761\n",
      "1762\n",
      "1763\n",
      "1764\n",
      "1765\n",
      "1766\n",
      "1767\n",
      "1768\n",
      "1769\n",
      "1770\n",
      "1771\n",
      "1772\n",
      "1773\n",
      "1774\n",
      "1775\n",
      "1776\n",
      "1777\n",
      "1778\n",
      "1779\n",
      "1780\n",
      "1781\n",
      "1782\n",
      "1783\n",
      "1784\n",
      "1785\n",
      "1786\n",
      "1787\n",
      "1788\n",
      "1789\n",
      "1790\n",
      "1791\n",
      "1792\n",
      "1793\n",
      "1794\n",
      "1795\n",
      "1796\n",
      "1797\n",
      "1798\n",
      "1799\n",
      "1800\n",
      "1801\n",
      "1802\n",
      "1803\n",
      "1804\n",
      "1805\n",
      "1806\n",
      "1807\n",
      "1808\n",
      "1809\n",
      "1810\n",
      "1811\n",
      "1812\n",
      "1813\n",
      "1814\n",
      "1815\n",
      "1816\n",
      "1817\n",
      "1818\n",
      "1819\n",
      "1820\n",
      "1821\n",
      "1822\n",
      "1823\n",
      "1824\n",
      "1825\n",
      "1826\n",
      "1827\n",
      "1828\n",
      "1829\n",
      "1830\n",
      "1831\n",
      "1832\n",
      "1833\n",
      "1834\n",
      "1835\n",
      "1836\n",
      "1837\n",
      "1838\n",
      "1839\n",
      "1840\n",
      "1841\n",
      "1842\n",
      "1843\n",
      "1844\n",
      "1845\n",
      "1846\n",
      "1847\n",
      "1848\n",
      "1849\n",
      "1850\n",
      "1851\n",
      "1852\n",
      "1853\n",
      "1854\n",
      "1855\n",
      "1856\n",
      "1857\n",
      "1858\n",
      "1859\n",
      "1860\n",
      "1861\n",
      "1862\n",
      "1863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1864\n",
      "1865\n",
      "1866\n",
      "1867\n",
      "1868\n",
      "1869\n",
      "1870\n",
      "1871\n",
      "1872\n",
      "1873\n",
      "1874\n",
      "1875\n",
      "1876\n",
      "1877\n",
      "1878\n",
      "1879\n",
      "1880\n",
      "1881\n",
      "1882\n",
      "1883\n",
      "1884\n",
      "1885\n",
      "1886\n",
      "1887\n",
      "1888\n",
      "1889\n",
      "1890\n",
      "1891\n",
      "1892\n",
      "1893\n",
      "1894\n",
      "1895\n",
      "1896\n",
      "1897\n",
      "1898\n",
      "1899\n",
      "1900\n",
      "1901\n",
      "1902\n",
      "1903\n",
      "1904\n",
      "1905\n",
      "1906\n",
      "1907\n",
      "1908\n",
      "1909\n",
      "1910\n",
      "1911\n",
      "1912\n",
      "1913\n",
      "1914\n",
      "1915\n",
      "1916\n",
      "1917\n",
      "1918\n",
      "1919\n",
      "1920\n",
      "1921\n",
      "1922\n",
      "1923\n",
      "1924\n",
      "1925\n",
      "1926\n",
      "1927\n",
      "1928\n",
      "1929\n",
      "1930\n",
      "1931\n",
      "1932\n",
      "1933\n",
      "1934\n",
      "1935\n",
      "1936\n",
      "1937\n",
      "1938\n",
      "1939\n",
      "1940\n",
      "1941\n",
      "1942\n",
      "1943\n",
      "1944\n",
      "1945\n",
      "1946\n",
      "1947\n",
      "1948\n",
      "1949\n",
      "1950\n",
      "1951\n",
      "1952\n",
      "1953\n",
      "1954\n",
      "1955\n",
      "1956\n",
      "1957\n",
      "1958\n",
      "1959\n",
      "1960\n",
      "1961\n",
      "1962\n",
      "1963\n",
      "1964\n",
      "1965\n",
      "1966\n",
      "1967\n",
      "1968\n",
      "1969\n",
      "1970\n",
      "1971\n",
      "1972\n",
      "1973\n",
      "1974\n",
      "1975\n",
      "1976\n",
      "1977\n",
      "1978\n",
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n",
      "2024\n",
      "2025\n",
      "2026\n",
      "2027\n",
      "2028\n",
      "2029\n",
      "2030\n",
      "2031\n",
      "2032\n",
      "2033\n",
      "2034\n",
      "2035\n",
      "2036\n",
      "2037\n",
      "2038\n",
      "2039\n",
      "2040\n",
      "2041\n",
      "2042\n",
      "2043\n",
      "2044\n",
      "2045\n",
      "2046\n",
      "2047\n",
      "2048\n",
      "2049\n",
      "2050\n",
      "2051\n",
      "2052\n",
      "2053\n",
      "2054\n",
      "2055\n",
      "2056\n",
      "2057\n",
      "2058\n",
      "2059\n",
      "2060\n",
      "2061\n",
      "2062\n",
      "2063\n",
      "2064\n",
      "2065\n",
      "2066\n",
      "2067\n",
      "2068\n",
      "2069\n",
      "2070\n",
      "2071\n",
      "2072\n",
      "2073\n",
      "2074\n",
      "2075\n",
      "2076\n",
      "2077\n",
      "2078\n",
      "2079\n",
      "2080\n",
      "2081\n",
      "2082\n",
      "2083\n",
      "2084\n",
      "2085\n",
      "2086\n",
      "2087\n",
      "2088\n",
      "2089\n",
      "2090\n",
      "2091\n",
      "2092\n",
      "2093\n",
      "2094\n",
      "2095\n",
      "2096\n",
      "2097\n",
      "2098\n",
      "2099\n",
      "2100\n",
      "2101\n",
      "2102\n",
      "2103\n",
      "2104\n",
      "2105\n",
      "2106\n",
      "2107\n",
      "2108\n",
      "2109\n",
      "2110\n",
      "2111\n",
      "2112\n",
      "2113\n",
      "2114\n",
      "2115\n",
      "2116\n",
      "2117\n",
      "2118\n",
      "2119\n",
      "2120\n",
      "2121\n",
      "2122\n",
      "2123\n",
      "2124\n",
      "2125\n",
      "2126\n",
      "2127\n",
      "2128\n",
      "2129\n",
      "2130\n",
      "2131\n",
      "2132\n",
      "2133\n",
      "2134\n",
      "2135\n",
      "2136\n",
      "2137\n",
      "2138\n",
      "2139\n",
      "2140\n",
      "2141\n",
      "2142\n",
      "2143\n",
      "2144\n",
      "2145\n",
      "2146\n",
      "2147\n",
      "2148\n",
      "2149\n",
      "2150\n",
      "2151\n",
      "2152\n",
      "2153\n",
      "2154\n",
      "2155\n",
      "2156\n",
      "2157\n",
      "2158\n",
      "2159\n",
      "2160\n",
      "2161\n",
      "2162\n",
      "2163\n",
      "2164\n",
      "2165\n",
      "2166\n",
      "2167\n",
      "2168\n",
      "2169\n",
      "2170\n",
      "2171\n",
      "2172\n",
      "2173\n",
      "2174\n",
      "2175\n",
      "2176\n",
      "2177\n",
      "2178\n",
      "2179\n",
      "2180\n",
      "2181\n",
      "2182\n",
      "2183\n",
      "2184\n",
      "2185\n",
      "2186\n",
      "2187\n",
      "2188\n",
      "2189\n",
      "2190\n",
      "2191\n",
      "2192\n",
      "2193\n",
      "2194\n",
      "2195\n",
      "2196\n",
      "2197\n",
      "2198\n",
      "2199\n",
      "2200\n",
      "2201\n",
      "2202\n",
      "2203\n",
      "2204\n",
      "2205\n",
      "2206\n",
      "2207\n",
      "2208\n",
      "2209\n",
      "2210\n",
      "2211\n",
      "2212\n",
      "2213\n",
      "2214\n",
      "2215\n",
      "2216\n",
      "2217\n",
      "2218\n",
      "2219\n",
      "2220\n",
      "2221\n",
      "2222\n",
      "2223\n",
      "2224\n",
      "2225\n",
      "2226\n",
      "2227\n",
      "2228\n",
      "2229\n",
      "2230\n",
      "2231\n",
      "2232\n",
      "2233\n",
      "2234\n",
      "2235\n",
      "2236\n",
      "2237\n",
      "2238\n",
      "2239\n",
      "2240\n",
      "2241\n",
      "2242\n",
      "2243\n",
      "2244\n",
      "2245\n",
      "2246\n",
      "2247\n",
      "2248\n",
      "2249\n",
      "2250\n",
      "2251\n",
      "2252\n",
      "2253\n",
      "2254\n",
      "2255\n",
      "2256\n",
      "2257\n",
      "2258\n",
      "2259\n",
      "2260\n",
      "2261\n",
      "2262\n",
      "2263\n",
      "2264\n",
      "2265\n",
      "2266\n",
      "2267\n",
      "2268\n",
      "2269\n",
      "2270\n",
      "2271\n",
      "2272\n",
      "2273\n",
      "2274\n",
      "2275\n",
      "2276\n",
      "2277\n",
      "2278\n",
      "2279\n",
      "2280\n",
      "2281\n",
      "2282\n",
      "2283\n",
      "2284\n",
      "2285\n",
      "2286\n",
      "2287\n",
      "2288\n",
      "2289\n",
      "2290\n",
      "2291\n",
      "2292\n",
      "2293\n",
      "2294\n",
      "2295\n",
      "2296\n",
      "2297\n",
      "2298\n",
      "2299\n",
      "2300\n",
      "2301\n",
      "2302\n",
      "2303\n",
      "2304\n",
      "2305\n",
      "2306\n",
      "2307\n",
      "2308\n",
      "2309\n",
      "2310\n",
      "2311\n",
      "2312\n",
      "2313\n",
      "2314\n",
      "2315\n",
      "2316\n",
      "2317\n",
      "2318\n",
      "2319\n",
      "2320\n",
      "2321\n",
      "2322\n",
      "2323\n",
      "2324\n",
      "2325\n",
      "2326\n",
      "2327\n",
      "2328\n",
      "2329\n",
      "2330\n",
      "2331\n",
      "2332\n",
      "2333\n",
      "2334\n",
      "2335\n",
      "2336\n",
      "2337\n",
      "2338\n",
      "2339\n",
      "2340\n",
      "2341\n",
      "2342\n",
      "2343\n",
      "2344\n",
      "2345\n",
      "2346\n",
      "2347\n",
      "2348\n",
      "2349\n",
      "2350\n",
      "2351\n",
      "2352\n",
      "2353\n",
      "2354\n",
      "2355\n",
      "2356\n",
      "2357\n",
      "2358\n",
      "2359\n",
      "2360\n",
      "2361\n",
      "2362\n",
      "2363\n",
      "2364\n",
      "2365\n",
      "2366\n",
      "2367\n",
      "2368\n",
      "2369\n",
      "2370\n",
      "2371\n",
      "2372\n",
      "2373\n",
      "2374\n",
      "2375\n",
      "2376\n",
      "2377\n",
      "2378\n",
      "2379\n",
      "2380\n",
      "2381\n",
      "2382\n",
      "2383\n",
      "2384\n",
      "2385\n",
      "2386\n",
      "2387\n",
      "2388\n",
      "2389\n",
      "2390\n",
      "2391\n",
      "2392\n",
      "2393\n",
      "2394\n",
      "2395\n",
      "2396\n",
      "2397\n",
      "2398\n",
      "2399\n",
      "2400\n",
      "2401\n",
      "2402\n",
      "2403\n",
      "2404\n",
      "2405\n",
      "2406\n",
      "2407\n",
      "2408\n",
      "2409\n",
      "2410\n",
      "2411\n",
      "2412\n",
      "2413\n",
      "2414\n",
      "2415\n",
      "2416\n",
      "2417\n",
      "2418\n",
      "2419\n",
      "2420\n",
      "2421\n",
      "2422\n",
      "2423\n",
      "2424\n",
      "2425\n",
      "2426\n",
      "2427\n",
      "2428\n",
      "2429\n",
      "2430\n",
      "2431\n",
      "2432\n",
      "2433\n",
      "2434\n",
      "2435\n",
      "2436\n",
      "2437\n",
      "2438\n",
      "2439\n",
      "2440\n",
      "2441\n",
      "2442\n",
      "2443\n",
      "2444\n",
      "2445\n",
      "2446\n",
      "2447\n",
      "2448\n",
      "2449\n",
      "2450\n",
      "2451\n",
      "2452\n",
      "2453\n",
      "2454\n",
      "2455\n",
      "2456\n",
      "2457\n",
      "2458\n",
      "2459\n",
      "2460\n",
      "2461\n",
      "2462\n",
      "2463\n",
      "2464\n",
      "2465\n",
      "2466\n",
      "2467\n",
      "2468\n",
      "2469\n",
      "2470\n",
      "2471\n",
      "2472\n",
      "2473\n",
      "2474\n",
      "2475\n",
      "2476\n",
      "2477\n",
      "2478\n",
      "2479\n",
      "2480\n",
      "2481\n",
      "2482\n",
      "2483\n",
      "2484\n",
      "2485\n",
      "2486\n",
      "2487\n",
      "2488\n",
      "2489\n",
      "2490\n",
      "2491\n",
      "2492\n",
      "2493\n",
      "2494\n",
      "2495\n",
      "2496\n",
      "2497\n",
      "2498\n",
      "2499\n",
      "2500\n",
      "2501\n",
      "2502\n",
      "2503\n",
      "2504\n",
      "2505\n",
      "2506\n",
      "2507\n",
      "2508\n",
      "2509\n",
      "2510\n",
      "2511\n",
      "2512\n",
      "2513\n",
      "2514\n",
      "2515\n",
      "2516\n",
      "2517\n",
      "2518\n",
      "2519\n",
      "2520\n",
      "2521\n",
      "2522\n",
      "2523\n",
      "2524\n",
      "2525\n",
      "2526\n",
      "2527\n",
      "2528\n",
      "2529\n",
      "2530\n",
      "2531\n",
      "2532\n",
      "2533\n",
      "2534\n",
      "2535\n",
      "2536\n",
      "2537\n",
      "2538\n",
      "2539\n",
      "2540\n",
      "2541\n",
      "2542\n",
      "2543\n",
      "2544\n",
      "2545\n",
      "2546\n",
      "2547\n",
      "2548\n",
      "2549\n",
      "2550\n",
      "2551\n",
      "2552\n",
      "2553\n",
      "2554\n",
      "2555\n",
      "2556\n",
      "2557\n",
      "2558\n",
      "2559\n",
      "2560\n",
      "2561\n",
      "2562\n",
      "2563\n",
      "2564\n",
      "2565\n",
      "2566\n",
      "2567\n",
      "2568\n",
      "2569\n",
      "2570\n",
      "2571\n",
      "2572\n",
      "2573\n",
      "2574\n",
      "2575\n",
      "2576\n",
      "2577\n",
      "2578\n",
      "2579\n",
      "2580\n",
      "2581\n",
      "2582\n",
      "2583\n",
      "2584\n",
      "2585\n",
      "2586\n",
      "2587\n",
      "2588\n",
      "2589\n",
      "2590\n",
      "2591\n",
      "2592\n",
      "2593\n",
      "2594\n",
      "2595\n",
      "2596\n",
      "2597\n",
      "2598\n",
      "2599\n",
      "2600\n",
      "2601\n",
      "2602\n",
      "2603\n",
      "2604\n",
      "2605\n",
      "2606\n",
      "2607\n",
      "2608\n",
      "2609\n",
      "2610\n",
      "2611\n",
      "2612\n",
      "2613\n",
      "2614\n",
      "2615\n",
      "2616\n",
      "2617\n",
      "2618\n",
      "2619\n",
      "2620\n",
      "2621\n",
      "2622\n",
      "2623\n",
      "2624\n",
      "2625\n",
      "2626\n",
      "2627\n",
      "2628\n",
      "2629\n",
      "2630\n",
      "2631\n",
      "2632\n",
      "2633\n",
      "2634\n",
      "2635\n",
      "2636\n",
      "2637\n",
      "2638\n",
      "2639\n",
      "2640\n",
      "2641\n",
      "2642\n",
      "2643\n",
      "2644\n",
      "2645\n",
      "2646\n",
      "2647\n",
      "2648\n",
      "2649\n",
      "2650\n",
      "2651\n",
      "2652\n",
      "2653\n",
      "2654\n",
      "2655\n",
      "2656\n",
      "2657\n",
      "2658\n",
      "2659\n",
      "2660\n",
      "2661\n",
      "2662\n",
      "2663\n",
      "2664\n",
      "2665\n",
      "2666\n",
      "2667\n",
      "2668\n",
      "2669\n",
      "2670\n",
      "2671\n",
      "2672\n",
      "2673\n",
      "2674\n",
      "2675\n",
      "2676\n",
      "2677\n",
      "2678\n",
      "2679\n",
      "2680\n",
      "2681\n",
      "2682\n",
      "2683\n",
      "2684\n",
      "2685\n",
      "2686\n",
      "2687\n",
      "2688\n",
      "2689\n",
      "2690\n",
      "2691\n",
      "2692\n",
      "2693\n",
      "2694\n",
      "2695\n",
      "2696\n",
      "2697\n",
      "2698\n",
      "2699\n",
      "2700\n",
      "2701\n",
      "2702\n",
      "2703\n",
      "2704\n",
      "2705\n",
      "2706\n",
      "2707\n",
      "2708\n",
      "2709\n",
      "2710\n",
      "2711\n",
      "2712\n",
      "2713\n",
      "2714\n",
      "2715\n",
      "2716\n",
      "2717\n",
      "2718\n",
      "2719\n",
      "2720\n",
      "2721\n",
      "2722\n",
      "2723\n",
      "2724\n",
      "2725\n",
      "2726\n",
      "2727\n",
      "2728\n",
      "2729\n",
      "2730\n",
      "2731\n",
      "2732\n",
      "2733\n",
      "2734\n",
      "2735\n",
      "2736\n",
      "2737\n",
      "2738\n",
      "2739\n",
      "2740\n",
      "2741\n",
      "2742\n",
      "2743\n",
      "2744\n",
      "2745\n",
      "2746\n",
      "2747\n",
      "2748\n",
      "2749\n",
      "2750\n",
      "2751\n",
      "2752\n",
      "2753\n",
      "2754\n",
      "2755\n",
      "2756\n",
      "2757\n",
      "2758\n",
      "2759\n",
      "2760\n",
      "2761\n",
      "2762\n",
      "2763\n",
      "2764\n",
      "2765\n",
      "2766\n",
      "2767\n",
      "2768\n",
      "2769\n",
      "2770\n",
      "2771\n",
      "2772\n",
      "2773\n",
      "2774\n",
      "2775\n",
      "2776\n",
      "2777\n",
      "2778\n",
      "2779\n",
      "2780\n",
      "2781\n",
      "2782\n",
      "2783\n",
      "2784\n",
      "2785\n",
      "2786\n",
      "2787\n",
      "2788\n",
      "2789\n",
      "2790\n",
      "2791\n",
      "2792\n",
      "2793\n",
      "2794\n",
      "2795\n",
      "2796\n",
      "2797\n",
      "2798\n",
      "2799\n",
      "2800\n",
      "2801\n",
      "2802\n",
      "2803\n",
      "2804\n",
      "2805\n",
      "2806\n",
      "2807\n",
      "2808\n",
      "2809\n",
      "2810\n",
      "2811\n",
      "2812\n",
      "2813\n",
      "2814\n",
      "2815\n",
      "2816\n",
      "2817\n",
      "2818\n",
      "2819\n",
      "2820\n",
      "2821\n",
      "2822\n",
      "2823\n",
      "2824\n",
      "2825\n",
      "2826\n",
      "2827\n",
      "2828\n",
      "2829\n",
      "2830\n",
      "2831\n",
      "2832\n",
      "2833\n",
      "2834\n",
      "2835\n",
      "2836\n",
      "2837\n",
      "2838\n",
      "2839\n",
      "2840\n",
      "2841\n",
      "2842\n",
      "2843\n",
      "2844\n",
      "2845\n",
      "2846\n",
      "2847\n",
      "2848\n",
      "2849\n",
      "2850\n",
      "2851\n",
      "2852\n",
      "2853\n",
      "2854\n",
      "2855\n",
      "2856\n",
      "2857\n",
      "2858\n",
      "2859\n",
      "2860\n",
      "2861\n",
      "2862\n",
      "2863\n",
      "2864\n",
      "2865\n",
      "2866\n",
      "2867\n",
      "2868\n",
      "2869\n",
      "2870\n",
      "2871\n",
      "2872\n",
      "2873\n",
      "2874\n",
      "2875\n",
      "2876\n",
      "2877\n",
      "2878\n",
      "2879\n",
      "2880\n",
      "2881\n",
      "2882\n",
      "2883\n",
      "2884\n",
      "2885\n",
      "2886\n",
      "2887\n",
      "2888\n",
      "2889\n",
      "2890\n",
      "2891\n",
      "2892\n",
      "2893\n",
      "2894\n",
      "2895\n",
      "2896\n",
      "2897\n",
      "2898\n",
      "2899\n",
      "2900\n",
      "2901\n",
      "2902\n",
      "2903\n",
      "2904\n",
      "2905\n",
      "2906\n",
      "2907\n",
      "2908\n",
      "2909\n",
      "2910\n",
      "2911\n",
      "2912\n",
      "2913\n",
      "2914\n",
      "2915\n",
      "2916\n",
      "2917\n",
      "2918\n",
      "2919\n",
      "2920\n",
      "2921\n",
      "2922\n",
      "2923\n",
      "2924\n",
      "2925\n",
      "2926\n",
      "2927\n",
      "2928\n",
      "2929\n",
      "2930\n",
      "2931\n",
      "2932\n",
      "2933\n",
      "2934\n",
      "2935\n",
      "2936\n",
      "2937\n",
      "2938\n",
      "2939\n",
      "2940\n",
      "2941\n",
      "2942\n",
      "2943\n",
      "2944\n",
      "2945\n",
      "2946\n",
      "2947\n",
      "2948\n",
      "2949\n",
      "2950\n",
      "2951\n",
      "2952\n",
      "2953\n",
      "2954\n",
      "2955\n",
      "2956\n",
      "2957\n",
      "2958\n",
      "2959\n",
      "2960\n",
      "2961\n",
      "2962\n",
      "2963\n",
      "2964\n",
      "2965\n",
      "2966\n",
      "2967\n",
      "2968\n",
      "2969\n",
      "2970\n",
      "2971\n",
      "2972\n",
      "2973\n",
      "2974\n",
      "2975\n",
      "2976\n",
      "2977\n",
      "2978\n",
      "2979\n",
      "2980\n",
      "2981\n",
      "2982\n",
      "2983\n",
      "2984\n",
      "2985\n",
      "2986\n",
      "2987\n",
      "2988\n",
      "2989\n",
      "2990\n",
      "2991\n",
      "2992\n",
      "2993\n",
      "2994\n",
      "2995\n",
      "2996\n",
      "2997\n",
      "2998\n",
      "2999\n",
      "3000\n",
      "3001\n",
      "3002\n",
      "3003\n",
      "3004\n",
      "3005\n",
      "3006\n",
      "3007\n",
      "3008\n",
      "3009\n",
      "3010\n",
      "3011\n",
      "3012\n",
      "3013\n",
      "3014\n",
      "3015\n",
      "3016\n",
      "3017\n",
      "3018\n",
      "3019\n",
      "3020\n",
      "3021\n",
      "3022\n",
      "3023\n",
      "3024\n",
      "3025\n",
      "3026\n",
      "3027\n",
      "3028\n",
      "3029\n",
      "3030\n",
      "3031\n",
      "3032\n",
      "3033\n",
      "3034\n",
      "3035\n",
      "3036\n",
      "3037\n",
      "3038\n",
      "3039\n",
      "3040\n",
      "3041\n",
      "3042\n",
      "3043\n",
      "3044\n",
      "3045\n",
      "3046\n",
      "3047\n",
      "3048\n",
      "3049\n",
      "3050\n",
      "3051\n",
      "3052\n",
      "3053\n",
      "3054\n",
      "3055\n",
      "3056\n",
      "3057\n",
      "3058\n",
      "3059\n",
      "3060\n",
      "3061\n",
      "3062\n",
      "3063\n",
      "3064\n",
      "3065\n",
      "3066\n",
      "3067\n",
      "3068\n",
      "3069\n",
      "3070\n",
      "3071\n",
      "3072\n",
      "3073\n",
      "3074\n",
      "3075\n",
      "3076\n",
      "3077\n",
      "3078\n",
      "3079\n",
      "3080\n",
      "3081\n",
      "3082\n",
      "3083\n",
      "3084\n",
      "3085\n",
      "3086\n",
      "3087\n",
      "3088\n",
      "3089\n",
      "3090\n",
      "3091\n",
      "3092\n",
      "3093\n",
      "3094\n",
      "3095\n",
      "3096\n",
      "3097\n",
      "3098\n",
      "3099\n",
      "3100\n",
      "3101\n",
      "3102\n",
      "3103\n",
      "3104\n",
      "3105\n",
      "3106\n",
      "3107\n",
      "3108\n",
      "3109\n",
      "3110\n",
      "3111\n",
      "3112\n",
      "3113\n",
      "3114\n",
      "3115\n",
      "3116\n",
      "3117\n",
      "3118\n",
      "3119\n",
      "3120\n",
      "3121\n",
      "3122\n",
      "3123\n",
      "3124\n",
      "3125\n",
      "3126\n",
      "3127\n",
      "3128\n",
      "3129\n",
      "3130\n",
      "3131\n",
      "3132\n",
      "3133\n",
      "3134\n",
      "3135\n",
      "3136\n",
      "3137\n",
      "3138\n",
      "3139\n",
      "3140\n",
      "3141\n",
      "3142\n",
      "3143\n",
      "3144\n",
      "3145\n",
      "3146\n",
      "3147\n",
      "3148\n",
      "3149\n",
      "3150\n",
      "3151\n",
      "3152\n",
      "3153\n",
      "3154\n",
      "3155\n",
      "3156\n",
      "3157\n",
      "3158\n",
      "3159\n",
      "3160\n",
      "3161\n",
      "3162\n",
      "3163\n",
      "3164\n",
      "3165\n",
      "3166\n",
      "3167\n",
      "3168\n",
      "3169\n",
      "3170\n",
      "3171\n",
      "3172\n",
      "3173\n",
      "3174\n",
      "3175\n",
      "3176\n",
      "3177\n",
      "3178\n",
      "3179\n",
      "3180\n",
      "3181\n",
      "3182\n",
      "3183\n",
      "3184\n",
      "3185\n",
      "3186\n",
      "3187\n",
      "3188\n",
      "3189\n",
      "3190\n",
      "3191\n",
      "3192\n",
      "3193\n",
      "3194\n",
      "3195\n",
      "3196\n",
      "3197\n",
      "3198\n",
      "3199\n",
      "3200\n",
      "3201\n",
      "3202\n",
      "3203\n",
      "3204\n",
      "3205\n",
      "3206\n",
      "3207\n",
      "3208\n",
      "3209\n",
      "3210\n",
      "3211\n",
      "3212\n",
      "3213\n",
      "3214\n",
      "3215\n",
      "3216\n",
      "3217\n",
      "3218\n",
      "3219\n",
      "3220\n",
      "3221\n",
      "3222\n",
      "3223\n",
      "3224\n",
      "3225\n",
      "3226\n",
      "3227\n",
      "3228\n",
      "3229\n",
      "3230\n",
      "3231\n",
      "3232\n",
      "3233\n",
      "3234\n",
      "3235\n",
      "3236\n",
      "3237\n",
      "3238\n",
      "3239\n",
      "3240\n",
      "3241\n",
      "3242\n",
      "3243\n",
      "3244\n",
      "3245\n",
      "3246\n",
      "3247\n",
      "3248\n",
      "3249\n",
      "3250\n",
      "3251\n",
      "3252\n",
      "3253\n",
      "3254\n",
      "3255\n",
      "3256\n",
      "3257\n",
      "3258\n",
      "3259\n",
      "3260\n",
      "3261\n",
      "3262\n",
      "3263\n",
      "3264\n",
      "3265\n",
      "3266\n",
      "3267\n",
      "3268\n",
      "3269\n",
      "3270\n",
      "3271\n",
      "3272\n",
      "3273\n",
      "3274\n",
      "3275\n",
      "3276\n",
      "3277\n",
      "3278\n",
      "3279\n",
      "3280\n",
      "3281\n",
      "3282\n",
      "3283\n",
      "3284\n",
      "3285\n",
      "3286\n",
      "3287\n",
      "3288\n",
      "3289\n",
      "3290\n",
      "3291\n",
      "3292\n",
      "3293\n",
      "3294\n",
      "3295\n",
      "3296\n",
      "3297\n",
      "3298\n",
      "3299\n",
      "3300\n",
      "3301\n",
      "3302\n",
      "3303\n",
      "3304\n",
      "3305\n",
      "3306\n",
      "3307\n",
      "3308\n",
      "3309\n",
      "3310\n",
      "3311\n",
      "3312\n",
      "3313\n",
      "3314\n",
      "3315\n",
      "3316\n",
      "3317\n",
      "3318\n",
      "3319\n",
      "3320\n",
      "3321\n",
      "3322\n",
      "3323\n",
      "3324\n",
      "3325\n",
      "3326\n",
      "3327\n",
      "3328\n",
      "3329\n",
      "3330\n",
      "3331\n",
      "3332\n",
      "3333\n",
      "3334\n",
      "3335\n",
      "3336\n",
      "3337\n",
      "3338\n",
      "3339\n",
      "3340\n",
      "3341\n",
      "3342\n",
      "3343\n",
      "3344\n",
      "3345\n",
      "3346\n",
      "3347\n",
      "3348\n",
      "3349\n",
      "3350\n",
      "3351\n",
      "3352\n",
      "3353\n",
      "3354\n",
      "3355\n",
      "3356\n",
      "3357\n",
      "3358\n",
      "3359\n",
      "3360\n",
      "3361\n",
      "3362\n",
      "3363\n",
      "3364\n",
      "3365\n",
      "3366\n",
      "3367\n",
      "3368\n",
      "3369\n",
      "3370\n",
      "3371\n",
      "3372\n",
      "3373\n",
      "3374\n",
      "3375\n",
      "3376\n",
      "3377\n",
      "3378\n",
      "3379\n",
      "3380\n",
      "3381\n",
      "3382\n",
      "3383\n",
      "3384\n",
      "3385\n",
      "3386\n",
      "3387\n",
      "3388\n",
      "3389\n",
      "3390\n",
      "3391\n",
      "3392\n",
      "3393\n",
      "3394\n",
      "3395\n",
      "3396\n",
      "3397\n",
      "3398\n",
      "3399\n",
      "3400\n",
      "3401\n",
      "3402\n",
      "3403\n",
      "3404\n",
      "3405\n",
      "3406\n",
      "3407\n",
      "3408\n",
      "3409\n",
      "3410\n",
      "3411\n",
      "3412\n",
      "3413\n",
      "3414\n",
      "3415\n",
      "3416\n",
      "3417\n",
      "3418\n",
      "3419\n",
      "3420\n",
      "3421\n",
      "3422\n",
      "3423\n",
      "3424\n",
      "3425\n",
      "3426\n",
      "3427\n",
      "3428\n",
      "3429\n",
      "3430\n",
      "3431\n",
      "3432\n",
      "3433\n",
      "3434\n",
      "3435\n",
      "3436\n",
      "3437\n",
      "3438\n",
      "3439\n",
      "3440\n",
      "3441\n",
      "3442\n",
      "3443\n",
      "3444\n",
      "3445\n",
      "3446\n",
      "3447\n",
      "3448\n",
      "3449\n",
      "3450\n",
      "3451\n",
      "3452\n",
      "3453\n",
      "3454\n",
      "3455\n",
      "3456\n",
      "3457\n",
      "3458\n",
      "3459\n",
      "3460\n",
      "3461\n",
      "3462\n",
      "3463\n",
      "3464\n",
      "3465\n",
      "3466\n",
      "3467\n",
      "3468\n",
      "3469\n",
      "3470\n",
      "3471\n",
      "3472\n",
      "3473\n",
      "3474\n",
      "3475\n",
      "3476\n",
      "3477\n",
      "3478\n",
      "3479\n",
      "3480\n",
      "3481\n",
      "3482\n",
      "3483\n",
      "3484\n",
      "3485\n",
      "3486\n",
      "3487\n",
      "3488\n",
      "3489\n",
      "3490\n",
      "3491\n",
      "3492\n",
      "3493\n",
      "3494\n",
      "3495\n",
      "3496\n",
      "3497\n",
      "3498\n",
      "3499\n",
      "3500\n",
      "3501\n",
      "3502\n",
      "3503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3504\n",
      "3505\n",
      "3506\n",
      "3507\n",
      "3508\n",
      "3509\n",
      "3510\n",
      "3511\n",
      "3512\n",
      "3513\n",
      "3514\n",
      "3515\n",
      "3516\n",
      "3517\n",
      "3518\n",
      "3519\n",
      "3520\n",
      "3521\n",
      "3522\n",
      "3523\n",
      "3524\n",
      "3525\n",
      "3526\n",
      "3527\n",
      "3528\n",
      "3529\n",
      "3530\n",
      "3531\n",
      "3532\n",
      "3533\n",
      "3534\n",
      "3535\n",
      "3536\n",
      "3537\n",
      "3538\n",
      "3539\n",
      "3540\n",
      "3541\n",
      "3542\n",
      "3543\n",
      "3544\n",
      "3545\n",
      "3546\n",
      "3547\n",
      "3548\n",
      "3549\n",
      "3550\n",
      "3551\n",
      "3552\n",
      "3553\n",
      "3554\n",
      "3555\n",
      "3556\n",
      "3557\n",
      "3558\n",
      "3559\n",
      "3560\n",
      "3561\n",
      "3562\n",
      "3563\n",
      "3564\n",
      "3565\n",
      "3566\n",
      "3567\n",
      "3568\n",
      "3569\n",
      "3570\n",
      "3571\n",
      "3572\n",
      "3573\n",
      "3574\n",
      "3575\n",
      "3576\n",
      "3577\n",
      "3578\n",
      "3579\n",
      "3580\n",
      "3581\n",
      "3582\n",
      "3583\n",
      "3584\n",
      "3585\n",
      "3586\n",
      "3587\n",
      "3588\n",
      "3589\n",
      "3590\n",
      "3591\n",
      "3592\n",
      "3593\n",
      "3594\n",
      "3595\n",
      "3596\n",
      "3597\n",
      "3598\n",
      "3599\n",
      "3600\n",
      "3601\n",
      "3602\n",
      "3603\n",
      "3604\n",
      "3605\n",
      "3606\n",
      "3607\n",
      "3608\n",
      "3609\n",
      "3610\n",
      "3611\n",
      "3612\n",
      "3613\n",
      "3614\n",
      "3615\n",
      "3616\n",
      "3617\n",
      "3618\n",
      "3619\n",
      "3620\n",
      "3621\n",
      "3622\n",
      "3623\n",
      "3624\n",
      "3625\n",
      "3626\n",
      "3627\n",
      "3628\n",
      "3629\n",
      "3630\n",
      "3631\n",
      "3632\n",
      "3633\n",
      "3634\n",
      "3635\n",
      "3636\n",
      "3637\n",
      "3638\n",
      "3639\n",
      "3640\n",
      "3641\n",
      "3642\n",
      "3643\n",
      "3644\n",
      "3645\n",
      "3646\n",
      "3647\n",
      "3648\n",
      "3649\n",
      "3650\n",
      "3651\n",
      "3652\n",
      "3653\n",
      "3654\n",
      "3655\n",
      "3656\n",
      "3657\n",
      "3658\n",
      "3659\n",
      "3660\n",
      "3661\n",
      "3662\n",
      "3663\n",
      "3664\n",
      "3665\n",
      "3666\n",
      "3667\n",
      "3668\n",
      "3669\n",
      "3670\n",
      "3671\n",
      "3672\n",
      "3673\n",
      "3674\n",
      "3675\n",
      "3676\n",
      "3677\n",
      "3678\n",
      "3679\n",
      "3680\n",
      "3681\n",
      "3682\n",
      "3683\n",
      "3684\n",
      "3685\n",
      "3686\n",
      "3687\n",
      "3688\n",
      "3689\n",
      "3690\n",
      "3691\n",
      "3692\n",
      "3693\n",
      "3694\n",
      "3695\n",
      "3696\n",
      "3697\n",
      "3698\n",
      "3699\n",
      "3700\n",
      "3701\n",
      "3702\n",
      "3703\n",
      "3704\n",
      "3705\n",
      "3706\n",
      "3707\n",
      "3708\n",
      "3709\n",
      "3710\n",
      "3711\n",
      "3712\n",
      "3713\n",
      "3714\n",
      "3715\n",
      "3716\n",
      "3717\n",
      "3718\n",
      "3719\n",
      "3720\n",
      "3721\n",
      "3722\n",
      "3723\n",
      "3724\n",
      "3725\n",
      "3726\n",
      "3727\n",
      "3728\n",
      "3729\n",
      "3730\n",
      "3731\n",
      "3732\n",
      "3733\n",
      "3734\n",
      "3735\n",
      "3736\n",
      "3737\n",
      "3738\n",
      "3739\n",
      "3740\n",
      "3741\n",
      "3742\n",
      "3743\n",
      "3744\n",
      "3745\n",
      "3746\n",
      "3747\n",
      "3748\n",
      "3749\n",
      "3750\n",
      "3751\n",
      "3752\n",
      "3753\n",
      "3754\n",
      "3755\n",
      "3756\n",
      "3757\n",
      "3758\n",
      "3759\n",
      "3760\n",
      "3761\n",
      "3762\n",
      "3763\n",
      "3764\n",
      "3765\n",
      "3766\n",
      "3767\n",
      "3768\n",
      "3769\n",
      "3770\n",
      "3771\n",
      "3772\n",
      "3773\n",
      "3774\n",
      "3775\n",
      "3776\n",
      "3777\n",
      "3778\n",
      "3779\n",
      "3780\n",
      "3781\n",
      "3782\n",
      "3783\n",
      "3784\n",
      "3785\n",
      "3786\n",
      "3787\n",
      "3788\n",
      "3789\n",
      "3790\n",
      "3791\n",
      "3792\n",
      "3793\n",
      "3794\n",
      "3795\n",
      "3796\n",
      "3797\n",
      "3798\n",
      "3799\n",
      "3800\n",
      "3801\n",
      "3802\n",
      "3803\n",
      "3804\n",
      "3805\n",
      "3806\n",
      "3807\n",
      "3808\n",
      "3809\n",
      "3810\n",
      "3811\n",
      "3812\n",
      "3813\n",
      "3814\n",
      "3815\n",
      "3816\n",
      "3817\n",
      "3818\n",
      "3819\n",
      "3820\n",
      "3821\n",
      "3822\n",
      "3823\n",
      "3824\n",
      "3825\n",
      "3826\n",
      "3827\n",
      "3828\n",
      "3829\n",
      "3830\n",
      "3831\n",
      "3832\n",
      "3833\n",
      "3834\n",
      "3835\n",
      "3836\n",
      "3837\n",
      "3838\n",
      "3839\n",
      "3840\n",
      "3841\n",
      "3842\n",
      "3843\n",
      "3844\n",
      "3845\n",
      "3846\n",
      "3847\n",
      "3848\n",
      "3849\n",
      "3850\n",
      "3851\n",
      "3852\n",
      "3853\n",
      "3854\n",
      "3855\n",
      "3856\n",
      "3857\n",
      "3858\n",
      "3859\n",
      "3860\n",
      "3861\n",
      "3862\n",
      "3863\n",
      "3864\n",
      "3865\n",
      "3866\n",
      "3867\n",
      "3868\n",
      "3869\n",
      "3870\n",
      "3871\n",
      "3872\n",
      "3873\n",
      "3874\n",
      "3875\n",
      "3876\n",
      "3877\n",
      "3878\n",
      "3879\n",
      "3880\n",
      "3881\n",
      "3882\n",
      "3883\n",
      "3884\n",
      "3885\n",
      "3886\n",
      "3887\n",
      "3888\n",
      "3889\n",
      "3890\n",
      "3891\n",
      "3892\n",
      "3893\n",
      "3894\n",
      "3895\n",
      "3896\n",
      "3897\n",
      "3898\n",
      "3899\n",
      "3900\n",
      "3901\n",
      "3902\n",
      "3903\n",
      "3904\n",
      "3905\n",
      "3906\n",
      "3907\n",
      "3908\n",
      "3909\n",
      "3910\n",
      "3911\n",
      "3912\n",
      "3913\n",
      "3914\n",
      "3915\n",
      "3916\n",
      "3917\n",
      "3918\n",
      "3919\n",
      "3920\n",
      "3921\n",
      "3922\n",
      "3923\n",
      "3924\n",
      "3925\n",
      "3926\n",
      "3927\n",
      "3928\n",
      "3929\n",
      "3930\n",
      "3931\n",
      "3932\n",
      "3933\n",
      "3934\n",
      "3935\n",
      "3936\n",
      "3937\n",
      "3938\n",
      "3939\n",
      "3940\n",
      "3941\n",
      "3942\n",
      "3943\n",
      "3944\n",
      "3945\n",
      "3946\n",
      "3947\n",
      "3948\n",
      "3949\n",
      "3950\n",
      "3951\n",
      "3952\n",
      "3953\n",
      "3954\n",
      "3955\n",
      "3956\n",
      "3957\n",
      "3958\n",
      "3959\n",
      "3960\n",
      "3961\n",
      "3962\n",
      "3963\n",
      "3964\n",
      "3965\n",
      "3966\n",
      "3967\n",
      "3968\n",
      "3969\n",
      "3970\n",
      "3971\n",
      "3972\n",
      "3973\n",
      "3974\n",
      "3975\n",
      "3976\n",
      "3977\n",
      "3978\n",
      "3979\n",
      "3980\n",
      "3981\n",
      "3982\n",
      "3983\n",
      "3984\n",
      "3985\n",
      "3986\n",
      "3987\n",
      "3988\n",
      "3989\n",
      "3990\n",
      "3991\n",
      "3992\n",
      "3993\n",
      "3994\n",
      "3995\n",
      "3996\n",
      "3997\n",
      "3998\n",
      "3999\n",
      "4000\n",
      "4001\n",
      "4002\n",
      "4003\n",
      "4004\n",
      "4005\n",
      "4006\n",
      "4007\n",
      "4008\n",
      "4009\n",
      "4010\n",
      "4011\n",
      "4012\n",
      "4013\n",
      "4014\n",
      "4015\n",
      "4016\n",
      "4017\n",
      "4018\n",
      "4019\n",
      "4020\n",
      "4021\n",
      "4022\n",
      "4023\n",
      "4024\n",
      "4025\n",
      "4026\n",
      "4027\n",
      "4028\n",
      "4029\n",
      "4030\n",
      "4031\n",
      "4032\n",
      "4033\n",
      "4034\n",
      "4035\n",
      "4036\n",
      "4037\n",
      "4038\n",
      "4039\n",
      "4040\n",
      "4041\n",
      "4042\n",
      "4043\n",
      "4044\n",
      "4045\n",
      "4046\n",
      "4047\n",
      "4048\n",
      "4049\n",
      "4050\n",
      "4051\n",
      "4052\n",
      "4053\n",
      "4054\n",
      "4055\n",
      "4056\n",
      "4057\n",
      "4058\n",
      "4059\n",
      "4060\n",
      "4061\n",
      "4062\n",
      "4063\n",
      "4064\n",
      "4065\n",
      "4066\n",
      "4067\n",
      "4068\n",
      "4069\n",
      "4070\n",
      "4071\n",
      "4072\n",
      "4073\n",
      "4074\n",
      "4075\n",
      "4076\n",
      "4077\n",
      "4078\n",
      "4079\n",
      "4080\n",
      "4081\n",
      "4082\n",
      "4083\n",
      "4084\n",
      "4085\n",
      "4086\n",
      "4087\n",
      "4088\n",
      "4089\n",
      "4090\n",
      "4091\n",
      "4092\n",
      "4093\n",
      "4094\n",
      "4095\n",
      "4096\n",
      "4097\n",
      "4098\n",
      "4099\n",
      "4100\n",
      "4101\n",
      "4102\n",
      "4103\n",
      "4104\n",
      "4105\n",
      "4106\n",
      "4107\n",
      "4108\n",
      "4109\n",
      "4110\n",
      "4111\n",
      "4112\n",
      "4113\n",
      "4114\n",
      "4115\n",
      "4116\n",
      "4117\n",
      "4118\n",
      "4119\n",
      "4120\n",
      "4121\n",
      "4122\n",
      "4123\n",
      "4124\n",
      "4125\n",
      "4126\n",
      "4127\n",
      "4128\n",
      "4129\n",
      "4130\n",
      "4131\n",
      "4132\n",
      "4133\n",
      "4134\n",
      "4135\n",
      "4136\n",
      "4137\n",
      "4138\n",
      "4139\n",
      "4140\n",
      "4141\n",
      "4142\n",
      "4143\n",
      "4144\n",
      "4145\n",
      "4146\n",
      "4147\n",
      "4148\n",
      "4149\n",
      "4150\n",
      "4151\n",
      "4152\n",
      "4153\n",
      "4154\n",
      "4155\n",
      "4156\n",
      "4157\n",
      "4158\n",
      "4159\n",
      "4160\n",
      "4161\n",
      "4162\n",
      "4163\n",
      "4164\n",
      "4165\n",
      "4166\n",
      "4167\n",
      "4168\n",
      "4169\n",
      "4170\n",
      "4171\n",
      "4172\n",
      "4173\n",
      "4174\n",
      "4175\n",
      "4176\n",
      "4177\n",
      "4178\n",
      "4179\n",
      "4180\n",
      "4181\n",
      "4182\n",
      "4183\n",
      "4184\n",
      "4185\n",
      "4186\n",
      "4187\n",
      "4188\n",
      "4189\n",
      "4190\n",
      "4191\n",
      "4192\n",
      "4193\n",
      "4194\n",
      "4195\n",
      "4196\n",
      "4197\n",
      "4198\n",
      "4199\n",
      "4200\n",
      "4201\n",
      "4202\n",
      "4203\n",
      "4204\n",
      "4205\n",
      "4206\n",
      "4207\n",
      "4208\n",
      "4209\n",
      "4210\n",
      "4211\n",
      "4212\n",
      "4213\n",
      "4214\n",
      "4215\n",
      "4216\n",
      "4217\n",
      "4218\n",
      "4219\n",
      "4220\n",
      "4221\n",
      "4222\n",
      "4223\n",
      "4224\n",
      "4225\n",
      "4226\n",
      "4227\n",
      "4228\n",
      "4229\n",
      "4230\n",
      "4231\n",
      "4232\n",
      "4233\n",
      "4234\n",
      "4235\n",
      "4236\n",
      "4237\n",
      "4238\n",
      "4239\n",
      "4240\n",
      "4241\n",
      "4242\n",
      "4243\n",
      "4244\n",
      "4245\n",
      "4246\n",
      "4247\n",
      "4248\n",
      "4249\n",
      "4250\n",
      "4251\n",
      "4252\n",
      "4253\n",
      "4254\n",
      "4255\n",
      "4256\n",
      "4257\n",
      "4258\n",
      "4259\n",
      "4260\n",
      "4261\n",
      "4262\n",
      "4263\n",
      "4264\n",
      "4265\n",
      "4266\n",
      "4267\n",
      "4268\n",
      "4269\n",
      "4270\n",
      "4271\n",
      "4272\n",
      "4273\n",
      "4274\n",
      "4275\n",
      "4276\n",
      "4277\n",
      "4278\n",
      "4279\n",
      "4280\n",
      "4281\n",
      "4282\n",
      "4283\n",
      "4284\n",
      "4285\n",
      "4286\n",
      "4287\n",
      "4288\n",
      "4289\n",
      "4290\n",
      "4291\n",
      "4292\n",
      "4293\n",
      "4294\n",
      "4295\n",
      "4296\n",
      "4297\n",
      "4298\n",
      "4299\n",
      "4300\n",
      "4301\n",
      "4302\n",
      "4303\n",
      "4304\n",
      "4305\n",
      "4306\n",
      "4307\n",
      "4308\n",
      "4309\n",
      "4310\n",
      "4311\n",
      "4312\n",
      "4313\n",
      "4314\n",
      "4315\n",
      "4316\n",
      "4317\n",
      "4318\n",
      "4319\n",
      "4320\n",
      "4321\n",
      "4322\n",
      "4323\n",
      "4324\n",
      "4325\n",
      "4326\n",
      "4327\n",
      "4328\n",
      "4329\n",
      "4330\n",
      "4331\n",
      "4332\n",
      "4333\n",
      "4334\n",
      "4335\n",
      "4336\n",
      "4337\n",
      "4338\n",
      "4339\n",
      "4340\n",
      "4341\n",
      "4342\n",
      "4343\n",
      "4344\n",
      "4345\n",
      "4346\n",
      "4347\n",
      "4348\n",
      "4349\n",
      "4350\n",
      "4351\n",
      "4352\n",
      "4353\n",
      "4354\n",
      "4355\n",
      "4356\n",
      "4357\n",
      "4358\n",
      "4359\n",
      "4360\n",
      "4361\n",
      "4362\n",
      "4363\n",
      "4364\n",
      "4365\n",
      "4366\n",
      "4367\n",
      "4368\n",
      "4369\n",
      "4370\n",
      "4371\n",
      "4372\n",
      "4373\n",
      "4374\n",
      "4375\n",
      "4376\n",
      "4377\n",
      "4378\n",
      "4379\n",
      "4380\n",
      "4381\n",
      "4382\n",
      "4383\n",
      "4384\n",
      "4385\n",
      "4386\n",
      "4387\n",
      "4388\n",
      "4389\n",
      "4390\n",
      "4391\n",
      "4392\n",
      "4393\n",
      "4394\n",
      "4395\n",
      "4396\n",
      "4397\n",
      "4398\n",
      "4399\n",
      "4400\n",
      "4401\n",
      "4402\n",
      "4403\n",
      "4404\n",
      "4405\n",
      "4406\n",
      "4407\n",
      "4408\n",
      "4409\n",
      "4410\n",
      "4411\n",
      "4412\n",
      "4413\n",
      "4414\n",
      "4415\n",
      "4416\n",
      "4417\n",
      "4418\n",
      "4419\n",
      "4420\n",
      "4421\n",
      "4422\n",
      "4423\n",
      "4424\n",
      "4425\n",
      "4426\n",
      "4427\n",
      "4428\n",
      "4429\n",
      "4430\n",
      "4431\n",
      "4432\n",
      "4433\n",
      "4434\n",
      "4435\n",
      "4436\n",
      "4437\n",
      "4438\n",
      "4439\n",
      "4440\n",
      "4441\n",
      "4442\n",
      "4443\n",
      "4444\n",
      "4445\n",
      "4446\n",
      "4447\n",
      "4448\n",
      "4449\n",
      "4450\n",
      "4451\n",
      "4452\n",
      "4453\n",
      "4454\n",
      "4455\n",
      "4456\n",
      "4457\n",
      "4458\n",
      "4459\n",
      "4460\n",
      "4461\n",
      "4462\n",
      "4463\n",
      "4464\n",
      "4465\n",
      "4466\n",
      "4467\n",
      "4468\n",
      "4469\n",
      "4470\n",
      "4471\n",
      "4472\n",
      "4473\n",
      "4474\n",
      "4475\n",
      "4476\n",
      "4477\n",
      "4478\n",
      "4479\n",
      "4480\n",
      "4481\n",
      "4482\n",
      "4483\n",
      "4484\n",
      "4485\n",
      "4486\n",
      "4487\n",
      "4488\n",
      "4489\n",
      "4490\n",
      "4491\n",
      "4492\n",
      "4493\n",
      "4494\n",
      "4495\n",
      "4496\n",
      "4497\n",
      "4498\n",
      "4499\n",
      "4500\n",
      "4501\n",
      "4502\n",
      "4503\n",
      "4504\n",
      "4505\n",
      "4506\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "GB = GradientBoostingRegressor()\n",
    "\n",
    "\n",
    "ScoresGB = []\n",
    "for i in np.arange(6, NormX.shape[1]):\n",
    "\n",
    "    x = NormX.iloc[:,[0, 1, 2, 3, 4, 5, 6, i]]\n",
    "    print(i)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "\n",
    "    GB.fit(x_train, y_train)\n",
    "    ScoresGB.append((mean_absolute_error(GB.predict(x_test), y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2097</th>\n",
       "      <td>0.042716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>0.047037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>0.047103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3606</th>\n",
       "      <td>0.047452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3945</th>\n",
       "      <td>0.047502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4246</th>\n",
       "      <td>0.087395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>0.087533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>0.087727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2178</th>\n",
       "      <td>0.089138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.090020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4501 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0\n",
       "2097  0.042716\n",
       "507   0.047037\n",
       "1502  0.047103\n",
       "3606  0.047452\n",
       "3945  0.047502\n",
       "...        ...\n",
       "4246  0.087395\n",
       "789   0.087533\n",
       "582   0.087727\n",
       "2178  0.089138\n",
       "60    0.090020\n",
       "\n",
       "[4501 rows x 1 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(ScoresGB).sort_values(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>r</th>\n",
       "      <td>0.139040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Area</th>\n",
       "      <td>0.115921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>In</th>\n",
       "      <td>0.002126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z</th>\n",
       "      <td>0.097094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tplus</th>\n",
       "      <td>0.041875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tminus</th>\n",
       "      <td>0.037089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tminus_</th>\n",
       "      <td>0.054746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analytic 0.4999000000000129</th>\n",
       "      <td>0.512108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    0\n",
       "r                            0.139040\n",
       "Area                         0.115921\n",
       "In                           0.002126\n",
       "Z                            0.097094\n",
       "tplus                        0.041875\n",
       "tminus                       0.037089\n",
       "tminus_                      0.054746\n",
       "analytic 0.4999000000000129  0.512108"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(GB.feature_importances_, index=x_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2097"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(pd.DataFrame(ScoresGB).sort_values(0).iloc[[0],[0]].index.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x1 = NormX.iloc[:,[0, 1, 2, 3, 4, 5, 6, int(pd.DataFrame(ScoresRF).sort_values(0).iloc[[0],[0]].index.to_numpy())]]\n",
    "x2 = NormX.iloc[:,[0, 1, 2, 3, 4, 5, 6, int(pd.DataFrame(ScoresGB).sort_values(0).iloc[[0],[0]].index.to_numpy())]]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x1, y, random_state=15)\n",
    "\n",
    "RF.fit(x_train, y_train)\n",
    "RFC = RF.predict(x_test)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x2, y, random_state=15)\n",
    "\n",
    "GB.fit(x_train, y_train)\n",
    "GBC = GB.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "variable=Analytic Function<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "Analytic Function",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "mode": "lines",
         "name": "Analytic Function",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          2,
          8,
          10,
          15,
          23,
          24,
          28,
          38,
          42,
          43,
          44,
          47,
          51,
          64,
          80,
          84,
          87,
          90,
          92,
          94,
          99,
          100,
          103,
          109,
          117,
          120,
          128,
          136,
          146,
          148,
          159,
          164,
          170,
          175,
          177,
          179,
          184,
          187,
          189,
          194,
          200,
          204,
          208,
          209,
          218,
          222,
          225,
          226,
          227,
          228,
          230,
          233,
          238,
          239,
          242,
          243,
          249,
          251,
          252,
          256,
          257,
          262,
          263,
          267,
          270,
          272,
          273,
          274,
          276,
          277,
          285,
          289,
          290,
          291,
          293,
          295,
          302,
          303,
          306,
          307,
          309,
          310,
          313,
          316,
          328,
          335,
          346,
          349,
          350,
          361,
          367,
          376,
          380,
          396,
          400,
          405,
          408,
          411,
          412,
          415,
          423,
          424,
          430,
          431,
          432,
          443,
          446,
          449,
          451,
          452,
          453,
          456,
          465,
          471,
          482,
          490,
          498,
          512,
          513,
          514,
          516,
          518,
          520,
          532,
          550,
          552,
          557,
          574,
          579,
          583,
          589,
          591,
          607,
          612,
          616,
          620,
          626,
          629,
          642,
          643,
          644,
          652,
          657,
          659,
          665,
          669,
          670,
          680,
          683,
          686,
          688,
          696,
          700,
          702,
          708,
          709,
          710,
          712,
          714,
          720,
          722,
          725,
          727,
          733,
          740,
          741,
          744,
          745,
          746,
          756,
          765,
          766,
          767,
          770,
          771,
          773,
          777,
          780,
          782,
          783,
          785,
          789,
          791,
          793,
          794,
          801,
          805,
          808,
          811,
          817,
          827,
          831,
          832,
          834,
          842,
          843,
          849,
          858,
          862,
          865,
          867,
          870,
          871,
          879,
          883,
          888,
          889,
          890,
          893,
          896,
          897,
          899,
          904,
          906,
          911,
          912,
          913,
          914,
          917,
          919,
          922,
          925,
          926,
          942,
          944,
          947,
          957,
          958,
          959,
          963,
          969,
          971,
          972,
          979,
          983,
          986,
          995,
          996,
          998,
          1001,
          1005,
          1007,
          1009,
          1016,
          1019,
          1023,
          1027,
          1029,
          1033,
          1036,
          1041,
          1042,
          1046,
          1049,
          1052,
          1054,
          1061,
          1065,
          1067,
          1074,
          1078,
          1082,
          1084,
          1087,
          1088,
          1104,
          1106,
          1107,
          1108,
          1114,
          1116,
          1119,
          1123,
          1126,
          1127,
          1138,
          1140,
          1149,
          1160,
          1161,
          1162,
          1171,
          1173,
          1179,
          1180,
          1182,
          1183,
          1185,
          1189,
          1196,
          1199,
          1204,
          1209,
          1215,
          1220,
          1224,
          1228,
          1229,
          1234,
          1235,
          1243,
          1254,
          1255,
          1258,
          1262,
          1264,
          1273,
          1274,
          1278,
          1290,
          1295,
          1298,
          1302,
          1304,
          1316,
          1318,
          1327,
          1328,
          1331,
          1334,
          1339,
          1344,
          1345,
          1346,
          1347,
          1353,
          1354,
          1358,
          1360,
          1362,
          1363,
          1367,
          1376,
          1385,
          1394,
          1396,
          1404,
          1405,
          1407,
          1415,
          1423,
          1424,
          1426,
          1432,
          1434,
          1438,
          1439,
          1441,
          1446,
          1452,
          1468,
          1470,
          1479,
          1482,
          1491,
          1492,
          1493,
          1502,
          1508,
          1511,
          1520,
          1522,
          1524,
          1528,
          1536,
          1537,
          1544,
          1545,
          1546,
          1551,
          1552,
          1557,
          1560,
          1561,
          1562,
          1563,
          1565,
          1567,
          1572,
          1576,
          1583,
          1588,
          1594,
          1597,
          1606,
          1609,
          1611,
          1616,
          1617,
          1629,
          1630,
          1631,
          1633,
          1635,
          1650,
          1652,
          1656,
          1657,
          1660,
          1668,
          1677,
          1678,
          1681,
          1692,
          1695,
          1697,
          1701,
          1704,
          1705,
          1706,
          1707,
          1710,
          1711,
          1712,
          1713,
          1714,
          1716,
          1717,
          1720,
          1721,
          1722,
          1723,
          1733,
          1737,
          1740,
          1741,
          1744,
          1745,
          1750,
          1752,
          1753,
          1756,
          1760,
          1769,
          1771,
          1772,
          1774,
          1780,
          1784,
          1785,
          1786,
          1792,
          1795,
          1797,
          1799,
          1800,
          1807,
          1808,
          1810,
          1814,
          1820,
          1821,
          1822,
          1827,
          1830,
          1831,
          1834,
          1835,
          1836,
          1839,
          1844,
          1846,
          1851,
          1852,
          1857,
          1861,
          1864
         ],
         "xaxis": "x",
         "y": [
          0.5437317641159531,
          0.39481878972685597,
          0.3617902751206064,
          0.2992130087433489,
          0.2589083791652349,
          0.25890837916523496,
          0.28920829603565507,
          0.016668093722959042,
          0.016168761932332975,
          0.01616876193233298,
          0.016168761932332975,
          0.016168761932332975,
          0.016168761932332975,
          0.016168761932332975,
          0.016168761932332975,
          0.01616876193233298,
          0.000997535855276603,
          0.0009975358552766027,
          0.0009975358552766027,
          0.000997535855276603,
          0.0009975358552766027,
          0.0009975358552766027,
          0.000997535855276603,
          0.0009975358552766027,
          0.0009975358552766027,
          0.0009975358552766027,
          0.0009975358552766027,
          0.0009975358552766027,
          0.0009975358552766027,
          0.0009975358552766027,
          0.0009975358552766027,
          0.0009975358552766027,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          4.933422546057942e-05,
          0.1845465394272044,
          0.17471681546691437,
          0.14509150736780382,
          0.1506445270044349,
          0.18114934114419456,
          0.00905520744499353,
          0.00905520744499353,
          0.00905520744499353,
          0.00905520744499353,
          0.00905520744499353,
          0.00905520744499353,
          0.00905520744499353,
          0.0005529386998178874,
          0.0005529386998178874,
          0.0005529386998178874,
          0.0005529386998178874,
          0.0005529386998178874,
          0.0005529386998178874,
          0.0005529386998178874,
          0.0005529386998178874,
          0.0005529386998178874,
          0.0005529386998178874,
          0.0005529386998178874,
          0.0005529386998178874,
          0.0005529386998178874,
          0.0005529386998178874,
          0.0005529386998178874,
          0.0005529386998178874,
          0.0005529386998178874,
          0.0005529386998178874,
          0.0005529386998178874,
          0.0005529386998178874,
          2.154690324440971e-05,
          2.154690324440971e-05,
          2.154690324440971e-05,
          2.154690324440971e-05,
          2.154690324440971e-05,
          2.154690324440971e-05,
          2.154690324440971e-05,
          2.154690324440971e-05,
          2.154690324440971e-05,
          2.154690324440971e-05,
          2.154690324440971e-05,
          2.154690324440971e-05,
          2.154690324440971e-05,
          2.154690324440971e-05,
          2.154690324440971e-05,
          2.154690324440971e-05,
          2.154690324440971e-05,
          2.154690324440971e-05,
          2.154690324440971e-05,
          2.154690324440971e-05,
          2.154690324440971e-05,
          2.154690324440971e-05,
          0.196135533119918,
          0.16921960853705129,
          0.10830669659697635,
          0.10830669659697635,
          0.10830669659697635,
          0.11257234142619015,
          0.13277627061725183,
          0.14304522156268773,
          0.006756156771816816,
          0.006756156771816816,
          0.006756156771816814,
          0.006756156771816814,
          0.006756156771816814,
          0.006756156771816816,
          0.006756156771816814,
          0.006756156771816814,
          0.006756156771816816,
          0.006756156771816816,
          0.006756156771816816,
          0.006756156771816816,
          0.006756156771816816,
          0.006756156771816814,
          0.006756156771816814,
          0.0004092480327443426,
          0.00040924803274434274,
          0.0004092480327443426,
          0.0004092480327443426,
          0.00040924803274434274,
          0.00040924803274434274,
          0.0004092480327443426,
          0.0004092480327443426,
          0.0004092480327443426,
          0.0004092480327443426,
          0.0004092480327443426,
          0.0004092480327443426,
          0.0004092480327443426,
          0.0004092480327443426,
          0.0004092480327443426,
          0.0004092480327443426,
          0.0004092480327443426,
          0.0004092480327443426,
          0.00040924803274434274,
          0.0004092480327443426,
          0.0004092480327443426,
          0.0004092480327443426,
          0.0004092480327443426,
          0.0004092480327443426,
          0.0004092480327443426,
          0.00040924803274434274,
          1.256623655231317e-05,
          1.256623655231316e-05,
          1.256623655231317e-05,
          1.256623655231316e-05,
          1.256623655231316e-05,
          1.256623655231316e-05,
          1.256623655231316e-05,
          1.256623655231316e-05,
          1.256623655231316e-05,
          1.256623655231316e-05,
          1.256623655231316e-05,
          1.256623655231316e-05,
          1.256623655231316e-05,
          1.256623655231316e-05,
          1.256623655231316e-05,
          1.256623655231316e-05,
          1.256623655231316e-05,
          1.256623655231316e-05,
          1.256623655231316e-05,
          1.256623655231316e-05,
          1.256623655231316e-05,
          1.256623655231316e-05,
          1.256623655231316e-05,
          1.256623655231316e-05,
          1.256623655231316e-05,
          1.256623655231316e-05,
          1.256623655231316e-05,
          1.256623655231316e-05,
          1.256623655231316e-05,
          1.256623655231316e-05,
          1.256623655231316e-05,
          1.256623655231316e-05,
          1.256623655231316e-05,
          1.256623655231316e-05,
          1.256623655231316e-05,
          1.256623655231316e-05,
          1.256623655231316e-05,
          1.256623655231316e-05,
          1.256623655231316e-05,
          1.256623655231317e-05,
          1.256623655231316e-05,
          0.06084358428253667,
          0.059860499965750956,
          0.0589086719947786,
          0.05683539167870165,
          0.05683539167870165,
          0.05709300620834826,
          0.057986633041689384,
          0.06512144157033813,
          0.07004617727247374,
          0.003539200214424645,
          0.003539200214424645,
          0.0035392002144246444,
          0.0035392002144246444,
          0.003539200214424645,
          0.003539200214424645,
          0.003539200214424645,
          0.003539200214424645,
          0.003539200214424645,
          0.003539200214424645,
          0.003539200214424645,
          0.003539200214424645,
          0.003539200214424645,
          0.003539200214424645,
          0.00020818824790733205,
          0.00020818824790733205,
          0.00020818824790733203,
          0.00020818824790733203,
          0.00020818824790733205,
          0.00020818824790733205,
          0.00020818824790733205,
          0.00020818824790733205,
          0.00020818824790733205,
          0.00020818824790733205,
          0.00020818824790733205,
          0.00020818824790733205,
          0.00020818824790733205,
          0.00020818824790733205,
          0.00020818824790733205,
          0.00020818824790733205,
          0.00020818824790733203,
          0.00020818824790733205,
          0.00020818824790733205,
          0.00020818824790733203,
          0.00020818824790733203,
          0.00020818824790733205,
          1.6940658945086007e-21,
          1.6940658945086007e-21,
          1.6940658945086007e-21,
          1.6940658945086007e-21,
          0,
          1.6940658945086007e-21,
          1.6940658945086007e-21,
          1.6940658945086007e-21,
          1.6940658945086007e-21,
          0,
          1.6940658945086007e-21,
          1.6940658945086007e-21,
          1.6940658945086007e-21,
          1.6940658945086007e-21,
          1.6940658945086007e-21,
          1.6940658945086007e-21,
          1.6940658945086007e-21,
          1.6940658945086007e-21,
          1.6940658945086007e-21,
          1.6940658945086007e-21,
          1.6940658945086007e-21,
          1.6940658945086007e-21,
          1.6940658945086007e-21,
          1.6940658945086007e-21,
          0,
          1.6940658945086007e-21,
          1.6940658945086007e-21,
          1.6940658945086007e-21,
          1.6940658945086007e-21,
          0,
          1.6940658945086007e-21,
          1.6940658945086007e-21,
          1.6940658945086007e-21,
          1.6940658945086007e-21,
          1.6940658945086007e-21,
          1,
          0.9436063863436029,
          0.7699292870274786,
          0.5251370928548138,
          0.6259218144268832,
          0.7050448424593487,
          0.8479663723606675,
          0.03515984009150197,
          0.03280805653793165,
          0.03280805653793165,
          0.03280805653793165,
          0.03280805653793165,
          0.03280805653793165,
          0.03280805653793165,
          0.03280805653793165,
          0.03280805653793165,
          0.03280805653793165,
          0.032808056537931646,
          0.032808056537931646,
          0.00203749176812652,
          0.00203749176812652,
          0.0020374917681265196,
          0.00203749176812652,
          0.00203749176812652,
          0.00203749176812652,
          0.00203749176812652,
          0.00203749176812652,
          0.00203749176812652,
          0.00203749176812652,
          0.00203749176812652,
          0.00203749176812652,
          0.00203749176812652,
          0.00203749176812652,
          0.00203749176812652,
          0.00203749176812652,
          0.00203749176812652,
          0.0020374917681265196,
          0.00011433147001369927,
          0.00011433147001369927,
          0.00011433147001369927,
          0.00011433147001369927,
          0.00011433147001369927,
          0.00011433147001369927,
          0.00011433147001369927,
          0.00011433147001369927,
          0.00011433147001369927,
          0.00011433147001369927,
          0.00011433147001369927,
          0.00011433147001369927,
          0.00011433147001369927,
          0.00011433147001369927,
          0.00011433147001369927,
          0.00011433147001369927,
          0.00011433147001369927,
          0.00011433147001369927,
          0.00011433147001369927,
          0.00011433147001369927,
          0.00011433147001369927,
          0.00011433147001369927,
          0.00011433147001369927,
          0.00011433147001369927,
          0.00011433147001369927,
          0.00011433147001369927,
          0.00011433147001369927,
          0.00011433147001369927,
          0.00011433147001369927,
          0.00011433147001369927,
          0.00011433147001369927,
          0.00011433147001369927,
          0.00011433147001369927,
          0.00011433147001369927,
          0.38836804456256663,
          0.3547854451607028,
          0.33183032728626477,
          0.306402843488476,
          0.33183032728626477,
          0.3674966002930161,
          0.38836804456256663,
          0.019137165952535548,
          0.019137165952535548,
          0.019137165952535548,
          0.019137165952535545,
          0.019137165952535548,
          0.019137165952535545,
          0.019137165952535548,
          0.019137165952535548,
          0.019137165952535548,
          0.019137165952535548,
          0.0011830611065392635,
          0.0011830611065392633,
          0.0011830611065392635,
          0.0011830611065392633,
          0.0011830611065392633,
          0.0011830611065392635,
          0.0011830611065392633,
          0.0011830611065392633,
          0.0011830611065392635,
          0.0011830611065392633,
          0.0011830611065392633,
          0.0011830611065392633,
          0.0011830611065392633,
          0.0011830611065392633,
          0.0011830611065392633,
          0.0011830611065392633,
          0.0011830611065392633,
          0.0011830611065392635,
          0.0011830611065392635,
          0.0011830611065392635,
          0.0011830611065392635,
          0.0011830611065392633,
          0.0011830611065392635,
          0.0011830611065392633,
          0.0011830611065392635,
          0.0011830611065392635,
          6.0929553664495706e-05,
          6.092955366449572e-05,
          6.092955366449572e-05,
          6.092955366449572e-05,
          6.092955366449572e-05,
          6.092955366449572e-05,
          6.0929553664495706e-05,
          6.092955366449572e-05,
          6.0929553664495706e-05,
          6.092955366449572e-05,
          6.092955366449572e-05,
          6.092955366449572e-05,
          6.092955366449572e-05,
          6.092955366449572e-05,
          6.0929553664495706e-05,
          6.092955366449572e-05,
          6.092955366449572e-05,
          6.092955366449572e-05,
          6.092955366449572e-05,
          6.092955366449572e-05,
          6.092955366449572e-05,
          6.092955366449572e-05,
          6.092955366449572e-05,
          6.092955366449572e-05,
          6.092955366449572e-05,
          6.092955366449572e-05,
          6.092955366449572e-05,
          6.092955366449572e-05,
          6.092955366449572e-05,
          6.092955366449572e-05,
          6.0929553664495706e-05,
          6.092955366449572e-05,
          6.092955366449572e-05,
          6.092955366449572e-05,
          6.092955366449572e-05,
          6.092955366449572e-05,
          6.092955366449572e-05,
          6.092955366449572e-05,
          6.0929553664495706e-05,
          6.092955366449572e-05,
          6.0929553664495706e-05,
          6.092955366449572e-05,
          6.092955366449572e-05,
          6.0929553664495706e-05,
          6.0929553664495706e-05,
          6.092955366449572e-05,
          6.0929553664495706e-05
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=Signal<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "Signal",
         "line": {
          "color": "#EF553B",
          "dash": "solid"
         },
         "mode": "lines",
         "name": "Signal",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          2,
          8,
          10,
          15,
          23,
          24,
          28,
          38,
          42,
          43,
          44,
          47,
          51,
          64,
          80,
          84,
          87,
          90,
          92,
          94,
          99,
          100,
          103,
          109,
          117,
          120,
          128,
          136,
          146,
          148,
          159,
          164,
          170,
          175,
          177,
          179,
          184,
          187,
          189,
          194,
          200,
          204,
          208,
          209,
          218,
          222,
          225,
          226,
          227,
          228,
          230,
          233,
          238,
          239,
          242,
          243,
          249,
          251,
          252,
          256,
          257,
          262,
          263,
          267,
          270,
          272,
          273,
          274,
          276,
          277,
          285,
          289,
          290,
          291,
          293,
          295,
          302,
          303,
          306,
          307,
          309,
          310,
          313,
          316,
          328,
          335,
          346,
          349,
          350,
          361,
          367,
          376,
          380,
          396,
          400,
          405,
          408,
          411,
          412,
          415,
          423,
          424,
          430,
          431,
          432,
          443,
          446,
          449,
          451,
          452,
          453,
          456,
          465,
          471,
          482,
          490,
          498,
          512,
          513,
          514,
          516,
          518,
          520,
          532,
          550,
          552,
          557,
          574,
          579,
          583,
          589,
          591,
          607,
          612,
          616,
          620,
          626,
          629,
          642,
          643,
          644,
          652,
          657,
          659,
          665,
          669,
          670,
          680,
          683,
          686,
          688,
          696,
          700,
          702,
          708,
          709,
          710,
          712,
          714,
          720,
          722,
          725,
          727,
          733,
          740,
          741,
          744,
          745,
          746,
          756,
          765,
          766,
          767,
          770,
          771,
          773,
          777,
          780,
          782,
          783,
          785,
          789,
          791,
          793,
          794,
          801,
          805,
          808,
          811,
          817,
          827,
          831,
          832,
          834,
          842,
          843,
          849,
          858,
          862,
          865,
          867,
          870,
          871,
          879,
          883,
          888,
          889,
          890,
          893,
          896,
          897,
          899,
          904,
          906,
          911,
          912,
          913,
          914,
          917,
          919,
          922,
          925,
          926,
          942,
          944,
          947,
          957,
          958,
          959,
          963,
          969,
          971,
          972,
          979,
          983,
          986,
          995,
          996,
          998,
          1001,
          1005,
          1007,
          1009,
          1016,
          1019,
          1023,
          1027,
          1029,
          1033,
          1036,
          1041,
          1042,
          1046,
          1049,
          1052,
          1054,
          1061,
          1065,
          1067,
          1074,
          1078,
          1082,
          1084,
          1087,
          1088,
          1104,
          1106,
          1107,
          1108,
          1114,
          1116,
          1119,
          1123,
          1126,
          1127,
          1138,
          1140,
          1149,
          1160,
          1161,
          1162,
          1171,
          1173,
          1179,
          1180,
          1182,
          1183,
          1185,
          1189,
          1196,
          1199,
          1204,
          1209,
          1215,
          1220,
          1224,
          1228,
          1229,
          1234,
          1235,
          1243,
          1254,
          1255,
          1258,
          1262,
          1264,
          1273,
          1274,
          1278,
          1290,
          1295,
          1298,
          1302,
          1304,
          1316,
          1318,
          1327,
          1328,
          1331,
          1334,
          1339,
          1344,
          1345,
          1346,
          1347,
          1353,
          1354,
          1358,
          1360,
          1362,
          1363,
          1367,
          1376,
          1385,
          1394,
          1396,
          1404,
          1405,
          1407,
          1415,
          1423,
          1424,
          1426,
          1432,
          1434,
          1438,
          1439,
          1441,
          1446,
          1452,
          1468,
          1470,
          1479,
          1482,
          1491,
          1492,
          1493,
          1502,
          1508,
          1511,
          1520,
          1522,
          1524,
          1528,
          1536,
          1537,
          1544,
          1545,
          1546,
          1551,
          1552,
          1557,
          1560,
          1561,
          1562,
          1563,
          1565,
          1567,
          1572,
          1576,
          1583,
          1588,
          1594,
          1597,
          1606,
          1609,
          1611,
          1616,
          1617,
          1629,
          1630,
          1631,
          1633,
          1635,
          1650,
          1652,
          1656,
          1657,
          1660,
          1668,
          1677,
          1678,
          1681,
          1692,
          1695,
          1697,
          1701,
          1704,
          1705,
          1706,
          1707,
          1710,
          1711,
          1712,
          1713,
          1714,
          1716,
          1717,
          1720,
          1721,
          1722,
          1723,
          1733,
          1737,
          1740,
          1741,
          1744,
          1745,
          1750,
          1752,
          1753,
          1756,
          1760,
          1769,
          1771,
          1772,
          1774,
          1780,
          1784,
          1785,
          1786,
          1792,
          1795,
          1797,
          1799,
          1800,
          1807,
          1808,
          1810,
          1814,
          1820,
          1821,
          1822,
          1827,
          1830,
          1831,
          1834,
          1835,
          1836,
          1839,
          1844,
          1846,
          1851,
          1852,
          1857,
          1861,
          1864
         ],
         "xaxis": "x",
         "y": [
          0.26878966550807026,
          0.6905562669743748,
          0.7176308346115418,
          0.7191224687952942,
          0.7210362635970898,
          0.7181655713943964,
          0.707330115531289,
          0,
          0.1357358963173522,
          0.1575081265918973,
          0.16595133895276024,
          0.1690471834850766,
          0.1695256321855255,
          0.16792142183696154,
          0.16814657416658454,
          0.10203622138102808,
          0.009808198359202408,
          0.09036770189831556,
          0.11032745591939544,
          0.1127084418051588,
          0.11396085163868677,
          0.113659710397816,
          0.11473199836764561,
          0.11432390977020387,
          0.11138004306038304,
          0.11040344483064321,
          0.1082447968703826,
          0.11075243094155893,
          0.1141437879065055,
          0.11444211474325597,
          0.11292233651830061,
          0.0634394832754035,
          0.07168850175196653,
          0.07995440665325132,
          0.08030902157240755,
          0.08014578613343087,
          0.08039345369601622,
          0.08039345369601622,
          0.08036530965481331,
          0.07978554240603405,
          0.07886804666282032,
          0.07878361453921168,
          0.07831360905112364,
          0.07808845672150064,
          0.0768416756962132,
          0.07659400813362788,
          0.07602831290545006,
          0.07528249581357385,
          0.07481249032548581,
          0.07450290587225417,
          0.07329834090877108,
          0.07293528277725397,
          0.07184329397858236,
          0.07134233004517115,
          0.07090609740652656,
          0.07108059046198442,
          0.07295498360609598,
          0.07296905562669742,
          0.07318013593571901,
          0.07498416897682339,
          0.07512207477871746,
          0.07659400813362788,
          0.07673191393552198,
          0.0773426396296244,
          0.07770288335702119,
          0.0778436035630356,
          0.07787456200835877,
          0.07811941516682377,
          0.07826013537283819,
          0.07867385277852046,
          0.07931553691794602,
          0.08003320996861937,
          0.07997692188621364,
          0.08014578613343087,
          0.08033998001773074,
          0.08044974177842196,
          0.08045255618254224,
          0.08058764758031603,
          0.07997692188621364,
          0.07912134303364618,
          0.07182640755386066,
          0.058750686011004305,
          0.04329116417826434,
          0.3138482754738753,
          0.680255547894122,
          0.6823944950255406,
          0.13227980805763898,
          0.04190366294696252,
          0.07775917143942698,
          0.18390723724019528,
          0.18401981340500675,
          0.18396352532260105,
          0.18475155847628158,
          0.00633803807888772,
          0.09822270379803832,
          0.1257560193068122,
          0.12663692779646227,
          0.12739400250481966,
          0.12768670053332956,
          0.12768670053332956,
          0.12522691133219818,
          0.12483008035123758,
          0.12348479518174013,
          0.12268269000745816,
          0.12273053487750299,
          0.12338347663340979,
          0.12372401953196457,
          0.12523254014043875,
          0.12634422976795237,
          0.12657782530993625,
          0.12698309950325762,
          0.12750939307375145,
          0.12593332676639038,
          0.1117768740413436,
          0.08979637786189718,
          0.09148783473819005,
          0.09137244416925827,
          0.08963595682704076,
          0.08971476014240884,
          0.08942206211389894,
          0.08905337517414125,
          0.08881977963215734,
          0.08878037797447333,
          0.08702700420753415,
          0.0818710158591672,
          0.08230161968957123,
          0.08374540900327879,
          0.08837228937703165,
          0.08874097631678934,
          0.08913499289362958,
          0.08969787371768712,
          0.09002997340388105,
          0.09122328075088298,
          0.09045494842604448,
          0.08829067165754328,
          0.03766798474592964,
          0.4101008963877123,
          0.6579091791790382,
          0.8839339740793378,
          0.8847220072330185,
          0.883033364760846,
          0.854326442733912,
          0.5307825450656458,
          0.3474522606701096,
          0.11846108382702672,
          0.27045016393904,
          0.2832838467275516,
          0.28992584045143044,
          0.2896444000394016,
          0.2888000788033154,
          0.2884623503088808,
          0.29017913682225627,
          0.29082644976992245,
          0.2892222394213585,
          0.24495166260923404,
          0.20608474170806182,
          0.16583876278794865,
          0.08573800712044241,
          0.00029551243263017946,
          0.1772370994751136,
          0.20740751164459706,
          0.21095366083615943,
          0.2119949903606659,
          0.21328961625599815,
          0.2127830235143464,
          0.21255787118472338,
          0.21067222042413067,
          0.2103344919296962,
          0.20957460281721849,
          0.20597216554325035,
          0.20791410438624877,
          0.2080266805510603,
          0.20808296863346606,
          0.20903986603436384,
          0.20974346706443578,
          0.2110099489185652,
          0.21289559967915786,
          0.21354291262682404,
          0.2138524970800557,
          0.2140495053684758,
          0.21343033646201257,
          0.2112351012481882,
          0.20025892517906643,
          0.15044397224997533,
          0.11747322798080576,
          0.10094704698647677,
          0.16254590996721222,
          0.16400940010976175,
          0.1641501203157761,
          0.16448784881021064,
          0.16350280736810993,
          0.1628836384616467,
          0.16282735037924093,
          0.1624614778436035,
          0.16127942811308274,
          0.1611668519482712,
          0.15998480221775044,
          0.15764884679791172,
          0.15511588308965277,
          0.1545248582243924,
          0.15339909657627734,
          0.1528643597934227,
          0.15244219917537957,
          0.15249848725778534,
          0.15387754527672623,
          0.1560446364493477,
          0.15694524576783983,
          0.1572829742622743,
          0.15894347269324396,
          0.15970336180572167,
          0.1596470737233159,
          0.16018181050617056,
          0.16136386023669133,
          0.16150458044270574,
          0.16220818147277768,
          0.1624614778436035,
          0.16260219804961792,
          0.16251776592600928,
          0.1630806467500668,
          0.16336208716209558,
          0.16384053586254452,
          0.16434712860419629,
          0.16448784881021064,
          0.1399152864359793,
          0.08602226193659146,
          0.00044186144688512785,
          0.9730942966100501,
          0.9922885327104117,
          0.9965382829320463,
          0.9957502497783656,
          0.9980862051982043,
          0.9999999999999999,
          0.9992401108875223,
          0.5788244233989558,
          0.13602015113350127,
          0.07726946512249694,
          0.3097111014170525,
          0.3096829573758496,
          0.3095703812110381,
          0.3104428464883272,
          0.3099925418290812,
          0.30948594908742943,
          0.30858533976893743,
          0.31100572731238474,
          0.31038655840592144,
          0.3100206858702841,
          0.3054050631130123,
          0.2667070064590574,
          0.10306066448081277,
          0.011744508393960296,
          0.18247189113884862,
          0.20819554479827754,
          0.22806523788750824,
          0.22935986378284054,
          0.2308796420077959,
          0.23107665029621596,
          0.22997903268930378,
          0.22699576432179894,
          0.226067010962104,
          0.22356219129504804,
          0.22297116642978762,
          0.22502568143759763,
          0.225841858632481,
          0.22623587520932129,
          0.22657360370375576,
          0.2302604731013326,
          0.22899399124720318,
          0.22823410213472548,
          0.22792451768149388,
          0.1293866006219833,
          0.06266270773820412,
          0.030544727917481668,
          0.14650380648157266,
          0.1767586507746647,
          0.17794070050518554,
          0.17979820722457537,
          0.1797419191421696,
          0.1783065730408229,
          0.17656164248624454,
          0.1763927782390273,
          0.1763083461154187,
          0.17490114405527488,
          0.17442269535482594,
          0.17152385911092968,
          0.17076396999845203,
          0.16946934410311978,
          0.1695256321855255,
          0.16918790369109096,
          0.16730225293049825,
          0.16724596484809254,
          0.1686531669082363,
          0.1694974881443226,
          0.17293106117107357,
          0.17473227980805764,
          0.1757173212501583,
          0.17636463419782447,
          0.17729338755751942,
          0.17729338755751942,
          0.17782812434037396,
          0.1778562683815769,
          0.1790664621533006,
          0.17878502174127178,
          0.17858801345285166,
          0.1772370994751136,
          0.14689782305841295,
          0.09308923068263372,
          0.4791382294583678,
          0.5330059243206732,
          0.594922814967001,
          0.5957389921618844,
          0.5908137849513811,
          0.5067475338783896,
          0.20225715210447068,
          0.01735924461393412,
          0.09361270984900724,
          0.09335097026582043,
          0.09264736923574854,
          0.09273461576347747,
          0.09289503679833389,
          0.09403487046705036,
          0.09412211699477924,
          0.09357893699956374,
          0.09196628343863897,
          0.08855804004897064,
          0.07874421288152764,
          0.045759396591756596,
          0.04963483106539268,
          0.05123341260571604,
          0.05167527405260122,
          0.051897611978103914,
          0.05211713549948635,
          0.05284325176252058,
          0.05134317436640726,
          0.048855241124072996,
          0.048469667759593604,
          0.04885805552819328,
          0.05085065364535693,
          0.05123341260571604,
          0.05217905239013268,
          0.05256744015873238,
          0.05002040442987207,
          0.047251030775509045,
          0.02487933242334267,
          0.02653138764195148,
          0.027158999760775654,
          0.027158999760775654,
          0.02711678369897133,
          0.02719840141845964,
          0.02678186960865711,
          0.026227431996960426,
          0.025101670348845362,
          0.02483993076565863,
          0.02350871761676257,
          0.02220283410494911,
          0.02057610852342287,
          0.020120175055936246,
          0.020258080857830374,
          0.02131066799881795,
          0.02234073990684321,
          0.023331410157184457,
          0.02496376454695129,
          0.025020052629357026,
          0.025059454287041066,
          0.02557449024105371,
          0.026128927852750367,
          0.026089526195066326,
          0.026880373752867198,
          0.026961991472355507,
          0.027060495616565594,
          0.02719840141845964,
          0.027308163179150885,
          0.027240617480263962,
          0.026961991472355507,
          0.02653138764195148,
          0.02543658443915961,
          0.023058412957516566,
          0.0019728972883216456,
          0.117231189226461,
          0.45634155608403804,
          0.4797573983648311,
          0.4774777310273981,
          0.479785542406034,
          0.36751896204776036,
          0.1370277078085642,
          0.10487032633015775,
          0.10442846488327256,
          0.10472397731590277,
          0.10438343441734796,
          0.10444816571211463,
          0.10489002715899973,
          0.10484499669307512,
          0.1047436781447448,
          0.10444816571211463,
          0.10480840943951142,
          0.06000872465277288,
          0.06054909024386809,
          0.061798685673275816,
          0.06195910670813223,
          0.06215048618831179,
          0.06140748350055583,
          0.059108115334280814,
          0.05875912922336518,
          0.05767558363705444,
          0.05919254745788943,
          0.0598511180220368,
          0.06094029241658808,
          0.06179024246091494,
          0.06203228121525972,
          0.06203228121525972,
          0.06195066349577133,
          0.06203228121525972,
          0.06164107904253971,
          0.0612076608080154,
          0.060974065266031524,
          0.06047591573674063,
          0.06055753345622894,
          0.05976668589842818,
          0.05872535637392173,
          0.02542814122679876,
          0.0029917115798657468,
          0.007548231850611403,
          0.02183133276107116,
          0.035478378340345895,
          0.035514965593909625,
          0.035388317408496656,
          0.03538268860025609,
          0.03515753627063309,
          0.03508154735938532,
          0.03460028425481612,
          0.03438076073343371,
          0.03434417347986998,
          0.03411902115024695,
          0.033663087682760384,
          0.033001702714492764,
          0.0328075088301929,
          0.03271744789834369,
          0.03234876095858602,
          0.030404007711467285,
          0.029818611654447436,
          0.0296244177701476,
          0.02948932637237378,
          0.028180628456440066,
          0.028315719854213883,
          0.0288054261711439,
          0.029323276529276826,
          0.029458367927050644,
          0.03092185806960021,
          0.03138060594120709,
          0.03192941474466318,
          0.03255139805524676,
          0.033288771934762124,
          0.033325359188325826,
          0.03341542012017504,
          0.03387416799178192,
          0.03400925938955571,
          0.03400363058131517,
          0.034324472651027976,
          0.034332915863388797,
          0.0344286056034786,
          0.034693159590785616,
          0.03517442269535481,
          0.03514346425003165,
          0.03524196839424171,
          0.03517442269535481,
          0.034369503116952554,
          0.03208702137539929,
          0.001398758847782955
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=GB<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "GB",
         "line": {
          "color": "#00cc96",
          "dash": "solid"
         },
         "mode": "lines",
         "name": "GB",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          2,
          8,
          10,
          15,
          23,
          24,
          28,
          38,
          42,
          43,
          44,
          47,
          51,
          64,
          80,
          84,
          87,
          90,
          92,
          94,
          99,
          100,
          103,
          109,
          117,
          120,
          128,
          136,
          146,
          148,
          159,
          164,
          170,
          175,
          177,
          179,
          184,
          187,
          189,
          194,
          200,
          204,
          208,
          209,
          218,
          222,
          225,
          226,
          227,
          228,
          230,
          233,
          238,
          239,
          242,
          243,
          249,
          251,
          252,
          256,
          257,
          262,
          263,
          267,
          270,
          272,
          273,
          274,
          276,
          277,
          285,
          289,
          290,
          291,
          293,
          295,
          302,
          303,
          306,
          307,
          309,
          310,
          313,
          316,
          328,
          335,
          346,
          349,
          350,
          361,
          367,
          376,
          380,
          396,
          400,
          405,
          408,
          411,
          412,
          415,
          423,
          424,
          430,
          431,
          432,
          443,
          446,
          449,
          451,
          452,
          453,
          456,
          465,
          471,
          482,
          490,
          498,
          512,
          513,
          514,
          516,
          518,
          520,
          532,
          550,
          552,
          557,
          574,
          579,
          583,
          589,
          591,
          607,
          612,
          616,
          620,
          626,
          629,
          642,
          643,
          644,
          652,
          657,
          659,
          665,
          669,
          670,
          680,
          683,
          686,
          688,
          696,
          700,
          702,
          708,
          709,
          710,
          712,
          714,
          720,
          722,
          725,
          727,
          733,
          740,
          741,
          744,
          745,
          746,
          756,
          765,
          766,
          767,
          770,
          771,
          773,
          777,
          780,
          782,
          783,
          785,
          789,
          791,
          793,
          794,
          801,
          805,
          808,
          811,
          817,
          827,
          831,
          832,
          834,
          842,
          843,
          849,
          858,
          862,
          865,
          867,
          870,
          871,
          879,
          883,
          888,
          889,
          890,
          893,
          896,
          897,
          899,
          904,
          906,
          911,
          912,
          913,
          914,
          917,
          919,
          922,
          925,
          926,
          942,
          944,
          947,
          957,
          958,
          959,
          963,
          969,
          971,
          972,
          979,
          983,
          986,
          995,
          996,
          998,
          1001,
          1005,
          1007,
          1009,
          1016,
          1019,
          1023,
          1027,
          1029,
          1033,
          1036,
          1041,
          1042,
          1046,
          1049,
          1052,
          1054,
          1061,
          1065,
          1067,
          1074,
          1078,
          1082,
          1084,
          1087,
          1088,
          1104,
          1106,
          1107,
          1108,
          1114,
          1116,
          1119,
          1123,
          1126,
          1127,
          1138,
          1140,
          1149,
          1160,
          1161,
          1162,
          1171,
          1173,
          1179,
          1180,
          1182,
          1183,
          1185,
          1189,
          1196,
          1199,
          1204,
          1209,
          1215,
          1220,
          1224,
          1228,
          1229,
          1234,
          1235,
          1243,
          1254,
          1255,
          1258,
          1262,
          1264,
          1273,
          1274,
          1278,
          1290,
          1295,
          1298,
          1302,
          1304,
          1316,
          1318,
          1327,
          1328,
          1331,
          1334,
          1339,
          1344,
          1345,
          1346,
          1347,
          1353,
          1354,
          1358,
          1360,
          1362,
          1363,
          1367,
          1376,
          1385,
          1394,
          1396,
          1404,
          1405,
          1407,
          1415,
          1423,
          1424,
          1426,
          1432,
          1434,
          1438,
          1439,
          1441,
          1446,
          1452,
          1468,
          1470,
          1479,
          1482,
          1491,
          1492,
          1493,
          1502,
          1508,
          1511,
          1520,
          1522,
          1524,
          1528,
          1536,
          1537,
          1544,
          1545,
          1546,
          1551,
          1552,
          1557,
          1560,
          1561,
          1562,
          1563,
          1565,
          1567,
          1572,
          1576,
          1583,
          1588,
          1594,
          1597,
          1606,
          1609,
          1611,
          1616,
          1617,
          1629,
          1630,
          1631,
          1633,
          1635,
          1650,
          1652,
          1656,
          1657,
          1660,
          1668,
          1677,
          1678,
          1681,
          1692,
          1695,
          1697,
          1701,
          1704,
          1705,
          1706,
          1707,
          1710,
          1711,
          1712,
          1713,
          1714,
          1716,
          1717,
          1720,
          1721,
          1722,
          1723,
          1733,
          1737,
          1740,
          1741,
          1744,
          1745,
          1750,
          1752,
          1753,
          1756,
          1760,
          1769,
          1771,
          1772,
          1774,
          1780,
          1784,
          1785,
          1786,
          1792,
          1795,
          1797,
          1799,
          1800,
          1807,
          1808,
          1810,
          1814,
          1820,
          1821,
          1822,
          1827,
          1830,
          1831,
          1834,
          1835,
          1836,
          1839,
          1844,
          1846,
          1851,
          1852,
          1857,
          1861,
          1864
         ],
         "xaxis": "x",
         "y": [
          0.2585097465903057,
          0.33053474732593535,
          0.4480726660347786,
          0.7205746010478079,
          0.7296437002929632,
          0.7303322343250236,
          0.7290718353298782,
          0.09885304606556858,
          0.10128346934173585,
          0.10128346934173585,
          0.10128346934173585,
          0.10128346934173585,
          0.14146351525841586,
          0.16448069796976186,
          0.14146351525841586,
          0.13851980747689938,
          0.0746594722093388,
          0.07272057822899328,
          0.07272057822899328,
          0.074386323679138,
          0.07812645637977675,
          0.07812645637977675,
          0.07812645637977675,
          0.08809674037990042,
          0.09502927443600442,
          0.09414752413429975,
          0.11463770155063294,
          0.09348606214891095,
          0.09459613641347994,
          0.08776225107792754,
          0.07812645637977675,
          0.07812645637977675,
          0.04442017040797108,
          0.05022143258021258,
          0.05669952999475436,
          0.05669952999475436,
          0.05669952999475436,
          0.05669952999475436,
          0.05831532731918715,
          0.05831532731918715,
          0.05831532731918715,
          0.05831532731918715,
          0.05831532731918715,
          0.056376433338841625,
          0.0604463703875337,
          0.0604463703875337,
          0.0604463703875337,
          0.0604463703875337,
          0.0604463703875337,
          0.0604463703875337,
          0.06020979979260113,
          0.059598152970495866,
          0.05893669098510709,
          0.05893669098510709,
          0.05833676305131921,
          0.05833676305131921,
          0.05893669098510709,
          0.05893669098510709,
          0.059598152970495866,
          0.06020979979260113,
          0.059877910034736526,
          0.0604463703875337,
          0.0604463703875337,
          0.0604463703875337,
          0.0604463703875337,
          0.05804217878898632,
          0.05804217878898632,
          0.05804217878898632,
          0.05804217878898632,
          0.056376433338841625,
          0.05831532731918715,
          0.05831532731918715,
          0.05831532731918715,
          0.05831532731918715,
          0.05831532731918715,
          0.05831532731918715,
          0.05669952999475436,
          0.05669952999475436,
          0.05669952999475436,
          0.05669952999475436,
          0.05669952999475436,
          0.05669952999475436,
          0.027480245089547745,
          0.3068124631294773,
          0.7081782605225133,
          0.6995354143815429,
          0.13868874237239712,
          0.0576375742302114,
          0.08410094191810938,
          0.16329255714077898,
          0.16630167327478332,
          0.16630167327478332,
          0.16630167327478332,
          0.0576375742302114,
          0.0829413958134618,
          0.09280868246334581,
          0.09280868246334581,
          0.10511939575529997,
          0.10511939575529997,
          0.10511939575529997,
          0.10555253377782445,
          0.10528243029822501,
          0.10400932149073097,
          0.10400932149073097,
          0.10400932149073097,
          0.10400932149073097,
          0.10467078347611977,
          0.10555253377782445,
          0.10511939575529997,
          0.10511939575529997,
          0.10511939575529997,
          0.10511939575529997,
          0.09280868246334581,
          0.09071118912314644,
          0.06740154752017108,
          0.06883360572662797,
          0.07027064486746407,
          0.07027064486746407,
          0.07027064486746407,
          0.07027064486746407,
          0.06999749633726324,
          0.06999749633726324,
          0.06999749633726324,
          0.07240168793581062,
          0.06886002239313924,
          0.06945995032692712,
          0.06945995032692712,
          0.07240168793581062,
          0.06999749633726324,
          0.06833175088711854,
          0.07027064486746407,
          0.07027064486746407,
          0.06883360572662797,
          0.06883360572662797,
          0.06740154752017108,
          0.044851493734591186,
          0.364093238416479,
          0.5097586778488672,
          0.9026589993998115,
          0.9039193983949569,
          0.9039193983949569,
          0.8888589155536502,
          0.8101818355520768,
          0.7181227395657762,
          0.19988924793350196,
          0.22281267245317168,
          0.22281267245317168,
          0.2696737030302099,
          0.2726828191642142,
          0.2726828191642142,
          0.28407714388999705,
          0.28407714388999705,
          0.2726828191642142,
          0.2726828191642142,
          0.26798202634131796,
          0.24848753259358328,
          0.2453628096767815,
          0.2287155167949569,
          0.22281267245317168,
          0.14575857113660665,
          0.14575857113660665,
          0.14575857113660665,
          0.18701767411419892,
          0.18701767411419892,
          0.18701767411419892,
          0.18701767411419892,
          0.18596424392990044,
          0.1941594797003581,
          0.19609937779249906,
          0.19482626898500502,
          0.19482626898500502,
          0.19482626898500502,
          0.19482626898500502,
          0.19548773097039382,
          0.19548773097039382,
          0.19609937779249906,
          0.18701767411419892,
          0.18701767411419892,
          0.18701767411419892,
          0.18701767411419892,
          0.18701767411419892,
          0.18701767411419892,
          0.18701767411419892,
          0.18701767411419892,
          0.18701767411419892,
          0.09448419285978374,
          0.10317156006066144,
          0.1369794396287575,
          0.14350465141668547,
          0.14350465141668547,
          0.14350465141668547,
          0.14350465141668547,
          0.14350465141668547,
          0.14350465141668547,
          0.14323150288648465,
          0.14323150288648465,
          0.14563569448503197,
          0.14563569448503197,
          0.15431779302547446,
          0.15370614620336917,
          0.15304468421798037,
          0.15304468421798037,
          0.15304468421798037,
          0.15304468421798037,
          0.15304468421798037,
          0.15370614620336917,
          0.15370614620336917,
          0.15431779302547446,
          0.1450672341322348,
          0.14563569448503197,
          0.14563569448503197,
          0.14563569448503197,
          0.14563569448503197,
          0.14563569448503197,
          0.14323150288648465,
          0.14323150288648465,
          0.14323150288648465,
          0.14323150288648465,
          0.14350465141668547,
          0.14350465141668547,
          0.14350465141668547,
          0.14350465141668547,
          0.14350465141668547,
          0.14350465141668547,
          0.1369794396287575,
          0.10317156006066144,
          0.986149308595025,
          0.9936283193194417,
          0.9936283193194417,
          1,
          1,
          0.9964587727234804,
          0.9964587727234804,
          0.6832486176640491,
          0.17303805807489045,
          0.12527446104728643,
          0.28287503118622803,
          0.28287503118622803,
          0.28287503118622803,
          0.2836193677133174,
          0.2902341035570244,
          0.2902341035570244,
          0.2911087176601682,
          0.2902341035570244,
          0.2836193677133174,
          0.28287503118622803,
          0.28287503118622803,
          0.26531055832537864,
          0.16139343315227214,
          0.08761806919064921,
          0.16075557629405587,
          0.1816272508614946,
          0.20183954741814938,
          0.20183954741814938,
          0.20183954741814938,
          0.20183954741814938,
          0.2007861172338509,
          0.21030960427434428,
          0.21030960427434428,
          0.20964814228895548,
          0.2090482143551676,
          0.20964814228895548,
          0.20964814228895548,
          0.21030960427434428,
          0.21030960427434428,
          0.20183954741814938,
          0.20183954741814938,
          0.20183954741814938,
          0.20183954741814938,
          0.1569691853639599,
          0.09411320837073964,
          0.00018096296503503084,
          0.11074157160010542,
          0.1591557540115975,
          0.1623056826290337,
          0.16257792079174263,
          0.16257792079174263,
          0.16257792079174263,
          0.16230477226154175,
          0.16302607083731915,
          0.16302607083731915,
          0.16302607083731915,
          0.16302607083731915,
          0.16492844006638022,
          0.16391698666392315,
          0.16391698666392315,
          0.16391698666392315,
          0.16311135523644507,
          0.16255061828598857,
          0.16218988740785753,
          0.16301609188360563,
          0.16391698666392315,
          0.1644485925454526,
          0.16302607083731915,
          0.16302607083731915,
          0.16302607083731915,
          0.16230477226154175,
          0.16230477226154175,
          0.16257792079174263,
          0.16257792079174263,
          0.16257792079174263,
          0.16257792079174263,
          0.16257792079174263,
          0.16257792079174263,
          0.14343570309029993,
          0.10885266279575198,
          0.3926203821049883,
          0.3530882652157251,
          0.4568894456238066,
          0.6125620810438458,
          0.6119902160807609,
          0.5337867840063347,
          0.5190107439833128,
          0.0493236715970832,
          0.05591006959735895,
          0.0758283297041015,
          0.07790805810057147,
          0.07492298993721275,
          0.07492298993721275,
          0.07790805810057147,
          0.08011734604277645,
          0.05591006959735895,
          0.05591006959735895,
          0.05591006959735895,
          0.05591006959735895,
          0.01935587242097031,
          0.01935587242097031,
          0.01935587242097031,
          0.01935587242097031,
          0.01935587242097031,
          0.017416978440624786,
          0.021180217230968884,
          0.03108673892437569,
          0.03221319242179718,
          0.04835031604634529,
          0.03198682845888132,
          0.03251065219708049,
          0.03108673892437569,
          0.03108673892437569,
          0.03108673892437569,
          0.01935587242097031,
          0.01935587242097031,
          0.01935587242097031,
          0.0023381096933970946,
          0.0023381096933970946,
          0.0023381096933970946,
          0.002931168489124031,
          0.0032105829439158518,
          0.0032105829439158518,
          0.004100520847243216,
          0.0038273723170424157,
          0.0038273723170424157,
          0.0038273723170424157,
          0.0038273723170424157,
          0.0072718231504592945,
          0.0072718231504592945,
          0.0072718231504592945,
          0.0072718231504592945,
          0.0032589119642452424,
          0.0038273723170424157,
          0.0038273723170424157,
          0.0038273723170424157,
          0.0038273723170424157,
          0.004100520847243216,
          0.004100520847243216,
          0.004100520847243216,
          0.004100520847243216,
          0.0032105829439158518,
          0.0032105829439158518,
          0.0032105829439158518,
          0.0032105829439158518,
          0.0023381096933970946,
          0.0023381096933970946,
          0.0023381096933970946,
          0.0023381096933970946,
          0.0023381096933970946,
          0,
          0.10504410226421709,
          0.43013141829003465,
          0.5121804648279,
          0.4992116218277194,
          0.5121804648279,
          0.3514635901040063,
          0.10504410226421709,
          0.07116733883633228,
          0.08709512116187432,
          0.09151495422122755,
          0.08917484955834429,
          0.08917484955834429,
          0.0905814208212852,
          0.09151495422122755,
          0.09151495422122755,
          0.08709512116187432,
          0.07975899470816653,
          0.02734256998424542,
          0.02734256998424542,
          0.02813463556088,
          0.0349685208964324,
          0.04487504258983921,
          0.04487504258983921,
          0.0422784001184473,
          0.04087182885550639,
          0.04087182885550639,
          0.042939862103836074,
          0.04382161240554075,
          0.04487504258983921,
          0.04487504258983921,
          0.04487504258983921,
          0.04487504258983921,
          0.04487504258983921,
          0.0349685208964324,
          0.02813463556088,
          0.026037142220680626,
          0.024371396770535902,
          0.02734256998424542,
          0.02734256998424542,
          0.02734256998424542,
          0.02734256998424542,
          0.02330146791776977,
          0.02330146791776977,
          0.011086465655300082,
          0.011086465655300082,
          0.013040127218813813,
          0.01391260046933257,
          0.01391260046933257,
          0.01391260046933257,
          0.01391260046933257,
          0.01372985777095767,
          0.01372985777095767,
          0.01372985777095767,
          0.01372985777095767,
          0.01372985777095767,
          0.01372985777095767,
          0.01345670924075687,
          0.01345670924075687,
          0.01345670924075687,
          0.01345670924075687,
          0.01345670924075687,
          0.011121923616896151,
          0.011121923616896151,
          0.010460461631507378,
          0.008453962434778556,
          0.008453962434778556,
          0.009053890368566464,
          0.009053890368566464,
          0.010460461631507378,
          0.01345670924075687,
          0.01345670924075687,
          0.01345670924075687,
          0.01345670924075687,
          0.01345670924075687,
          0.01345670924075687,
          0.01345670924075687,
          0.01372985777095767,
          0.01372985777095767,
          0.01372985777095767,
          0.01372985777095767,
          0.01372985777095767,
          0.01372985777095767,
          0.01372985777095767,
          0.01391260046933257,
          0.01391260046933257,
          0.013040127218813813,
          0.013040127218813813,
          0.013040127218813813,
          0.012159146257002346,
          0.011086465655300082
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=RF<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "RF",
         "line": {
          "color": "#ab63fa",
          "dash": "solid"
         },
         "mode": "lines",
         "name": "RF",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          2,
          8,
          10,
          15,
          23,
          24,
          28,
          38,
          42,
          43,
          44,
          47,
          51,
          64,
          80,
          84,
          87,
          90,
          92,
          94,
          99,
          100,
          103,
          109,
          117,
          120,
          128,
          136,
          146,
          148,
          159,
          164,
          170,
          175,
          177,
          179,
          184,
          187,
          189,
          194,
          200,
          204,
          208,
          209,
          218,
          222,
          225,
          226,
          227,
          228,
          230,
          233,
          238,
          239,
          242,
          243,
          249,
          251,
          252,
          256,
          257,
          262,
          263,
          267,
          270,
          272,
          273,
          274,
          276,
          277,
          285,
          289,
          290,
          291,
          293,
          295,
          302,
          303,
          306,
          307,
          309,
          310,
          313,
          316,
          328,
          335,
          346,
          349,
          350,
          361,
          367,
          376,
          380,
          396,
          400,
          405,
          408,
          411,
          412,
          415,
          423,
          424,
          430,
          431,
          432,
          443,
          446,
          449,
          451,
          452,
          453,
          456,
          465,
          471,
          482,
          490,
          498,
          512,
          513,
          514,
          516,
          518,
          520,
          532,
          550,
          552,
          557,
          574,
          579,
          583,
          589,
          591,
          607,
          612,
          616,
          620,
          626,
          629,
          642,
          643,
          644,
          652,
          657,
          659,
          665,
          669,
          670,
          680,
          683,
          686,
          688,
          696,
          700,
          702,
          708,
          709,
          710,
          712,
          714,
          720,
          722,
          725,
          727,
          733,
          740,
          741,
          744,
          745,
          746,
          756,
          765,
          766,
          767,
          770,
          771,
          773,
          777,
          780,
          782,
          783,
          785,
          789,
          791,
          793,
          794,
          801,
          805,
          808,
          811,
          817,
          827,
          831,
          832,
          834,
          842,
          843,
          849,
          858,
          862,
          865,
          867,
          870,
          871,
          879,
          883,
          888,
          889,
          890,
          893,
          896,
          897,
          899,
          904,
          906,
          911,
          912,
          913,
          914,
          917,
          919,
          922,
          925,
          926,
          942,
          944,
          947,
          957,
          958,
          959,
          963,
          969,
          971,
          972,
          979,
          983,
          986,
          995,
          996,
          998,
          1001,
          1005,
          1007,
          1009,
          1016,
          1019,
          1023,
          1027,
          1029,
          1033,
          1036,
          1041,
          1042,
          1046,
          1049,
          1052,
          1054,
          1061,
          1065,
          1067,
          1074,
          1078,
          1082,
          1084,
          1087,
          1088,
          1104,
          1106,
          1107,
          1108,
          1114,
          1116,
          1119,
          1123,
          1126,
          1127,
          1138,
          1140,
          1149,
          1160,
          1161,
          1162,
          1171,
          1173,
          1179,
          1180,
          1182,
          1183,
          1185,
          1189,
          1196,
          1199,
          1204,
          1209,
          1215,
          1220,
          1224,
          1228,
          1229,
          1234,
          1235,
          1243,
          1254,
          1255,
          1258,
          1262,
          1264,
          1273,
          1274,
          1278,
          1290,
          1295,
          1298,
          1302,
          1304,
          1316,
          1318,
          1327,
          1328,
          1331,
          1334,
          1339,
          1344,
          1345,
          1346,
          1347,
          1353,
          1354,
          1358,
          1360,
          1362,
          1363,
          1367,
          1376,
          1385,
          1394,
          1396,
          1404,
          1405,
          1407,
          1415,
          1423,
          1424,
          1426,
          1432,
          1434,
          1438,
          1439,
          1441,
          1446,
          1452,
          1468,
          1470,
          1479,
          1482,
          1491,
          1492,
          1493,
          1502,
          1508,
          1511,
          1520,
          1522,
          1524,
          1528,
          1536,
          1537,
          1544,
          1545,
          1546,
          1551,
          1552,
          1557,
          1560,
          1561,
          1562,
          1563,
          1565,
          1567,
          1572,
          1576,
          1583,
          1588,
          1594,
          1597,
          1606,
          1609,
          1611,
          1616,
          1617,
          1629,
          1630,
          1631,
          1633,
          1635,
          1650,
          1652,
          1656,
          1657,
          1660,
          1668,
          1677,
          1678,
          1681,
          1692,
          1695,
          1697,
          1701,
          1704,
          1705,
          1706,
          1707,
          1710,
          1711,
          1712,
          1713,
          1714,
          1716,
          1717,
          1720,
          1721,
          1722,
          1723,
          1733,
          1737,
          1740,
          1741,
          1744,
          1745,
          1750,
          1752,
          1753,
          1756,
          1760,
          1769,
          1771,
          1772,
          1774,
          1780,
          1784,
          1785,
          1786,
          1792,
          1795,
          1797,
          1799,
          1800,
          1807,
          1808,
          1810,
          1814,
          1820,
          1821,
          1822,
          1827,
          1830,
          1831,
          1834,
          1835,
          1836,
          1839,
          1844,
          1846,
          1851,
          1852,
          1857,
          1861,
          1864
         ],
         "xaxis": "x",
         "y": [
          0.22224152244255702,
          0.24833053528294258,
          0.3931432487410069,
          0.6885046986144835,
          0.7183556060702182,
          0.7179929690768827,
          0.7109070535950343,
          0.0549607625210119,
          0.11366926184685783,
          0.13857465046632272,
          0.13888413290977325,
          0.07846667359636772,
          0.16022842720200886,
          0.16236957697300547,
          0.1623854920761616,
          0.14158520537694574,
          0.0452078167881744,
          0.07732881477029549,
          0.09702764844190767,
          0.03807248112171552,
          0.10294901888007263,
          0.10430765713269069,
          0.10771616256643163,
          0.10795020268428557,
          0.10606147781727077,
          0.10445895860801374,
          0.10273541261787766,
          0.10354546673799109,
          0.10695930752059929,
          0.10731942746458423,
          0.10715878865507983,
          0.09234276310300288,
          0.06033761617758876,
          0.039455417379146096,
          0.071000223329091,
          0.07106794635599303,
          0.07346107226836884,
          0.07365101679154684,
          0.07345371451047472,
          0.07324781286339632,
          0.07225772861210863,
          0.07196141307453663,
          0.07172530304594449,
          0.0712320385804836,
          0.0703490203436937,
          0.06974512838574004,
          0.06942869966289109,
          0.06914998641887513,
          0.06891657437919926,
          0.06811760778095094,
          0.06664403419905335,
          0.06610128641636692,
          0.06518791201496266,
          0.06479610870043892,
          0.06422912746654794,
          0.06422912746654794,
          0.06506324370690891,
          0.06555526054196195,
          0.06568393272998507,
          0.06664403419905335,
          0.06724783413090035,
          0.06933411847842219,
          0.0693667444398918,
          0.07002382552518713,
          0.07045952942024047,
          0.07057212877506791,
          0.07075995541213306,
          0.07083705129800083,
          0.07097234388473886,
          0.0712320385804836,
          0.07190896428369026,
          0.07250828633345197,
          0.07256118984153162,
          0.07265747621562427,
          0.07317581833648656,
          0.07322004953433767,
          0.07346107226836884,
          0.07359508609822121,
          0.07332609155540007,
          0.07106794635599303,
          0.071000223329091,
          0.0644881315604951,
          0.08083750767305739,
          0.33200519902842957,
          0.6797560337838794,
          0.6813580215423716,
          0.1428891638822929,
          0.04861124265149161,
          0.071418864365973,
          0.1779895260148342,
          0.1781516990213367,
          0.1779442106005813,
          0.17885838170447044,
          0.023005518443603795,
          0.08805895494185614,
          0.11868445852046927,
          0.11995650345467829,
          0.12085699609580247,
          0.12088488257744495,
          0.1209553286974438,
          0.11879670986774726,
          0.11789546931150774,
          0.11682055174110134,
          0.11635331841623947,
          0.11567993903378698,
          0.11713046115862086,
          0.11729881101153689,
          0.11879670986774726,
          0.1198252711139004,
          0.12039565467475494,
          0.12058728956864717,
          0.12099470436784104,
          0.11963622716090944,
          0.10724628146101517,
          0.08108571811968437,
          0.08426903885068496,
          0.08406912857681587,
          0.0827823740137815,
          0.08259668549503876,
          0.08254200574776765,
          0.08213886102759063,
          0.0819513327987095,
          0.08187740809187816,
          0.07986680384917175,
          0.07500464131300749,
          0.07527541974130716,
          0.07693544647062833,
          0.08151959536418638,
          0.0819556810322502,
          0.08240456746408689,
          0.08303747890745594,
          0.08326474658052044,
          0.08449766120217317,
          0.08400619997473957,
          0.08108571811968437,
          0.039758786244131095,
          0.3052158884927238,
          0.4811905397121723,
          0.8862495711270465,
          0.8866970413041667,
          0.8858793081467798,
          0.8753033621152772,
          0.818135155840003,
          0.7523647297346672,
          0.128843133283092,
          0.21725397636602423,
          0.1710808190489953,
          0.2845988007354005,
          0.2850484023995518,
          0.2855760164889942,
          0.2844818531470326,
          0.2844818531470326,
          0.28524464130542876,
          0.28530103097896076,
          0.2828873553662983,
          0.2844017092347126,
          0.2819074052193439,
          0.2607517191094179,
          0.1710808190489953,
          0.15039724168526442,
          0.12402569381078515,
          0.1001939141640224,
          0.19651319036407705,
          0.2070932931336229,
          0.20757000460614333,
          0.20752527179834446,
          0.20691808219312335,
          0.2067162066882126,
          0.2058715694278735,
          0.20107204797986805,
          0.20065336708756182,
          0.2009269931825336,
          0.20132423131531924,
          0.2022604041019938,
          0.2025327607780233,
          0.20394214736850247,
          0.20655107801969547,
          0.20757000460614333,
          0.20755776987059232,
          0.2077124400427503,
          0.20789651052600483,
          0.20667831358546163,
          0.2032005319812692,
          0.19651319036407705,
          0.17134291711478178,
          0.09083397343073016,
          0.11512884258856992,
          0.1276475502696074,
          0.15075464943042022,
          0.1584856760400699,
          0.15759012939618272,
          0.15738800758610466,
          0.15719503696034048,
          0.1567303727878449,
          0.1556788391864795,
          0.15562980551151823,
          0.15440212582201052,
          0.15259974038961682,
          0.14905999408914988,
          0.1490883191834566,
          0.14818524128539734,
          0.14749364479379645,
          0.14730350261748848,
          0.14551333229743108,
          0.14712690587208765,
          0.14860423479580126,
          0.1486273116953773,
          0.14905999408914988,
          0.1515251867283287,
          0.1527275727717496,
          0.15292879462063835,
          0.153372286021971,
          0.1544391189635726,
          0.15457588938131733,
          0.15540600884666567,
          0.155433576078918,
          0.15566500820397539,
          0.1558893164257153,
          0.15615021043816224,
          0.1567062159348402,
          0.15709244138464007,
          0.15759012939618272,
          0.15790395059989842,
          0.15052182378650913,
          0.1276475502696074,
          0.11512884258856992,
          0.9501719245596622,
          0.9834913487559659,
          0.9935496939504337,
          0.9993806561820714,
          0.9993806561820714,
          1,
          0.9985813532825362,
          0.6414669386349386,
          0.17037130717207732,
          0.0534538201374985,
          0.30467570336644434,
          0.3049688538826086,
          0.3049715442929042,
          0.30564406634390723,
          0.30539193037483314,
          0.3044607486511213,
          0.30330349938304063,
          0.30567837854844837,
          0.30564406634390723,
          0.30499234760631483,
          0.3009560216675606,
          0.26520295557675494,
          0.11659001010395925,
          0.03787338805320675,
          0.1567238220177064,
          0.1887336274264408,
          0.2222920718416282,
          0.22430490609343035,
          0.22550157027833656,
          0.22564216316282182,
          0.2245251123867979,
          0.2216115827657717,
          0.22054226308930197,
          0.21799191517496308,
          0.21760910010441176,
          0.21922167898512251,
          0.22002787842935867,
          0.22054226308930197,
          0.22099122530734466,
          0.22481688927798726,
          0.2233065886182427,
          0.22254308996417055,
          0.2222920718416282,
          0.13116008052568506,
          0.07057810830668176,
          0.01088406432325767,
          0.119497585771216,
          0.16448798140907556,
          0.17046766459573726,
          0.17341545431828997,
          0.17328292319141503,
          0.17208842838664196,
          0.17062496833853646,
          0.17043213981190786,
          0.1702556526858408,
          0.16846790346438378,
          0.16830771037250053,
          0.1654389656622891,
          0.1646222745652285,
          0.16380638192997538,
          0.16348582252059574,
          0.16301879002917774,
          0.16152802326390148,
          0.1608305486047365,
          0.1624670569798342,
          0.16380638192997538,
          0.16687877635430198,
          0.16846790346438378,
          0.1697420116780382,
          0.1702556526858408,
          0.17100354411821705,
          0.17109768479536597,
          0.1718650674924705,
          0.17196157175190374,
          0.173514099538028,
          0.17362809146438146,
          0.17349551297112858,
          0.17187039621004563,
          0.14620306965867746,
          0.1072908153300246,
          0.38539577841941797,
          0.3543739622470541,
          0.4118282799209919,
          0.5929972077756728,
          0.5921609963173965,
          0.5657059652009169,
          0.5322153181778437,
          0.05182994911216826,
          0.08665310235687296,
          0.08672569417381468,
          0.08631172528883618,
          0.08614734500909838,
          0.08591214252031718,
          0.08625734300242169,
          0.08712753773957632,
          0.08665310235687296,
          0.08666492689958669,
          0.08656818012429565,
          0.08665633842784787,
          0.02651206091278377,
          0.04220270427443465,
          0.027776687798466038,
          0.0408354519722329,
          0.04322228688522314,
          0.0443254901005099,
          0.04533761724333196,
          0.044718981552761244,
          0.04232890535849454,
          0.04058374040591062,
          0.0408537031851737,
          0.042713664345216784,
          0.04291357130343981,
          0.04367393646857967,
          0.04522878588390991,
          0.043192218708189206,
          0.0408354519722329,
          0.027776687798466038,
          0.016127967660510556,
          0.01962664731087002,
          0.019839781803935752,
          0.019818196945280447,
          0.019581838716865585,
          0.019487932841620842,
          0.018987263725550763,
          0.01750979743934067,
          0.017368976248911233,
          0.016381159899823222,
          0.015280760774140117,
          0.013400248763482614,
          0.013358533193990774,
          0.013034075869996858,
          0.01336312820636315,
          0.014349019112167877,
          0.01532251376307106,
          0.01722253835332624,
          0.017368976248911233,
          0.01750979743934067,
          0.01765085119868784,
          0.018306478136165122,
          0.018409246125483725,
          0.01924072595322035,
          0.01920872901725665,
          0.019487932841620842,
          0.019581838716865585,
          0.019687951246165403,
          0.019548606465752266,
          0.01945351372439655,
          0.016127967660510556,
          0.01544296456878863,
          0.01445835966015857,
          0,
          0.16904341443570425,
          0.456381734927656,
          0.4763619722535798,
          0.47857518049609815,
          0.4763619722535798,
          0.3623949795617628,
          0.16904341443570425,
          0.09816761222983769,
          0.09786668652265237,
          0.09807701408413208,
          0.09800637755129227,
          0.09748795727590537,
          0.09832777500723916,
          0.09824522961953219,
          0.09807701408413208,
          0.09786668652265237,
          0.09815994645515139,
          0.05244581481692781,
          0.05326590215098878,
          0.05443149155565219,
          0.0546531775746818,
          0.05506605798028405,
          0.05417746945738963,
          0.05188359295674261,
          0.05156910246609961,
          0.05047680014914732,
          0.05210413155908783,
          0.05297909771454146,
          0.05352997343243529,
          0.054762731271375265,
          0.0550606781070207,
          0.05507546967967997,
          0.05506605798028405,
          0.05497800483009185,
          0.05443149155565219,
          0.054221591238906075,
          0.053958508899775326,
          0.05335962837232677,
          0.05326590215098878,
          0.05244581481692781,
          0.050079330394770666,
          0.022166329737774992,
          0.003285284642212255,
          0.014241654689486044,
          0.014241654689486044,
          0.027767900387937694,
          0.028039233002865538,
          0.027868495613810773,
          0.027860921729898125,
          0.027685157008768557,
          0.027580035384105583,
          0.027137019067548923,
          0.026889613578689936,
          0.026813256608325953,
          0.026617829088512568,
          0.02636312538806354,
          0.025515002435934214,
          0.025345067027333495,
          0.025179090500642487,
          0.024969894521943536,
          0.022981819149814453,
          0.0222819212267287,
          0.022283702202557876,
          0.02202683243762313,
          0.02067567996745351,
          0.0207262672596272,
          0.02111572546475496,
          0.021821357087950938,
          0.02202683243762313,
          0.023303604536401068,
          0.023954361736511165,
          0.024529350256677296,
          0.025179090500642487,
          0.02585519487084631,
          0.025949485699415242,
          0.025966869160302608,
          0.02636115731503974,
          0.026617829088512568,
          0.026709602394067417,
          0.026889613578689936,
          0.027040120246182825,
          0.027137019067548923,
          0.027472388184144297,
          0.0279168477542521,
          0.027868495613810773,
          0.028064647433217182,
          0.02799040347699519,
          0.027359170698078666,
          0.024650737495696268,
          0.014241654689486044
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "variable"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "index"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "value"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"99197b5f-614d-48e5-9bc4-f54f207528c5\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"99197b5f-614d-48e5-9bc4-f54f207528c5\")) {                    Plotly.newPlot(                        \"99197b5f-614d-48e5-9bc4-f54f207528c5\",                        [{\"hovertemplate\": \"variable=Analytic Function<br>index=%{x}<br>value=%{y}<extra></extra>\", \"legendgroup\": \"Analytic Function\", \"line\": {\"color\": \"#636efa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"Analytic Function\", \"showlegend\": true, \"type\": \"scattergl\", \"x\": [2, 8, 10, 15, 23, 24, 28, 38, 42, 43, 44, 47, 51, 64, 80, 84, 87, 90, 92, 94, 99, 100, 103, 109, 117, 120, 128, 136, 146, 148, 159, 164, 170, 175, 177, 179, 184, 187, 189, 194, 200, 204, 208, 209, 218, 222, 225, 226, 227, 228, 230, 233, 238, 239, 242, 243, 249, 251, 252, 256, 257, 262, 263, 267, 270, 272, 273, 274, 276, 277, 285, 289, 290, 291, 293, 295, 302, 303, 306, 307, 309, 310, 313, 316, 328, 335, 346, 349, 350, 361, 367, 376, 380, 396, 400, 405, 408, 411, 412, 415, 423, 424, 430, 431, 432, 443, 446, 449, 451, 452, 453, 456, 465, 471, 482, 490, 498, 512, 513, 514, 516, 518, 520, 532, 550, 552, 557, 574, 579, 583, 589, 591, 607, 612, 616, 620, 626, 629, 642, 643, 644, 652, 657, 659, 665, 669, 670, 680, 683, 686, 688, 696, 700, 702, 708, 709, 710, 712, 714, 720, 722, 725, 727, 733, 740, 741, 744, 745, 746, 756, 765, 766, 767, 770, 771, 773, 777, 780, 782, 783, 785, 789, 791, 793, 794, 801, 805, 808, 811, 817, 827, 831, 832, 834, 842, 843, 849, 858, 862, 865, 867, 870, 871, 879, 883, 888, 889, 890, 893, 896, 897, 899, 904, 906, 911, 912, 913, 914, 917, 919, 922, 925, 926, 942, 944, 947, 957, 958, 959, 963, 969, 971, 972, 979, 983, 986, 995, 996, 998, 1001, 1005, 1007, 1009, 1016, 1019, 1023, 1027, 1029, 1033, 1036, 1041, 1042, 1046, 1049, 1052, 1054, 1061, 1065, 1067, 1074, 1078, 1082, 1084, 1087, 1088, 1104, 1106, 1107, 1108, 1114, 1116, 1119, 1123, 1126, 1127, 1138, 1140, 1149, 1160, 1161, 1162, 1171, 1173, 1179, 1180, 1182, 1183, 1185, 1189, 1196, 1199, 1204, 1209, 1215, 1220, 1224, 1228, 1229, 1234, 1235, 1243, 1254, 1255, 1258, 1262, 1264, 1273, 1274, 1278, 1290, 1295, 1298, 1302, 1304, 1316, 1318, 1327, 1328, 1331, 1334, 1339, 1344, 1345, 1346, 1347, 1353, 1354, 1358, 1360, 1362, 1363, 1367, 1376, 1385, 1394, 1396, 1404, 1405, 1407, 1415, 1423, 1424, 1426, 1432, 1434, 1438, 1439, 1441, 1446, 1452, 1468, 1470, 1479, 1482, 1491, 1492, 1493, 1502, 1508, 1511, 1520, 1522, 1524, 1528, 1536, 1537, 1544, 1545, 1546, 1551, 1552, 1557, 1560, 1561, 1562, 1563, 1565, 1567, 1572, 1576, 1583, 1588, 1594, 1597, 1606, 1609, 1611, 1616, 1617, 1629, 1630, 1631, 1633, 1635, 1650, 1652, 1656, 1657, 1660, 1668, 1677, 1678, 1681, 1692, 1695, 1697, 1701, 1704, 1705, 1706, 1707, 1710, 1711, 1712, 1713, 1714, 1716, 1717, 1720, 1721, 1722, 1723, 1733, 1737, 1740, 1741, 1744, 1745, 1750, 1752, 1753, 1756, 1760, 1769, 1771, 1772, 1774, 1780, 1784, 1785, 1786, 1792, 1795, 1797, 1799, 1800, 1807, 1808, 1810, 1814, 1820, 1821, 1822, 1827, 1830, 1831, 1834, 1835, 1836, 1839, 1844, 1846, 1851, 1852, 1857, 1861, 1864], \"xaxis\": \"x\", \"y\": [0.5437317641159531, 0.39481878972685597, 0.3617902751206064, 0.2992130087433489, 0.2589083791652349, 0.25890837916523496, 0.28920829603565507, 0.016668093722959042, 0.016168761932332975, 0.01616876193233298, 0.016168761932332975, 0.016168761932332975, 0.016168761932332975, 0.016168761932332975, 0.016168761932332975, 0.01616876193233298, 0.000997535855276603, 0.0009975358552766027, 0.0009975358552766027, 0.000997535855276603, 0.0009975358552766027, 0.0009975358552766027, 0.000997535855276603, 0.0009975358552766027, 0.0009975358552766027, 0.0009975358552766027, 0.0009975358552766027, 0.0009975358552766027, 0.0009975358552766027, 0.0009975358552766027, 0.0009975358552766027, 0.0009975358552766027, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 4.933422546057942e-05, 0.1845465394272044, 0.17471681546691437, 0.14509150736780382, 0.1506445270044349, 0.18114934114419456, 0.00905520744499353, 0.00905520744499353, 0.00905520744499353, 0.00905520744499353, 0.00905520744499353, 0.00905520744499353, 0.00905520744499353, 0.0005529386998178874, 0.0005529386998178874, 0.0005529386998178874, 0.0005529386998178874, 0.0005529386998178874, 0.0005529386998178874, 0.0005529386998178874, 0.0005529386998178874, 0.0005529386998178874, 0.0005529386998178874, 0.0005529386998178874, 0.0005529386998178874, 0.0005529386998178874, 0.0005529386998178874, 0.0005529386998178874, 0.0005529386998178874, 0.0005529386998178874, 0.0005529386998178874, 0.0005529386998178874, 0.0005529386998178874, 2.154690324440971e-05, 2.154690324440971e-05, 2.154690324440971e-05, 2.154690324440971e-05, 2.154690324440971e-05, 2.154690324440971e-05, 2.154690324440971e-05, 2.154690324440971e-05, 2.154690324440971e-05, 2.154690324440971e-05, 2.154690324440971e-05, 2.154690324440971e-05, 2.154690324440971e-05, 2.154690324440971e-05, 2.154690324440971e-05, 2.154690324440971e-05, 2.154690324440971e-05, 2.154690324440971e-05, 2.154690324440971e-05, 2.154690324440971e-05, 2.154690324440971e-05, 2.154690324440971e-05, 0.196135533119918, 0.16921960853705129, 0.10830669659697635, 0.10830669659697635, 0.10830669659697635, 0.11257234142619015, 0.13277627061725183, 0.14304522156268773, 0.006756156771816816, 0.006756156771816816, 0.006756156771816814, 0.006756156771816814, 0.006756156771816814, 0.006756156771816816, 0.006756156771816814, 0.006756156771816814, 0.006756156771816816, 0.006756156771816816, 0.006756156771816816, 0.006756156771816816, 0.006756156771816816, 0.006756156771816814, 0.006756156771816814, 0.0004092480327443426, 0.00040924803274434274, 0.0004092480327443426, 0.0004092480327443426, 0.00040924803274434274, 0.00040924803274434274, 0.0004092480327443426, 0.0004092480327443426, 0.0004092480327443426, 0.0004092480327443426, 0.0004092480327443426, 0.0004092480327443426, 0.0004092480327443426, 0.0004092480327443426, 0.0004092480327443426, 0.0004092480327443426, 0.0004092480327443426, 0.0004092480327443426, 0.00040924803274434274, 0.0004092480327443426, 0.0004092480327443426, 0.0004092480327443426, 0.0004092480327443426, 0.0004092480327443426, 0.0004092480327443426, 0.00040924803274434274, 1.256623655231317e-05, 1.256623655231316e-05, 1.256623655231317e-05, 1.256623655231316e-05, 1.256623655231316e-05, 1.256623655231316e-05, 1.256623655231316e-05, 1.256623655231316e-05, 1.256623655231316e-05, 1.256623655231316e-05, 1.256623655231316e-05, 1.256623655231316e-05, 1.256623655231316e-05, 1.256623655231316e-05, 1.256623655231316e-05, 1.256623655231316e-05, 1.256623655231316e-05, 1.256623655231316e-05, 1.256623655231316e-05, 1.256623655231316e-05, 1.256623655231316e-05, 1.256623655231316e-05, 1.256623655231316e-05, 1.256623655231316e-05, 1.256623655231316e-05, 1.256623655231316e-05, 1.256623655231316e-05, 1.256623655231316e-05, 1.256623655231316e-05, 1.256623655231316e-05, 1.256623655231316e-05, 1.256623655231316e-05, 1.256623655231316e-05, 1.256623655231316e-05, 1.256623655231316e-05, 1.256623655231316e-05, 1.256623655231316e-05, 1.256623655231316e-05, 1.256623655231316e-05, 1.256623655231317e-05, 1.256623655231316e-05, 0.06084358428253667, 0.059860499965750956, 0.0589086719947786, 0.05683539167870165, 0.05683539167870165, 0.05709300620834826, 0.057986633041689384, 0.06512144157033813, 0.07004617727247374, 0.003539200214424645, 0.003539200214424645, 0.0035392002144246444, 0.0035392002144246444, 0.003539200214424645, 0.003539200214424645, 0.003539200214424645, 0.003539200214424645, 0.003539200214424645, 0.003539200214424645, 0.003539200214424645, 0.003539200214424645, 0.003539200214424645, 0.003539200214424645, 0.00020818824790733205, 0.00020818824790733205, 0.00020818824790733203, 0.00020818824790733203, 0.00020818824790733205, 0.00020818824790733205, 0.00020818824790733205, 0.00020818824790733205, 0.00020818824790733205, 0.00020818824790733205, 0.00020818824790733205, 0.00020818824790733205, 0.00020818824790733205, 0.00020818824790733205, 0.00020818824790733205, 0.00020818824790733205, 0.00020818824790733203, 0.00020818824790733205, 0.00020818824790733205, 0.00020818824790733203, 0.00020818824790733203, 0.00020818824790733205, 1.6940658945086007e-21, 1.6940658945086007e-21, 1.6940658945086007e-21, 1.6940658945086007e-21, 0.0, 1.6940658945086007e-21, 1.6940658945086007e-21, 1.6940658945086007e-21, 1.6940658945086007e-21, 0.0, 1.6940658945086007e-21, 1.6940658945086007e-21, 1.6940658945086007e-21, 1.6940658945086007e-21, 1.6940658945086007e-21, 1.6940658945086007e-21, 1.6940658945086007e-21, 1.6940658945086007e-21, 1.6940658945086007e-21, 1.6940658945086007e-21, 1.6940658945086007e-21, 1.6940658945086007e-21, 1.6940658945086007e-21, 1.6940658945086007e-21, 0.0, 1.6940658945086007e-21, 1.6940658945086007e-21, 1.6940658945086007e-21, 1.6940658945086007e-21, 0.0, 1.6940658945086007e-21, 1.6940658945086007e-21, 1.6940658945086007e-21, 1.6940658945086007e-21, 1.6940658945086007e-21, 1.0, 0.9436063863436029, 0.7699292870274786, 0.5251370928548138, 0.6259218144268832, 0.7050448424593487, 0.8479663723606675, 0.03515984009150197, 0.03280805653793165, 0.03280805653793165, 0.03280805653793165, 0.03280805653793165, 0.03280805653793165, 0.03280805653793165, 0.03280805653793165, 0.03280805653793165, 0.03280805653793165, 0.032808056537931646, 0.032808056537931646, 0.00203749176812652, 0.00203749176812652, 0.0020374917681265196, 0.00203749176812652, 0.00203749176812652, 0.00203749176812652, 0.00203749176812652, 0.00203749176812652, 0.00203749176812652, 0.00203749176812652, 0.00203749176812652, 0.00203749176812652, 0.00203749176812652, 0.00203749176812652, 0.00203749176812652, 0.00203749176812652, 0.00203749176812652, 0.0020374917681265196, 0.00011433147001369927, 0.00011433147001369927, 0.00011433147001369927, 0.00011433147001369927, 0.00011433147001369927, 0.00011433147001369927, 0.00011433147001369927, 0.00011433147001369927, 0.00011433147001369927, 0.00011433147001369927, 0.00011433147001369927, 0.00011433147001369927, 0.00011433147001369927, 0.00011433147001369927, 0.00011433147001369927, 0.00011433147001369927, 0.00011433147001369927, 0.00011433147001369927, 0.00011433147001369927, 0.00011433147001369927, 0.00011433147001369927, 0.00011433147001369927, 0.00011433147001369927, 0.00011433147001369927, 0.00011433147001369927, 0.00011433147001369927, 0.00011433147001369927, 0.00011433147001369927, 0.00011433147001369927, 0.00011433147001369927, 0.00011433147001369927, 0.00011433147001369927, 0.00011433147001369927, 0.00011433147001369927, 0.38836804456256663, 0.3547854451607028, 0.33183032728626477, 0.306402843488476, 0.33183032728626477, 0.3674966002930161, 0.38836804456256663, 0.019137165952535548, 0.019137165952535548, 0.019137165952535548, 0.019137165952535545, 0.019137165952535548, 0.019137165952535545, 0.019137165952535548, 0.019137165952535548, 0.019137165952535548, 0.019137165952535548, 0.0011830611065392635, 0.0011830611065392633, 0.0011830611065392635, 0.0011830611065392633, 0.0011830611065392633, 0.0011830611065392635, 0.0011830611065392633, 0.0011830611065392633, 0.0011830611065392635, 0.0011830611065392633, 0.0011830611065392633, 0.0011830611065392633, 0.0011830611065392633, 0.0011830611065392633, 0.0011830611065392633, 0.0011830611065392633, 0.0011830611065392633, 0.0011830611065392635, 0.0011830611065392635, 0.0011830611065392635, 0.0011830611065392635, 0.0011830611065392633, 0.0011830611065392635, 0.0011830611065392633, 0.0011830611065392635, 0.0011830611065392635, 6.0929553664495706e-05, 6.092955366449572e-05, 6.092955366449572e-05, 6.092955366449572e-05, 6.092955366449572e-05, 6.092955366449572e-05, 6.0929553664495706e-05, 6.092955366449572e-05, 6.0929553664495706e-05, 6.092955366449572e-05, 6.092955366449572e-05, 6.092955366449572e-05, 6.092955366449572e-05, 6.092955366449572e-05, 6.0929553664495706e-05, 6.092955366449572e-05, 6.092955366449572e-05, 6.092955366449572e-05, 6.092955366449572e-05, 6.092955366449572e-05, 6.092955366449572e-05, 6.092955366449572e-05, 6.092955366449572e-05, 6.092955366449572e-05, 6.092955366449572e-05, 6.092955366449572e-05, 6.092955366449572e-05, 6.092955366449572e-05, 6.092955366449572e-05, 6.092955366449572e-05, 6.0929553664495706e-05, 6.092955366449572e-05, 6.092955366449572e-05, 6.092955366449572e-05, 6.092955366449572e-05, 6.092955366449572e-05, 6.092955366449572e-05, 6.092955366449572e-05, 6.0929553664495706e-05, 6.092955366449572e-05, 6.0929553664495706e-05, 6.092955366449572e-05, 6.092955366449572e-05, 6.0929553664495706e-05, 6.0929553664495706e-05, 6.092955366449572e-05, 6.0929553664495706e-05], \"yaxis\": \"y\"}, {\"hovertemplate\": \"variable=Signal<br>index=%{x}<br>value=%{y}<extra></extra>\", \"legendgroup\": \"Signal\", \"line\": {\"color\": \"#EF553B\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"Signal\", \"showlegend\": true, \"type\": \"scattergl\", \"x\": [2, 8, 10, 15, 23, 24, 28, 38, 42, 43, 44, 47, 51, 64, 80, 84, 87, 90, 92, 94, 99, 100, 103, 109, 117, 120, 128, 136, 146, 148, 159, 164, 170, 175, 177, 179, 184, 187, 189, 194, 200, 204, 208, 209, 218, 222, 225, 226, 227, 228, 230, 233, 238, 239, 242, 243, 249, 251, 252, 256, 257, 262, 263, 267, 270, 272, 273, 274, 276, 277, 285, 289, 290, 291, 293, 295, 302, 303, 306, 307, 309, 310, 313, 316, 328, 335, 346, 349, 350, 361, 367, 376, 380, 396, 400, 405, 408, 411, 412, 415, 423, 424, 430, 431, 432, 443, 446, 449, 451, 452, 453, 456, 465, 471, 482, 490, 498, 512, 513, 514, 516, 518, 520, 532, 550, 552, 557, 574, 579, 583, 589, 591, 607, 612, 616, 620, 626, 629, 642, 643, 644, 652, 657, 659, 665, 669, 670, 680, 683, 686, 688, 696, 700, 702, 708, 709, 710, 712, 714, 720, 722, 725, 727, 733, 740, 741, 744, 745, 746, 756, 765, 766, 767, 770, 771, 773, 777, 780, 782, 783, 785, 789, 791, 793, 794, 801, 805, 808, 811, 817, 827, 831, 832, 834, 842, 843, 849, 858, 862, 865, 867, 870, 871, 879, 883, 888, 889, 890, 893, 896, 897, 899, 904, 906, 911, 912, 913, 914, 917, 919, 922, 925, 926, 942, 944, 947, 957, 958, 959, 963, 969, 971, 972, 979, 983, 986, 995, 996, 998, 1001, 1005, 1007, 1009, 1016, 1019, 1023, 1027, 1029, 1033, 1036, 1041, 1042, 1046, 1049, 1052, 1054, 1061, 1065, 1067, 1074, 1078, 1082, 1084, 1087, 1088, 1104, 1106, 1107, 1108, 1114, 1116, 1119, 1123, 1126, 1127, 1138, 1140, 1149, 1160, 1161, 1162, 1171, 1173, 1179, 1180, 1182, 1183, 1185, 1189, 1196, 1199, 1204, 1209, 1215, 1220, 1224, 1228, 1229, 1234, 1235, 1243, 1254, 1255, 1258, 1262, 1264, 1273, 1274, 1278, 1290, 1295, 1298, 1302, 1304, 1316, 1318, 1327, 1328, 1331, 1334, 1339, 1344, 1345, 1346, 1347, 1353, 1354, 1358, 1360, 1362, 1363, 1367, 1376, 1385, 1394, 1396, 1404, 1405, 1407, 1415, 1423, 1424, 1426, 1432, 1434, 1438, 1439, 1441, 1446, 1452, 1468, 1470, 1479, 1482, 1491, 1492, 1493, 1502, 1508, 1511, 1520, 1522, 1524, 1528, 1536, 1537, 1544, 1545, 1546, 1551, 1552, 1557, 1560, 1561, 1562, 1563, 1565, 1567, 1572, 1576, 1583, 1588, 1594, 1597, 1606, 1609, 1611, 1616, 1617, 1629, 1630, 1631, 1633, 1635, 1650, 1652, 1656, 1657, 1660, 1668, 1677, 1678, 1681, 1692, 1695, 1697, 1701, 1704, 1705, 1706, 1707, 1710, 1711, 1712, 1713, 1714, 1716, 1717, 1720, 1721, 1722, 1723, 1733, 1737, 1740, 1741, 1744, 1745, 1750, 1752, 1753, 1756, 1760, 1769, 1771, 1772, 1774, 1780, 1784, 1785, 1786, 1792, 1795, 1797, 1799, 1800, 1807, 1808, 1810, 1814, 1820, 1821, 1822, 1827, 1830, 1831, 1834, 1835, 1836, 1839, 1844, 1846, 1851, 1852, 1857, 1861, 1864], \"xaxis\": \"x\", \"y\": [0.26878966550807026, 0.6905562669743748, 0.7176308346115418, 0.7191224687952942, 0.7210362635970898, 0.7181655713943964, 0.707330115531289, 0.0, 0.1357358963173522, 0.1575081265918973, 0.16595133895276024, 0.1690471834850766, 0.1695256321855255, 0.16792142183696154, 0.16814657416658454, 0.10203622138102808, 0.009808198359202408, 0.09036770189831556, 0.11032745591939544, 0.1127084418051588, 0.11396085163868677, 0.113659710397816, 0.11473199836764561, 0.11432390977020387, 0.11138004306038304, 0.11040344483064321, 0.1082447968703826, 0.11075243094155893, 0.1141437879065055, 0.11444211474325597, 0.11292233651830061, 0.0634394832754035, 0.07168850175196653, 0.07995440665325132, 0.08030902157240755, 0.08014578613343087, 0.08039345369601622, 0.08039345369601622, 0.08036530965481331, 0.07978554240603405, 0.07886804666282032, 0.07878361453921168, 0.07831360905112364, 0.07808845672150064, 0.0768416756962132, 0.07659400813362788, 0.07602831290545006, 0.07528249581357385, 0.07481249032548581, 0.07450290587225417, 0.07329834090877108, 0.07293528277725397, 0.07184329397858236, 0.07134233004517115, 0.07090609740652656, 0.07108059046198442, 0.07295498360609598, 0.07296905562669742, 0.07318013593571901, 0.07498416897682339, 0.07512207477871746, 0.07659400813362788, 0.07673191393552198, 0.0773426396296244, 0.07770288335702119, 0.0778436035630356, 0.07787456200835877, 0.07811941516682377, 0.07826013537283819, 0.07867385277852046, 0.07931553691794602, 0.08003320996861937, 0.07997692188621364, 0.08014578613343087, 0.08033998001773074, 0.08044974177842196, 0.08045255618254224, 0.08058764758031603, 0.07997692188621364, 0.07912134303364618, 0.07182640755386066, 0.058750686011004305, 0.04329116417826434, 0.3138482754738753, 0.680255547894122, 0.6823944950255406, 0.13227980805763898, 0.04190366294696252, 0.07775917143942698, 0.18390723724019528, 0.18401981340500675, 0.18396352532260105, 0.18475155847628158, 0.00633803807888772, 0.09822270379803832, 0.1257560193068122, 0.12663692779646227, 0.12739400250481966, 0.12768670053332956, 0.12768670053332956, 0.12522691133219818, 0.12483008035123758, 0.12348479518174013, 0.12268269000745816, 0.12273053487750299, 0.12338347663340979, 0.12372401953196457, 0.12523254014043875, 0.12634422976795237, 0.12657782530993625, 0.12698309950325762, 0.12750939307375145, 0.12593332676639038, 0.1117768740413436, 0.08979637786189718, 0.09148783473819005, 0.09137244416925827, 0.08963595682704076, 0.08971476014240884, 0.08942206211389894, 0.08905337517414125, 0.08881977963215734, 0.08878037797447333, 0.08702700420753415, 0.0818710158591672, 0.08230161968957123, 0.08374540900327879, 0.08837228937703165, 0.08874097631678934, 0.08913499289362958, 0.08969787371768712, 0.09002997340388105, 0.09122328075088298, 0.09045494842604448, 0.08829067165754328, 0.03766798474592964, 0.4101008963877123, 0.6579091791790382, 0.8839339740793378, 0.8847220072330185, 0.883033364760846, 0.854326442733912, 0.5307825450656458, 0.3474522606701096, 0.11846108382702672, 0.27045016393904, 0.2832838467275516, 0.28992584045143044, 0.2896444000394016, 0.2888000788033154, 0.2884623503088808, 0.29017913682225627, 0.29082644976992245, 0.2892222394213585, 0.24495166260923404, 0.20608474170806182, 0.16583876278794865, 0.08573800712044241, 0.00029551243263017946, 0.1772370994751136, 0.20740751164459706, 0.21095366083615943, 0.2119949903606659, 0.21328961625599815, 0.2127830235143464, 0.21255787118472338, 0.21067222042413067, 0.2103344919296962, 0.20957460281721849, 0.20597216554325035, 0.20791410438624877, 0.2080266805510603, 0.20808296863346606, 0.20903986603436384, 0.20974346706443578, 0.2110099489185652, 0.21289559967915786, 0.21354291262682404, 0.2138524970800557, 0.2140495053684758, 0.21343033646201257, 0.2112351012481882, 0.20025892517906643, 0.15044397224997533, 0.11747322798080576, 0.10094704698647677, 0.16254590996721222, 0.16400940010976175, 0.1641501203157761, 0.16448784881021064, 0.16350280736810993, 0.1628836384616467, 0.16282735037924093, 0.1624614778436035, 0.16127942811308274, 0.1611668519482712, 0.15998480221775044, 0.15764884679791172, 0.15511588308965277, 0.1545248582243924, 0.15339909657627734, 0.1528643597934227, 0.15244219917537957, 0.15249848725778534, 0.15387754527672623, 0.1560446364493477, 0.15694524576783983, 0.1572829742622743, 0.15894347269324396, 0.15970336180572167, 0.1596470737233159, 0.16018181050617056, 0.16136386023669133, 0.16150458044270574, 0.16220818147277768, 0.1624614778436035, 0.16260219804961792, 0.16251776592600928, 0.1630806467500668, 0.16336208716209558, 0.16384053586254452, 0.16434712860419629, 0.16448784881021064, 0.1399152864359793, 0.08602226193659146, 0.00044186144688512785, 0.9730942966100501, 0.9922885327104117, 0.9965382829320463, 0.9957502497783656, 0.9980862051982043, 0.9999999999999999, 0.9992401108875223, 0.5788244233989558, 0.13602015113350127, 0.07726946512249694, 0.3097111014170525, 0.3096829573758496, 0.3095703812110381, 0.3104428464883272, 0.3099925418290812, 0.30948594908742943, 0.30858533976893743, 0.31100572731238474, 0.31038655840592144, 0.3100206858702841, 0.3054050631130123, 0.2667070064590574, 0.10306066448081277, 0.011744508393960296, 0.18247189113884862, 0.20819554479827754, 0.22806523788750824, 0.22935986378284054, 0.2308796420077959, 0.23107665029621596, 0.22997903268930378, 0.22699576432179894, 0.226067010962104, 0.22356219129504804, 0.22297116642978762, 0.22502568143759763, 0.225841858632481, 0.22623587520932129, 0.22657360370375576, 0.2302604731013326, 0.22899399124720318, 0.22823410213472548, 0.22792451768149388, 0.1293866006219833, 0.06266270773820412, 0.030544727917481668, 0.14650380648157266, 0.1767586507746647, 0.17794070050518554, 0.17979820722457537, 0.1797419191421696, 0.1783065730408229, 0.17656164248624454, 0.1763927782390273, 0.1763083461154187, 0.17490114405527488, 0.17442269535482594, 0.17152385911092968, 0.17076396999845203, 0.16946934410311978, 0.1695256321855255, 0.16918790369109096, 0.16730225293049825, 0.16724596484809254, 0.1686531669082363, 0.1694974881443226, 0.17293106117107357, 0.17473227980805764, 0.1757173212501583, 0.17636463419782447, 0.17729338755751942, 0.17729338755751942, 0.17782812434037396, 0.1778562683815769, 0.1790664621533006, 0.17878502174127178, 0.17858801345285166, 0.1772370994751136, 0.14689782305841295, 0.09308923068263372, 0.4791382294583678, 0.5330059243206732, 0.594922814967001, 0.5957389921618844, 0.5908137849513811, 0.5067475338783896, 0.20225715210447068, 0.01735924461393412, 0.09361270984900724, 0.09335097026582043, 0.09264736923574854, 0.09273461576347747, 0.09289503679833389, 0.09403487046705036, 0.09412211699477924, 0.09357893699956374, 0.09196628343863897, 0.08855804004897064, 0.07874421288152764, 0.045759396591756596, 0.04963483106539268, 0.05123341260571604, 0.05167527405260122, 0.051897611978103914, 0.05211713549948635, 0.05284325176252058, 0.05134317436640726, 0.048855241124072996, 0.048469667759593604, 0.04885805552819328, 0.05085065364535693, 0.05123341260571604, 0.05217905239013268, 0.05256744015873238, 0.05002040442987207, 0.047251030775509045, 0.02487933242334267, 0.02653138764195148, 0.027158999760775654, 0.027158999760775654, 0.02711678369897133, 0.02719840141845964, 0.02678186960865711, 0.026227431996960426, 0.025101670348845362, 0.02483993076565863, 0.02350871761676257, 0.02220283410494911, 0.02057610852342287, 0.020120175055936246, 0.020258080857830374, 0.02131066799881795, 0.02234073990684321, 0.023331410157184457, 0.02496376454695129, 0.025020052629357026, 0.025059454287041066, 0.02557449024105371, 0.026128927852750367, 0.026089526195066326, 0.026880373752867198, 0.026961991472355507, 0.027060495616565594, 0.02719840141845964, 0.027308163179150885, 0.027240617480263962, 0.026961991472355507, 0.02653138764195148, 0.02543658443915961, 0.023058412957516566, 0.0019728972883216456, 0.117231189226461, 0.45634155608403804, 0.4797573983648311, 0.4774777310273981, 0.479785542406034, 0.36751896204776036, 0.1370277078085642, 0.10487032633015775, 0.10442846488327256, 0.10472397731590277, 0.10438343441734796, 0.10444816571211463, 0.10489002715899973, 0.10484499669307512, 0.1047436781447448, 0.10444816571211463, 0.10480840943951142, 0.06000872465277288, 0.06054909024386809, 0.061798685673275816, 0.06195910670813223, 0.06215048618831179, 0.06140748350055583, 0.059108115334280814, 0.05875912922336518, 0.05767558363705444, 0.05919254745788943, 0.0598511180220368, 0.06094029241658808, 0.06179024246091494, 0.06203228121525972, 0.06203228121525972, 0.06195066349577133, 0.06203228121525972, 0.06164107904253971, 0.0612076608080154, 0.060974065266031524, 0.06047591573674063, 0.06055753345622894, 0.05976668589842818, 0.05872535637392173, 0.02542814122679876, 0.0029917115798657468, 0.007548231850611403, 0.02183133276107116, 0.035478378340345895, 0.035514965593909625, 0.035388317408496656, 0.03538268860025609, 0.03515753627063309, 0.03508154735938532, 0.03460028425481612, 0.03438076073343371, 0.03434417347986998, 0.03411902115024695, 0.033663087682760384, 0.033001702714492764, 0.0328075088301929, 0.03271744789834369, 0.03234876095858602, 0.030404007711467285, 0.029818611654447436, 0.0296244177701476, 0.02948932637237378, 0.028180628456440066, 0.028315719854213883, 0.0288054261711439, 0.029323276529276826, 0.029458367927050644, 0.03092185806960021, 0.03138060594120709, 0.03192941474466318, 0.03255139805524676, 0.033288771934762124, 0.033325359188325826, 0.03341542012017504, 0.03387416799178192, 0.03400925938955571, 0.03400363058131517, 0.034324472651027976, 0.034332915863388797, 0.0344286056034786, 0.034693159590785616, 0.03517442269535481, 0.03514346425003165, 0.03524196839424171, 0.03517442269535481, 0.034369503116952554, 0.03208702137539929, 0.001398758847782955], \"yaxis\": \"y\"}, {\"hovertemplate\": \"variable=GB<br>index=%{x}<br>value=%{y}<extra></extra>\", \"legendgroup\": \"GB\", \"line\": {\"color\": \"#00cc96\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"GB\", \"showlegend\": true, \"type\": \"scattergl\", \"x\": [2, 8, 10, 15, 23, 24, 28, 38, 42, 43, 44, 47, 51, 64, 80, 84, 87, 90, 92, 94, 99, 100, 103, 109, 117, 120, 128, 136, 146, 148, 159, 164, 170, 175, 177, 179, 184, 187, 189, 194, 200, 204, 208, 209, 218, 222, 225, 226, 227, 228, 230, 233, 238, 239, 242, 243, 249, 251, 252, 256, 257, 262, 263, 267, 270, 272, 273, 274, 276, 277, 285, 289, 290, 291, 293, 295, 302, 303, 306, 307, 309, 310, 313, 316, 328, 335, 346, 349, 350, 361, 367, 376, 380, 396, 400, 405, 408, 411, 412, 415, 423, 424, 430, 431, 432, 443, 446, 449, 451, 452, 453, 456, 465, 471, 482, 490, 498, 512, 513, 514, 516, 518, 520, 532, 550, 552, 557, 574, 579, 583, 589, 591, 607, 612, 616, 620, 626, 629, 642, 643, 644, 652, 657, 659, 665, 669, 670, 680, 683, 686, 688, 696, 700, 702, 708, 709, 710, 712, 714, 720, 722, 725, 727, 733, 740, 741, 744, 745, 746, 756, 765, 766, 767, 770, 771, 773, 777, 780, 782, 783, 785, 789, 791, 793, 794, 801, 805, 808, 811, 817, 827, 831, 832, 834, 842, 843, 849, 858, 862, 865, 867, 870, 871, 879, 883, 888, 889, 890, 893, 896, 897, 899, 904, 906, 911, 912, 913, 914, 917, 919, 922, 925, 926, 942, 944, 947, 957, 958, 959, 963, 969, 971, 972, 979, 983, 986, 995, 996, 998, 1001, 1005, 1007, 1009, 1016, 1019, 1023, 1027, 1029, 1033, 1036, 1041, 1042, 1046, 1049, 1052, 1054, 1061, 1065, 1067, 1074, 1078, 1082, 1084, 1087, 1088, 1104, 1106, 1107, 1108, 1114, 1116, 1119, 1123, 1126, 1127, 1138, 1140, 1149, 1160, 1161, 1162, 1171, 1173, 1179, 1180, 1182, 1183, 1185, 1189, 1196, 1199, 1204, 1209, 1215, 1220, 1224, 1228, 1229, 1234, 1235, 1243, 1254, 1255, 1258, 1262, 1264, 1273, 1274, 1278, 1290, 1295, 1298, 1302, 1304, 1316, 1318, 1327, 1328, 1331, 1334, 1339, 1344, 1345, 1346, 1347, 1353, 1354, 1358, 1360, 1362, 1363, 1367, 1376, 1385, 1394, 1396, 1404, 1405, 1407, 1415, 1423, 1424, 1426, 1432, 1434, 1438, 1439, 1441, 1446, 1452, 1468, 1470, 1479, 1482, 1491, 1492, 1493, 1502, 1508, 1511, 1520, 1522, 1524, 1528, 1536, 1537, 1544, 1545, 1546, 1551, 1552, 1557, 1560, 1561, 1562, 1563, 1565, 1567, 1572, 1576, 1583, 1588, 1594, 1597, 1606, 1609, 1611, 1616, 1617, 1629, 1630, 1631, 1633, 1635, 1650, 1652, 1656, 1657, 1660, 1668, 1677, 1678, 1681, 1692, 1695, 1697, 1701, 1704, 1705, 1706, 1707, 1710, 1711, 1712, 1713, 1714, 1716, 1717, 1720, 1721, 1722, 1723, 1733, 1737, 1740, 1741, 1744, 1745, 1750, 1752, 1753, 1756, 1760, 1769, 1771, 1772, 1774, 1780, 1784, 1785, 1786, 1792, 1795, 1797, 1799, 1800, 1807, 1808, 1810, 1814, 1820, 1821, 1822, 1827, 1830, 1831, 1834, 1835, 1836, 1839, 1844, 1846, 1851, 1852, 1857, 1861, 1864], \"xaxis\": \"x\", \"y\": [0.2585097465903057, 0.33053474732593535, 0.4480726660347786, 0.7205746010478079, 0.7296437002929632, 0.7303322343250236, 0.7290718353298782, 0.09885304606556858, 0.10128346934173585, 0.10128346934173585, 0.10128346934173585, 0.10128346934173585, 0.14146351525841586, 0.16448069796976186, 0.14146351525841586, 0.13851980747689938, 0.0746594722093388, 0.07272057822899328, 0.07272057822899328, 0.074386323679138, 0.07812645637977675, 0.07812645637977675, 0.07812645637977675, 0.08809674037990042, 0.09502927443600442, 0.09414752413429975, 0.11463770155063294, 0.09348606214891095, 0.09459613641347994, 0.08776225107792754, 0.07812645637977675, 0.07812645637977675, 0.04442017040797108, 0.05022143258021258, 0.05669952999475436, 0.05669952999475436, 0.05669952999475436, 0.05669952999475436, 0.05831532731918715, 0.05831532731918715, 0.05831532731918715, 0.05831532731918715, 0.05831532731918715, 0.056376433338841625, 0.0604463703875337, 0.0604463703875337, 0.0604463703875337, 0.0604463703875337, 0.0604463703875337, 0.0604463703875337, 0.06020979979260113, 0.059598152970495866, 0.05893669098510709, 0.05893669098510709, 0.05833676305131921, 0.05833676305131921, 0.05893669098510709, 0.05893669098510709, 0.059598152970495866, 0.06020979979260113, 0.059877910034736526, 0.0604463703875337, 0.0604463703875337, 0.0604463703875337, 0.0604463703875337, 0.05804217878898632, 0.05804217878898632, 0.05804217878898632, 0.05804217878898632, 0.056376433338841625, 0.05831532731918715, 0.05831532731918715, 0.05831532731918715, 0.05831532731918715, 0.05831532731918715, 0.05831532731918715, 0.05669952999475436, 0.05669952999475436, 0.05669952999475436, 0.05669952999475436, 0.05669952999475436, 0.05669952999475436, 0.027480245089547745, 0.3068124631294773, 0.7081782605225133, 0.6995354143815429, 0.13868874237239712, 0.0576375742302114, 0.08410094191810938, 0.16329255714077898, 0.16630167327478332, 0.16630167327478332, 0.16630167327478332, 0.0576375742302114, 0.0829413958134618, 0.09280868246334581, 0.09280868246334581, 0.10511939575529997, 0.10511939575529997, 0.10511939575529997, 0.10555253377782445, 0.10528243029822501, 0.10400932149073097, 0.10400932149073097, 0.10400932149073097, 0.10400932149073097, 0.10467078347611977, 0.10555253377782445, 0.10511939575529997, 0.10511939575529997, 0.10511939575529997, 0.10511939575529997, 0.09280868246334581, 0.09071118912314644, 0.06740154752017108, 0.06883360572662797, 0.07027064486746407, 0.07027064486746407, 0.07027064486746407, 0.07027064486746407, 0.06999749633726324, 0.06999749633726324, 0.06999749633726324, 0.07240168793581062, 0.06886002239313924, 0.06945995032692712, 0.06945995032692712, 0.07240168793581062, 0.06999749633726324, 0.06833175088711854, 0.07027064486746407, 0.07027064486746407, 0.06883360572662797, 0.06883360572662797, 0.06740154752017108, 0.044851493734591186, 0.364093238416479, 0.5097586778488672, 0.9026589993998115, 0.9039193983949569, 0.9039193983949569, 0.8888589155536502, 0.8101818355520768, 0.7181227395657762, 0.19988924793350196, 0.22281267245317168, 0.22281267245317168, 0.2696737030302099, 0.2726828191642142, 0.2726828191642142, 0.28407714388999705, 0.28407714388999705, 0.2726828191642142, 0.2726828191642142, 0.26798202634131796, 0.24848753259358328, 0.2453628096767815, 0.2287155167949569, 0.22281267245317168, 0.14575857113660665, 0.14575857113660665, 0.14575857113660665, 0.18701767411419892, 0.18701767411419892, 0.18701767411419892, 0.18701767411419892, 0.18596424392990044, 0.1941594797003581, 0.19609937779249906, 0.19482626898500502, 0.19482626898500502, 0.19482626898500502, 0.19482626898500502, 0.19548773097039382, 0.19548773097039382, 0.19609937779249906, 0.18701767411419892, 0.18701767411419892, 0.18701767411419892, 0.18701767411419892, 0.18701767411419892, 0.18701767411419892, 0.18701767411419892, 0.18701767411419892, 0.18701767411419892, 0.09448419285978374, 0.10317156006066144, 0.1369794396287575, 0.14350465141668547, 0.14350465141668547, 0.14350465141668547, 0.14350465141668547, 0.14350465141668547, 0.14350465141668547, 0.14323150288648465, 0.14323150288648465, 0.14563569448503197, 0.14563569448503197, 0.15431779302547446, 0.15370614620336917, 0.15304468421798037, 0.15304468421798037, 0.15304468421798037, 0.15304468421798037, 0.15304468421798037, 0.15370614620336917, 0.15370614620336917, 0.15431779302547446, 0.1450672341322348, 0.14563569448503197, 0.14563569448503197, 0.14563569448503197, 0.14563569448503197, 0.14563569448503197, 0.14323150288648465, 0.14323150288648465, 0.14323150288648465, 0.14323150288648465, 0.14350465141668547, 0.14350465141668547, 0.14350465141668547, 0.14350465141668547, 0.14350465141668547, 0.14350465141668547, 0.1369794396287575, 0.10317156006066144, 0.986149308595025, 0.9936283193194417, 0.9936283193194417, 1.0, 1.0, 0.9964587727234804, 0.9964587727234804, 0.6832486176640491, 0.17303805807489045, 0.12527446104728643, 0.28287503118622803, 0.28287503118622803, 0.28287503118622803, 0.2836193677133174, 0.2902341035570244, 0.2902341035570244, 0.2911087176601682, 0.2902341035570244, 0.2836193677133174, 0.28287503118622803, 0.28287503118622803, 0.26531055832537864, 0.16139343315227214, 0.08761806919064921, 0.16075557629405587, 0.1816272508614946, 0.20183954741814938, 0.20183954741814938, 0.20183954741814938, 0.20183954741814938, 0.2007861172338509, 0.21030960427434428, 0.21030960427434428, 0.20964814228895548, 0.2090482143551676, 0.20964814228895548, 0.20964814228895548, 0.21030960427434428, 0.21030960427434428, 0.20183954741814938, 0.20183954741814938, 0.20183954741814938, 0.20183954741814938, 0.1569691853639599, 0.09411320837073964, 0.00018096296503503084, 0.11074157160010542, 0.1591557540115975, 0.1623056826290337, 0.16257792079174263, 0.16257792079174263, 0.16257792079174263, 0.16230477226154175, 0.16302607083731915, 0.16302607083731915, 0.16302607083731915, 0.16302607083731915, 0.16492844006638022, 0.16391698666392315, 0.16391698666392315, 0.16391698666392315, 0.16311135523644507, 0.16255061828598857, 0.16218988740785753, 0.16301609188360563, 0.16391698666392315, 0.1644485925454526, 0.16302607083731915, 0.16302607083731915, 0.16302607083731915, 0.16230477226154175, 0.16230477226154175, 0.16257792079174263, 0.16257792079174263, 0.16257792079174263, 0.16257792079174263, 0.16257792079174263, 0.16257792079174263, 0.14343570309029993, 0.10885266279575198, 0.3926203821049883, 0.3530882652157251, 0.4568894456238066, 0.6125620810438458, 0.6119902160807609, 0.5337867840063347, 0.5190107439833128, 0.0493236715970832, 0.05591006959735895, 0.0758283297041015, 0.07790805810057147, 0.07492298993721275, 0.07492298993721275, 0.07790805810057147, 0.08011734604277645, 0.05591006959735895, 0.05591006959735895, 0.05591006959735895, 0.05591006959735895, 0.01935587242097031, 0.01935587242097031, 0.01935587242097031, 0.01935587242097031, 0.01935587242097031, 0.017416978440624786, 0.021180217230968884, 0.03108673892437569, 0.03221319242179718, 0.04835031604634529, 0.03198682845888132, 0.03251065219708049, 0.03108673892437569, 0.03108673892437569, 0.03108673892437569, 0.01935587242097031, 0.01935587242097031, 0.01935587242097031, 0.0023381096933970946, 0.0023381096933970946, 0.0023381096933970946, 0.002931168489124031, 0.0032105829439158518, 0.0032105829439158518, 0.004100520847243216, 0.0038273723170424157, 0.0038273723170424157, 0.0038273723170424157, 0.0038273723170424157, 0.0072718231504592945, 0.0072718231504592945, 0.0072718231504592945, 0.0072718231504592945, 0.0032589119642452424, 0.0038273723170424157, 0.0038273723170424157, 0.0038273723170424157, 0.0038273723170424157, 0.004100520847243216, 0.004100520847243216, 0.004100520847243216, 0.004100520847243216, 0.0032105829439158518, 0.0032105829439158518, 0.0032105829439158518, 0.0032105829439158518, 0.0023381096933970946, 0.0023381096933970946, 0.0023381096933970946, 0.0023381096933970946, 0.0023381096933970946, 0.0, 0.10504410226421709, 0.43013141829003465, 0.5121804648279, 0.4992116218277194, 0.5121804648279, 0.3514635901040063, 0.10504410226421709, 0.07116733883633228, 0.08709512116187432, 0.09151495422122755, 0.08917484955834429, 0.08917484955834429, 0.0905814208212852, 0.09151495422122755, 0.09151495422122755, 0.08709512116187432, 0.07975899470816653, 0.02734256998424542, 0.02734256998424542, 0.02813463556088, 0.0349685208964324, 0.04487504258983921, 0.04487504258983921, 0.0422784001184473, 0.04087182885550639, 0.04087182885550639, 0.042939862103836074, 0.04382161240554075, 0.04487504258983921, 0.04487504258983921, 0.04487504258983921, 0.04487504258983921, 0.04487504258983921, 0.0349685208964324, 0.02813463556088, 0.026037142220680626, 0.024371396770535902, 0.02734256998424542, 0.02734256998424542, 0.02734256998424542, 0.02734256998424542, 0.02330146791776977, 0.02330146791776977, 0.011086465655300082, 0.011086465655300082, 0.013040127218813813, 0.01391260046933257, 0.01391260046933257, 0.01391260046933257, 0.01391260046933257, 0.01372985777095767, 0.01372985777095767, 0.01372985777095767, 0.01372985777095767, 0.01372985777095767, 0.01372985777095767, 0.01345670924075687, 0.01345670924075687, 0.01345670924075687, 0.01345670924075687, 0.01345670924075687, 0.011121923616896151, 0.011121923616896151, 0.010460461631507378, 0.008453962434778556, 0.008453962434778556, 0.009053890368566464, 0.009053890368566464, 0.010460461631507378, 0.01345670924075687, 0.01345670924075687, 0.01345670924075687, 0.01345670924075687, 0.01345670924075687, 0.01345670924075687, 0.01345670924075687, 0.01372985777095767, 0.01372985777095767, 0.01372985777095767, 0.01372985777095767, 0.01372985777095767, 0.01372985777095767, 0.01372985777095767, 0.01391260046933257, 0.01391260046933257, 0.013040127218813813, 0.013040127218813813, 0.013040127218813813, 0.012159146257002346, 0.011086465655300082], \"yaxis\": \"y\"}, {\"hovertemplate\": \"variable=RF<br>index=%{x}<br>value=%{y}<extra></extra>\", \"legendgroup\": \"RF\", \"line\": {\"color\": \"#ab63fa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"RF\", \"showlegend\": true, \"type\": \"scattergl\", \"x\": [2, 8, 10, 15, 23, 24, 28, 38, 42, 43, 44, 47, 51, 64, 80, 84, 87, 90, 92, 94, 99, 100, 103, 109, 117, 120, 128, 136, 146, 148, 159, 164, 170, 175, 177, 179, 184, 187, 189, 194, 200, 204, 208, 209, 218, 222, 225, 226, 227, 228, 230, 233, 238, 239, 242, 243, 249, 251, 252, 256, 257, 262, 263, 267, 270, 272, 273, 274, 276, 277, 285, 289, 290, 291, 293, 295, 302, 303, 306, 307, 309, 310, 313, 316, 328, 335, 346, 349, 350, 361, 367, 376, 380, 396, 400, 405, 408, 411, 412, 415, 423, 424, 430, 431, 432, 443, 446, 449, 451, 452, 453, 456, 465, 471, 482, 490, 498, 512, 513, 514, 516, 518, 520, 532, 550, 552, 557, 574, 579, 583, 589, 591, 607, 612, 616, 620, 626, 629, 642, 643, 644, 652, 657, 659, 665, 669, 670, 680, 683, 686, 688, 696, 700, 702, 708, 709, 710, 712, 714, 720, 722, 725, 727, 733, 740, 741, 744, 745, 746, 756, 765, 766, 767, 770, 771, 773, 777, 780, 782, 783, 785, 789, 791, 793, 794, 801, 805, 808, 811, 817, 827, 831, 832, 834, 842, 843, 849, 858, 862, 865, 867, 870, 871, 879, 883, 888, 889, 890, 893, 896, 897, 899, 904, 906, 911, 912, 913, 914, 917, 919, 922, 925, 926, 942, 944, 947, 957, 958, 959, 963, 969, 971, 972, 979, 983, 986, 995, 996, 998, 1001, 1005, 1007, 1009, 1016, 1019, 1023, 1027, 1029, 1033, 1036, 1041, 1042, 1046, 1049, 1052, 1054, 1061, 1065, 1067, 1074, 1078, 1082, 1084, 1087, 1088, 1104, 1106, 1107, 1108, 1114, 1116, 1119, 1123, 1126, 1127, 1138, 1140, 1149, 1160, 1161, 1162, 1171, 1173, 1179, 1180, 1182, 1183, 1185, 1189, 1196, 1199, 1204, 1209, 1215, 1220, 1224, 1228, 1229, 1234, 1235, 1243, 1254, 1255, 1258, 1262, 1264, 1273, 1274, 1278, 1290, 1295, 1298, 1302, 1304, 1316, 1318, 1327, 1328, 1331, 1334, 1339, 1344, 1345, 1346, 1347, 1353, 1354, 1358, 1360, 1362, 1363, 1367, 1376, 1385, 1394, 1396, 1404, 1405, 1407, 1415, 1423, 1424, 1426, 1432, 1434, 1438, 1439, 1441, 1446, 1452, 1468, 1470, 1479, 1482, 1491, 1492, 1493, 1502, 1508, 1511, 1520, 1522, 1524, 1528, 1536, 1537, 1544, 1545, 1546, 1551, 1552, 1557, 1560, 1561, 1562, 1563, 1565, 1567, 1572, 1576, 1583, 1588, 1594, 1597, 1606, 1609, 1611, 1616, 1617, 1629, 1630, 1631, 1633, 1635, 1650, 1652, 1656, 1657, 1660, 1668, 1677, 1678, 1681, 1692, 1695, 1697, 1701, 1704, 1705, 1706, 1707, 1710, 1711, 1712, 1713, 1714, 1716, 1717, 1720, 1721, 1722, 1723, 1733, 1737, 1740, 1741, 1744, 1745, 1750, 1752, 1753, 1756, 1760, 1769, 1771, 1772, 1774, 1780, 1784, 1785, 1786, 1792, 1795, 1797, 1799, 1800, 1807, 1808, 1810, 1814, 1820, 1821, 1822, 1827, 1830, 1831, 1834, 1835, 1836, 1839, 1844, 1846, 1851, 1852, 1857, 1861, 1864], \"xaxis\": \"x\", \"y\": [0.22224152244255702, 0.24833053528294258, 0.3931432487410069, 0.6885046986144835, 0.7183556060702182, 0.7179929690768827, 0.7109070535950343, 0.0549607625210119, 0.11366926184685783, 0.13857465046632272, 0.13888413290977325, 0.07846667359636772, 0.16022842720200886, 0.16236957697300547, 0.1623854920761616, 0.14158520537694574, 0.0452078167881744, 0.07732881477029549, 0.09702764844190767, 0.03807248112171552, 0.10294901888007263, 0.10430765713269069, 0.10771616256643163, 0.10795020268428557, 0.10606147781727077, 0.10445895860801374, 0.10273541261787766, 0.10354546673799109, 0.10695930752059929, 0.10731942746458423, 0.10715878865507983, 0.09234276310300288, 0.06033761617758876, 0.039455417379146096, 0.071000223329091, 0.07106794635599303, 0.07346107226836884, 0.07365101679154684, 0.07345371451047472, 0.07324781286339632, 0.07225772861210863, 0.07196141307453663, 0.07172530304594449, 0.0712320385804836, 0.0703490203436937, 0.06974512838574004, 0.06942869966289109, 0.06914998641887513, 0.06891657437919926, 0.06811760778095094, 0.06664403419905335, 0.06610128641636692, 0.06518791201496266, 0.06479610870043892, 0.06422912746654794, 0.06422912746654794, 0.06506324370690891, 0.06555526054196195, 0.06568393272998507, 0.06664403419905335, 0.06724783413090035, 0.06933411847842219, 0.0693667444398918, 0.07002382552518713, 0.07045952942024047, 0.07057212877506791, 0.07075995541213306, 0.07083705129800083, 0.07097234388473886, 0.0712320385804836, 0.07190896428369026, 0.07250828633345197, 0.07256118984153162, 0.07265747621562427, 0.07317581833648656, 0.07322004953433767, 0.07346107226836884, 0.07359508609822121, 0.07332609155540007, 0.07106794635599303, 0.071000223329091, 0.0644881315604951, 0.08083750767305739, 0.33200519902842957, 0.6797560337838794, 0.6813580215423716, 0.1428891638822929, 0.04861124265149161, 0.071418864365973, 0.1779895260148342, 0.1781516990213367, 0.1779442106005813, 0.17885838170447044, 0.023005518443603795, 0.08805895494185614, 0.11868445852046927, 0.11995650345467829, 0.12085699609580247, 0.12088488257744495, 0.1209553286974438, 0.11879670986774726, 0.11789546931150774, 0.11682055174110134, 0.11635331841623947, 0.11567993903378698, 0.11713046115862086, 0.11729881101153689, 0.11879670986774726, 0.1198252711139004, 0.12039565467475494, 0.12058728956864717, 0.12099470436784104, 0.11963622716090944, 0.10724628146101517, 0.08108571811968437, 0.08426903885068496, 0.08406912857681587, 0.0827823740137815, 0.08259668549503876, 0.08254200574776765, 0.08213886102759063, 0.0819513327987095, 0.08187740809187816, 0.07986680384917175, 0.07500464131300749, 0.07527541974130716, 0.07693544647062833, 0.08151959536418638, 0.0819556810322502, 0.08240456746408689, 0.08303747890745594, 0.08326474658052044, 0.08449766120217317, 0.08400619997473957, 0.08108571811968437, 0.039758786244131095, 0.3052158884927238, 0.4811905397121723, 0.8862495711270465, 0.8866970413041667, 0.8858793081467798, 0.8753033621152772, 0.818135155840003, 0.7523647297346672, 0.128843133283092, 0.21725397636602423, 0.1710808190489953, 0.2845988007354005, 0.2850484023995518, 0.2855760164889942, 0.2844818531470326, 0.2844818531470326, 0.28524464130542876, 0.28530103097896076, 0.2828873553662983, 0.2844017092347126, 0.2819074052193439, 0.2607517191094179, 0.1710808190489953, 0.15039724168526442, 0.12402569381078515, 0.1001939141640224, 0.19651319036407705, 0.2070932931336229, 0.20757000460614333, 0.20752527179834446, 0.20691808219312335, 0.2067162066882126, 0.2058715694278735, 0.20107204797986805, 0.20065336708756182, 0.2009269931825336, 0.20132423131531924, 0.2022604041019938, 0.2025327607780233, 0.20394214736850247, 0.20655107801969547, 0.20757000460614333, 0.20755776987059232, 0.2077124400427503, 0.20789651052600483, 0.20667831358546163, 0.2032005319812692, 0.19651319036407705, 0.17134291711478178, 0.09083397343073016, 0.11512884258856992, 0.1276475502696074, 0.15075464943042022, 0.1584856760400699, 0.15759012939618272, 0.15738800758610466, 0.15719503696034048, 0.1567303727878449, 0.1556788391864795, 0.15562980551151823, 0.15440212582201052, 0.15259974038961682, 0.14905999408914988, 0.1490883191834566, 0.14818524128539734, 0.14749364479379645, 0.14730350261748848, 0.14551333229743108, 0.14712690587208765, 0.14860423479580126, 0.1486273116953773, 0.14905999408914988, 0.1515251867283287, 0.1527275727717496, 0.15292879462063835, 0.153372286021971, 0.1544391189635726, 0.15457588938131733, 0.15540600884666567, 0.155433576078918, 0.15566500820397539, 0.1558893164257153, 0.15615021043816224, 0.1567062159348402, 0.15709244138464007, 0.15759012939618272, 0.15790395059989842, 0.15052182378650913, 0.1276475502696074, 0.11512884258856992, 0.9501719245596622, 0.9834913487559659, 0.9935496939504337, 0.9993806561820714, 0.9993806561820714, 1.0, 0.9985813532825362, 0.6414669386349386, 0.17037130717207732, 0.0534538201374985, 0.30467570336644434, 0.3049688538826086, 0.3049715442929042, 0.30564406634390723, 0.30539193037483314, 0.3044607486511213, 0.30330349938304063, 0.30567837854844837, 0.30564406634390723, 0.30499234760631483, 0.3009560216675606, 0.26520295557675494, 0.11659001010395925, 0.03787338805320675, 0.1567238220177064, 0.1887336274264408, 0.2222920718416282, 0.22430490609343035, 0.22550157027833656, 0.22564216316282182, 0.2245251123867979, 0.2216115827657717, 0.22054226308930197, 0.21799191517496308, 0.21760910010441176, 0.21922167898512251, 0.22002787842935867, 0.22054226308930197, 0.22099122530734466, 0.22481688927798726, 0.2233065886182427, 0.22254308996417055, 0.2222920718416282, 0.13116008052568506, 0.07057810830668176, 0.01088406432325767, 0.119497585771216, 0.16448798140907556, 0.17046766459573726, 0.17341545431828997, 0.17328292319141503, 0.17208842838664196, 0.17062496833853646, 0.17043213981190786, 0.1702556526858408, 0.16846790346438378, 0.16830771037250053, 0.1654389656622891, 0.1646222745652285, 0.16380638192997538, 0.16348582252059574, 0.16301879002917774, 0.16152802326390148, 0.1608305486047365, 0.1624670569798342, 0.16380638192997538, 0.16687877635430198, 0.16846790346438378, 0.1697420116780382, 0.1702556526858408, 0.17100354411821705, 0.17109768479536597, 0.1718650674924705, 0.17196157175190374, 0.173514099538028, 0.17362809146438146, 0.17349551297112858, 0.17187039621004563, 0.14620306965867746, 0.1072908153300246, 0.38539577841941797, 0.3543739622470541, 0.4118282799209919, 0.5929972077756728, 0.5921609963173965, 0.5657059652009169, 0.5322153181778437, 0.05182994911216826, 0.08665310235687296, 0.08672569417381468, 0.08631172528883618, 0.08614734500909838, 0.08591214252031718, 0.08625734300242169, 0.08712753773957632, 0.08665310235687296, 0.08666492689958669, 0.08656818012429565, 0.08665633842784787, 0.02651206091278377, 0.04220270427443465, 0.027776687798466038, 0.0408354519722329, 0.04322228688522314, 0.0443254901005099, 0.04533761724333196, 0.044718981552761244, 0.04232890535849454, 0.04058374040591062, 0.0408537031851737, 0.042713664345216784, 0.04291357130343981, 0.04367393646857967, 0.04522878588390991, 0.043192218708189206, 0.0408354519722329, 0.027776687798466038, 0.016127967660510556, 0.01962664731087002, 0.019839781803935752, 0.019818196945280447, 0.019581838716865585, 0.019487932841620842, 0.018987263725550763, 0.01750979743934067, 0.017368976248911233, 0.016381159899823222, 0.015280760774140117, 0.013400248763482614, 0.013358533193990774, 0.013034075869996858, 0.01336312820636315, 0.014349019112167877, 0.01532251376307106, 0.01722253835332624, 0.017368976248911233, 0.01750979743934067, 0.01765085119868784, 0.018306478136165122, 0.018409246125483725, 0.01924072595322035, 0.01920872901725665, 0.019487932841620842, 0.019581838716865585, 0.019687951246165403, 0.019548606465752266, 0.01945351372439655, 0.016127967660510556, 0.01544296456878863, 0.01445835966015857, 0.0, 0.16904341443570425, 0.456381734927656, 0.4763619722535798, 0.47857518049609815, 0.4763619722535798, 0.3623949795617628, 0.16904341443570425, 0.09816761222983769, 0.09786668652265237, 0.09807701408413208, 0.09800637755129227, 0.09748795727590537, 0.09832777500723916, 0.09824522961953219, 0.09807701408413208, 0.09786668652265237, 0.09815994645515139, 0.05244581481692781, 0.05326590215098878, 0.05443149155565219, 0.0546531775746818, 0.05506605798028405, 0.05417746945738963, 0.05188359295674261, 0.05156910246609961, 0.05047680014914732, 0.05210413155908783, 0.05297909771454146, 0.05352997343243529, 0.054762731271375265, 0.0550606781070207, 0.05507546967967997, 0.05506605798028405, 0.05497800483009185, 0.05443149155565219, 0.054221591238906075, 0.053958508899775326, 0.05335962837232677, 0.05326590215098878, 0.05244581481692781, 0.050079330394770666, 0.022166329737774992, 0.003285284642212255, 0.014241654689486044, 0.014241654689486044, 0.027767900387937694, 0.028039233002865538, 0.027868495613810773, 0.027860921729898125, 0.027685157008768557, 0.027580035384105583, 0.027137019067548923, 0.026889613578689936, 0.026813256608325953, 0.026617829088512568, 0.02636312538806354, 0.025515002435934214, 0.025345067027333495, 0.025179090500642487, 0.024969894521943536, 0.022981819149814453, 0.0222819212267287, 0.022283702202557876, 0.02202683243762313, 0.02067567996745351, 0.0207262672596272, 0.02111572546475496, 0.021821357087950938, 0.02202683243762313, 0.023303604536401068, 0.023954361736511165, 0.024529350256677296, 0.025179090500642487, 0.02585519487084631, 0.025949485699415242, 0.025966869160302608, 0.02636115731503974, 0.026617829088512568, 0.026709602394067417, 0.026889613578689936, 0.027040120246182825, 0.027137019067548923, 0.027472388184144297, 0.0279168477542521, 0.027868495613810773, 0.028064647433217182, 0.02799040347699519, 0.027359170698078666, 0.024650737495696268, 0.014241654689486044], \"yaxis\": \"y\"}],                        {\"legend\": {\"title\": {\"text\": \"variable\"}, \"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"index\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"value\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('99197b5f-614d-48e5-9bc4-f54f207528c5');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ToREP = pd.DataFrame()\n",
    "ToREP['Analytic Function'] =  np.ravel(x_test.iloc[:,[-1]].to_numpy())\n",
    "ToREP['Signal'] = y_test.to_numpy()\n",
    "ToREP['GB'] =  GBC\n",
    "ToREP['RF'] = RFC\n",
    "\n",
    "NormREP = pd.DataFrame(MMS.fit_transform(ToREP), columns = ToREP.columns, index=x_test.index)\n",
    "NormREP.sort_index().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Adamax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1398/1398 [==============================] - 1s 504us/step - loss: 0.3558 - MAE: 0.3558\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 499us/step - loss: 0.2351 - MAE: 0.2351\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 560us/step - loss: 0.2214 - MAE: 0.2214\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 603us/step - loss: 0.2155 - MAE: 0.2155\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 430us/step - loss: 0.2122 - MAE: 0.2122\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 649us/step - loss: 0.2086 - MAE: 0.2086\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 436us/step - loss: 0.2046 - MAE: 0.2046\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 525us/step - loss: 0.2011 - MAE: 0.2011\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 499us/step - loss: 0.1944 - MAE: 0.1944\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 496us/step - loss: 0.1878 - MAE: 0.1878\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 551us/step - loss: 0.1779 - MAE: 0.1779\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 652us/step - loss: 0.1644 - MAE: 0.1644\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 616us/step - loss: 0.1538 - MAE: 0.1538\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 579us/step - loss: 0.1370 - MAE: 0.1370\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 674us/step - loss: 0.1315 - MAE: 0.1315\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 693us/step - loss: 0.1177 - MAE: 0.1177\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 715us/step - loss: 0.1151 - MAE: 0.1151\n",
      "Epoch 18/50\n",
      "1398/1398 [==============================] - 1s 584us/step - loss: 0.1096 - MAE: 0.1096\n",
      "Epoch 19/50\n",
      "1398/1398 [==============================] - 1s 598us/step - loss: 0.1083 - MAE: 0.1083\n",
      "Epoch 20/50\n",
      "1398/1398 [==============================] - 1s 661us/step - loss: 0.1087 - MAE: 0.1087\n",
      "Epoch 21/50\n",
      "1398/1398 [==============================] - 1s 563us/step - loss: 0.1081 - MAE: 0.1081\n",
      "Epoch 22/50\n",
      "1398/1398 [==============================] - 1s 716us/step - loss: 0.1056 - MAE: 0.1056\n",
      "Epoch 23/50\n",
      "1398/1398 [==============================] - 1s 634us/step - loss: 0.1045 - MAE: 0.1045\n",
      "Epoch 24/50\n",
      "1398/1398 [==============================] - 1s 629us/step - loss: 0.1078 - MAE: 0.1078\n",
      "Epoch 25/50\n",
      "1398/1398 [==============================] - 1s 665us/step - loss: 0.1042 - MAE: 0.1042\n",
      "Epoch 26/50\n",
      "1398/1398 [==============================] - 1s 734us/step - loss: 0.1059 - MAE: 0.1059\n",
      "Epoch 27/50\n",
      "1398/1398 [==============================] - 1s 623us/step - loss: 0.1038 - MAE: 0.1038\n",
      "Epoch 28/50\n",
      "1398/1398 [==============================] - 1s 683us/step - loss: 0.1038 - MAE: 0.1038\n",
      "Epoch 29/50\n",
      "1398/1398 [==============================] - 1s 695us/step - loss: 0.1026 - MAE: 0.1026\n",
      "Epoch 30/50\n",
      "1398/1398 [==============================] - 1s 591us/step - loss: 0.1026 - MAE: 0.1026\n",
      "Epoch 31/50\n",
      "1398/1398 [==============================] - 1s 525us/step - loss: 0.1039 - MAE: 0.1039\n",
      "Epoch 32/50\n",
      "1398/1398 [==============================] - 1s 526us/step - loss: 0.1035 - MAE: 0.1035\n",
      "Epoch 33/50\n",
      "1398/1398 [==============================] - 1s 552us/step - loss: 0.1046 - MAE: 0.1046\n",
      "Epoch 34/50\n",
      "1398/1398 [==============================] - 1s 419us/step - loss: 0.1014 - MAE: 0.1014\n",
      "Epoch 35/50\n",
      "1398/1398 [==============================] - 1s 638us/step - loss: 0.1007 - MAE: 0.1007\n",
      "Epoch 36/50\n",
      "1398/1398 [==============================] - 1s 538us/step - loss: 0.1003 - MAE: 0.1003\n",
      "Epoch 37/50\n",
      "1398/1398 [==============================] - 1s 539us/step - loss: 0.1003 - MAE: 0.1003\n",
      "Epoch 38/50\n",
      "1398/1398 [==============================] - 1s 503us/step - loss: 0.1006 - MAE: 0.1006\n",
      "Epoch 39/50\n",
      "1398/1398 [==============================] - 1s 647us/step - loss: 0.1010 - MAE: 0.1010\n",
      "Epoch 40/50\n",
      "1398/1398 [==============================] - 1s 560us/step - loss: 0.0971 - MAE: 0.0971\n",
      "Epoch 41/50\n",
      "1398/1398 [==============================] - 1s 628us/step - loss: 0.0989 - MAE: 0.0989\n",
      "Epoch 42/50\n",
      "1398/1398 [==============================] - 1s 671us/step - loss: 0.0976 - MAE: 0.0976\n",
      "Epoch 43/50\n",
      "1398/1398 [==============================] - 1s 702us/step - loss: 0.0987 - MAE: 0.0987\n",
      "Epoch 44/50\n",
      "1398/1398 [==============================] - 1s 509us/step - loss: 0.0997 - MAE: 0.0997\n",
      "Epoch 45/50\n",
      "1398/1398 [==============================] - 1s 625us/step - loss: 0.0976 - MAE: 0.0976\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Test score: 0.08374973015207295\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ModelDenseAdamax1 = Sequential()\n",
    "ModelDenseAdamax1.add(Dense(14))\n",
    "ModelDenseAdamax1.add(Dense(70, activation='relu'))\n",
    "ModelDenseAdamax1.add(Dense(7, activation='relu'))\n",
    "ModelDenseAdamax1.add(Dense(1, activation='linear'))\n",
    "ModelDenseAdamax1.compile(optimizer='Adamax', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseAdamax1.fit(x_train.values, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "\n",
    "DenseAdamaxPrediction1 = ModelDenseAdamax1.predict(x_test)\n",
    "score = mean_absolute_error(DenseAdamaxPrediction1, y_test)\n",
    "print('Test score:', score)\n",
    "scores.append(score)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1398/1398 [==============================] - 1s 460us/step - loss: 0.4492 - MAE: 0.4492\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 428us/step - loss: 0.2727 - MAE: 0.2727\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 484us/step - loss: 0.2525 - MAE: 0.2525\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 480us/step - loss: 0.2481 - MAE: 0.2481\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 439us/step - loss: 0.2397 - MAE: 0.2397\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 437us/step - loss: 0.2373 - MAE: 0.2373\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 430us/step - loss: 0.2347 - MAE: 0.2347 0s - loss: 0.2203 - MAE:\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 519us/step - loss: 0.2302 - MAE: 0.2302\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 492us/step - loss: 0.2330 - MAE: 0.2330\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 535us/step - loss: 0.2304 - MAE: 0.2304\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 551us/step - loss: 0.2281 - MAE: 0.2281\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 470us/step - loss: 0.2284 - MAE: 0.2284\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 523us/step - loss: 0.2228 - MAE: 0.2228\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 474us/step - loss: 0.2196 - MAE: 0.2196\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 493us/step - loss: 0.2122 - MAE: 0.2122\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 481us/step - loss: 0.2088 - MAE: 0.2088\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 545us/step - loss: 0.2045 - MAE: 0.2045\n",
      "Epoch 18/50\n",
      "1398/1398 [==============================] - 1s 525us/step - loss: 0.1926 - MAE: 0.1926\n",
      "Epoch 19/50\n",
      "1398/1398 [==============================] - 1s 474us/step - loss: 0.1899 - MAE: 0.1899\n",
      "Epoch 20/50\n",
      "1398/1398 [==============================] - 1s 413us/step - loss: 0.1830 - MAE: 0.1830\n",
      "Epoch 21/50\n",
      "1398/1398 [==============================] - 1s 482us/step - loss: 0.1690 - MAE: 0.1690\n",
      "Epoch 22/50\n",
      "1398/1398 [==============================] - 1s 438us/step - loss: 0.1667 - MAE: 0.1667\n",
      "Epoch 23/50\n",
      "1398/1398 [==============================] - 1s 506us/step - loss: 0.1574 - MAE: 0.1574\n",
      "Epoch 24/50\n",
      "1398/1398 [==============================] - 1s 452us/step - loss: 0.1493 - MAE: 0.1493\n",
      "Epoch 25/50\n",
      "1398/1398 [==============================] - 1s 569us/step - loss: 0.1553 - MAE: 0.1553\n",
      "Epoch 26/50\n",
      "1398/1398 [==============================] - 1s 558us/step - loss: 0.1436 - MAE: 0.1436\n",
      "Epoch 27/50\n",
      "1398/1398 [==============================] - 1s 436us/step - loss: 0.1478 - MAE: 0.1478\n",
      "Epoch 28/50\n",
      "1398/1398 [==============================] - 1s 458us/step - loss: 0.1406 - MAE: 0.1406\n",
      "Epoch 29/50\n",
      "1398/1398 [==============================] - 1s 524us/step - loss: 0.1411 - MAE: 0.1411\n",
      "Epoch 30/50\n",
      "1398/1398 [==============================] - 1s 439us/step - loss: 0.1359 - MAE: 0.1359\n",
      "Epoch 31/50\n",
      "1398/1398 [==============================] - 1s 439us/step - loss: 0.1351 - MAE: 0.1351\n",
      "Epoch 32/50\n",
      "1398/1398 [==============================] - 1s 504us/step - loss: 0.1345 - MAE: 0.1345\n",
      "Epoch 33/50\n",
      "1398/1398 [==============================] - 1s 550us/step - loss: 0.1349 - MAE: 0.1349\n",
      "Epoch 34/50\n",
      "1398/1398 [==============================] - 1s 513us/step - loss: 0.1317 - MAE: 0.1317\n",
      "Epoch 35/50\n",
      "1398/1398 [==============================] - 1s 428us/step - loss: 0.1303 - MAE: 0.1303\n",
      "Epoch 36/50\n",
      "1398/1398 [==============================] - 1s 400us/step - loss: 0.1327 - MAE: 0.1327\n",
      "Epoch 37/50\n",
      "1398/1398 [==============================] - 1s 482us/step - loss: 0.1318 - MAE: 0.1318\n",
      "Epoch 38/50\n",
      "1398/1398 [==============================] - 1s 523us/step - loss: 0.1303 - MAE: 0.1303\n",
      "Epoch 39/50\n",
      "1398/1398 [==============================] - 1s 562us/step - loss: 0.1310 - MAE: 0.1310\n",
      "Epoch 40/50\n",
      "1398/1398 [==============================] - 1s 420us/step - loss: 0.1280 - MAE: 0.1280\n",
      "Epoch 41/50\n",
      "1398/1398 [==============================] - 1s 436us/step - loss: 0.1299 - MAE: 0.1299\n",
      "Epoch 42/50\n",
      "1398/1398 [==============================] - 1s 493us/step - loss: 0.1336 - MAE: 0.1336\n",
      "Epoch 43/50\n",
      "1398/1398 [==============================] - 1s 481us/step - loss: 0.1287 - MAE: 0.1287\n",
      "Epoch 44/50\n",
      "1398/1398 [==============================] - 1s 460us/step - loss: 0.1275 - MAE: 0.1275\n",
      "Epoch 45/50\n",
      "1398/1398 [==============================] - 1s 447us/step - loss: 0.1291 - MAE: 0.1291\n",
      "Epoch 46/50\n",
      "1398/1398 [==============================] - 1s 547us/step - loss: 0.1240 - MAE: 0.1240\n",
      "Epoch 47/50\n",
      "1398/1398 [==============================] - 1s 411us/step - loss: 0.1271 - MAE: 0.1271\n",
      "Epoch 48/50\n",
      "1398/1398 [==============================] - 1s 416us/step - loss: 0.1231 - MAE: 0.1231\n",
      "Epoch 49/50\n",
      "1398/1398 [==============================] - 1s 413us/step - loss: 0.1260 - MAE: 0.1260\n",
      "Epoch 50/50\n",
      "1398/1398 [==============================] - 1s 475us/step - loss: 0.1251 - MAE: 0.1251\n",
      "WARNING:tensorflow:Layer dense_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Test score: 0.08828327438828266\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ModelDenseAdamax1_1 = Sequential()\n",
    "ModelDenseAdamax1_1.add(Dense(14))\n",
    "ModelDenseAdamax1_1.add(Dropout(0.1))\n",
    "ModelDenseAdamax1_1.add(Dense(70, activation='relu'))\n",
    "ModelDenseAdamax1_1.add(Dropout(0.1))\n",
    "ModelDenseAdamax1_1.add(Dense(7, activation='relu'))\n",
    "ModelDenseAdamax1_1.add(Dense(1, activation='linear'))\n",
    "ModelDenseAdamax1_1.compile(optimizer='Adamax', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseAdamax1_1.fit(x_train.values, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "\n",
    "DenseAdamaxPrediction1_1 = ModelDenseAdamax1_1.predict(x_test)\n",
    "score = mean_absolute_error(DenseAdamaxPrediction1_1, y_test)\n",
    "print('Test score:', score)\n",
    "scores.append(score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_8 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 408us/step - loss: 0.3244 - MAE: 0.3244\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 614us/step - loss: 0.2364 - MAE: 0.2364\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 521us/step - loss: 0.2208 - MAE: 0.2208\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 618us/step - loss: 0.2178 - MAE: 0.2178\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 406us/step - loss: 0.2132 - MAE: 0.2132\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 474us/step - loss: 0.2089 - MAE: 0.2089\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 661us/step - loss: 0.2059 - MAE: 0.2059\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 490us/step - loss: 0.2039 - MAE: 0.2039\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 486us/step - loss: 0.1965 - MAE: 0.1965\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 469us/step - loss: 0.1913 - MAE: 0.1913\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 558us/step - loss: 0.1813 - MAE: 0.1813\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 568us/step - loss: 0.1768 - MAE: 0.1768\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 424us/step - loss: 0.1650 - MAE: 0.1650\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 555us/step - loss: 0.1549 - MAE: 0.1549\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 534us/step - loss: 0.1434 - MAE: 0.1434\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 548us/step - loss: 0.1330 - MAE: 0.1330\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 427us/step - loss: 0.1251 - MAE: 0.1251\n",
      "Epoch 18/50\n",
      "1398/1398 [==============================] - 1s 557us/step - loss: 0.1195 - MAE: 0.1195\n",
      "Epoch 19/50\n",
      "1398/1398 [==============================] - 1s 526us/step - loss: 0.1164 - MAE: 0.1164\n",
      "Epoch 20/50\n",
      "1398/1398 [==============================] - 1s 407us/step - loss: 0.1139 - MAE: 0.1139\n",
      "Epoch 21/50\n",
      "1398/1398 [==============================] - 1s 533us/step - loss: 0.1110 - MAE: 0.1110\n",
      "Epoch 22/50\n",
      "1398/1398 [==============================] - 1s 425us/step - loss: 0.1091 - MAE: 0.1091\n",
      "Epoch 23/50\n",
      "1398/1398 [==============================] - 1s 473us/step - loss: 0.1088 - MAE: 0.1088\n",
      "Epoch 24/50\n",
      "1398/1398 [==============================] - 1s 482us/step - loss: 0.1084 - MAE: 0.1084\n",
      "Epoch 25/50\n",
      "1398/1398 [==============================] - 1s 508us/step - loss: 0.1091 - MAE: 0.1091\n",
      "Epoch 26/50\n",
      "1398/1398 [==============================] - 1s 421us/step - loss: 0.1083 - MAE: 0.1083\n",
      "Epoch 27/50\n",
      "1398/1398 [==============================] - 1s 561us/step - loss: 0.1057 - MAE: 0.1057 0s - loss: 0.1037 - MAE: 0\n",
      "Epoch 28/50\n",
      "1398/1398 [==============================] - 1s 493us/step - loss: 0.1055 - MAE: 0.1055\n",
      "Epoch 29/50\n",
      "1398/1398 [==============================] - 1s 435us/step - loss: 0.1063 - MAE: 0.1063\n",
      "Epoch 30/50\n",
      "1398/1398 [==============================] - 1s 454us/step - loss: 0.1060 - MAE: 0.1060\n",
      "Epoch 31/50\n",
      "1398/1398 [==============================] - 1s 440us/step - loss: 0.1044 - MAE: 0.1044 0s - loss: 0.1064 - MAE: 0.1\n",
      "Epoch 32/50\n",
      "1398/1398 [==============================] - 1s 459us/step - loss: 0.1070 - MAE: 0.1070\n",
      "Epoch 33/50\n",
      "1398/1398 [==============================] - 1s 437us/step - loss: 0.1059 - MAE: 0.1059\n",
      "Epoch 34/50\n",
      "1398/1398 [==============================] - 1s 535us/step - loss: 0.1058 - MAE: 0.1058\n",
      "Epoch 35/50\n",
      "1398/1398 [==============================] - 1s 582us/step - loss: 0.1013 - MAE: 0.1013\n",
      "Epoch 36/50\n",
      "1398/1398 [==============================] - 1s 494us/step - loss: 0.1023 - MAE: 0.1023\n",
      "Epoch 37/50\n",
      "1398/1398 [==============================] - 1s 535us/step - loss: 0.1040 - MAE: 0.1040\n",
      "Epoch 38/50\n",
      "1398/1398 [==============================] - 1s 528us/step - loss: 0.1017 - MAE: 0.1017\n",
      "Epoch 39/50\n",
      "1398/1398 [==============================] - 1s 578us/step - loss: 0.1027 - MAE: 0.1027\n",
      "Epoch 40/50\n",
      "1398/1398 [==============================] - 1s 606us/step - loss: 0.1033 - MAE: 0.1033\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (1, 70)                   1050      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (1, 7)                    497       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (1, 1)                    8         \n",
      "=================================================================\n",
      "Total params: 1,681\n",
      "Trainable params: 1,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.08670090998108433\n"
     ]
    }
   ],
   "source": [
    "ModelDenseAdamax2 = Sequential()\n",
    "ModelDenseAdamax2.add(Dense(14))\n",
    "ModelDenseAdamax2.add(Dense(70, activation='relu'))\n",
    "ModelDenseAdamax2.add(Dense(7, activation='elu'))\n",
    "ModelDenseAdamax2.add(Dense(1, activation='linear'))\n",
    "ModelDenseAdamax2.compile(optimizer='Adamax', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseAdamax2.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseAdamax2.summary()\n",
    "\n",
    "DenseAdamaxPrediction2 = ModelDenseAdamax2.predict(x_test)\n",
    "score = mean_absolute_error(DenseAdamaxPrediction2, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_12 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 478us/step - loss: 0.3697 - MAE: 0.3697\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 528us/step - loss: 0.2726 - MAE: 0.2726\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 424us/step - loss: 0.2511 - MAE: 0.2511\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 506us/step - loss: 0.2496 - MAE: 0.2496\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 443us/step - loss: 0.2368 - MAE: 0.2368\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 480us/step - loss: 0.2373 - MAE: 0.2373\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 454us/step - loss: 0.2334 - MAE: 0.2334\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 453us/step - loss: 0.2258 - MAE: 0.2258\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 448us/step - loss: 0.2236 - MAE: 0.2236\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 410us/step - loss: 0.2181 - MAE: 0.2181\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 482us/step - loss: 0.2150 - MAE: 0.2150\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 420us/step - loss: 0.2082 - MAE: 0.2082\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 400us/step - loss: 0.1976 - MAE: 0.1976\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 471us/step - loss: 0.1887 - MAE: 0.1887\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 543us/step - loss: 0.1820 - MAE: 0.1820\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 512us/step - loss: 0.1708 - MAE: 0.1708\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 472us/step - loss: 0.1588 - MAE: 0.1588\n",
      "Epoch 18/50\n",
      "1398/1398 [==============================] - 1s 399us/step - loss: 0.1535 - MAE: 0.1535\n",
      "Epoch 19/50\n",
      "1398/1398 [==============================] - 1s 408us/step - loss: 0.1570 - MAE: 0.1570\n",
      "Epoch 20/50\n",
      "1398/1398 [==============================] - 1s 410us/step - loss: 0.1429 - MAE: 0.1429\n",
      "Epoch 21/50\n",
      "1398/1398 [==============================] - 1s 526us/step - loss: 0.1467 - MAE: 0.1467\n",
      "Epoch 22/50\n",
      "1398/1398 [==============================] - 1s 448us/step - loss: 0.1410 - MAE: 0.1410\n",
      "Epoch 23/50\n",
      "1398/1398 [==============================] - 1s 495us/step - loss: 0.1362 - MAE: 0.1362\n",
      "Epoch 24/50\n",
      "1398/1398 [==============================] - 1s 510us/step - loss: 0.1380 - MAE: 0.1380\n",
      "Epoch 25/50\n",
      "1398/1398 [==============================] - 1s 526us/step - loss: 0.1353 - MAE: 0.1353\n",
      "Epoch 26/50\n",
      "1398/1398 [==============================] - 1s 456us/step - loss: 0.1350 - MAE: 0.1350\n",
      "Epoch 27/50\n",
      "1398/1398 [==============================] - 1s 412us/step - loss: 0.1289 - MAE: 0.1289\n",
      "Epoch 28/50\n",
      "1398/1398 [==============================] - 1s 427us/step - loss: 0.1355 - MAE: 0.1355\n",
      "Epoch 29/50\n",
      "1398/1398 [==============================] - 1s 530us/step - loss: 0.1384 - MAE: 0.1384\n",
      "Epoch 30/50\n",
      "1398/1398 [==============================] - 1s 445us/step - loss: 0.1376 - MAE: 0.1376\n",
      "Epoch 31/50\n",
      "1398/1398 [==============================] - 1s 426us/step - loss: 0.1338 - MAE: 0.1338\n",
      "Epoch 32/50\n",
      "1398/1398 [==============================] - 1s 446us/step - loss: 0.1264 - MAE: 0.1264\n",
      "Epoch 33/50\n",
      "1398/1398 [==============================] - 1s 482us/step - loss: 0.1311 - MAE: 0.1311\n",
      "Epoch 34/50\n",
      "1398/1398 [==============================] - 1s 430us/step - loss: 0.1345 - MAE: 0.1345\n",
      "Epoch 35/50\n",
      "1398/1398 [==============================] - 1s 456us/step - loss: 0.1302 - MAE: 0.1302\n",
      "Epoch 36/50\n",
      "1398/1398 [==============================] - 1s 459us/step - loss: 0.1264 - MAE: 0.1264\n",
      "Epoch 37/50\n",
      "1398/1398 [==============================] - 1s 433us/step - loss: 0.1285 - MAE: 0.1285\n",
      "Epoch 38/50\n",
      "1398/1398 [==============================] - 1s 483us/step - loss: 0.1269 - MAE: 0.1269\n",
      "Epoch 39/50\n",
      "1398/1398 [==============================] - 1s 457us/step - loss: 0.1241 - MAE: 0.1241\n",
      "Epoch 40/50\n",
      "1398/1398 [==============================] - 1s 435us/step - loss: 0.1272 - MAE: 0.1272\n",
      "Epoch 41/50\n",
      "1398/1398 [==============================] - 1s 430us/step - loss: 0.1252 - MAE: 0.1252\n",
      "Epoch 42/50\n",
      "1398/1398 [==============================] - 1s 434us/step - loss: 0.1297 - MAE: 0.1297\n",
      "Epoch 43/50\n",
      "1398/1398 [==============================] - 1s 434us/step - loss: 0.1261 - MAE: 0.1261\n",
      "Epoch 44/50\n",
      "1398/1398 [==============================] - 1s 413us/step - loss: 0.1226 - MAE: 0.1226\n",
      "Epoch 45/50\n",
      "1398/1398 [==============================] - 1s 446us/step - loss: 0.1192 - MAE: 0.1192 0s - loss: 0.1390 - MA\n",
      "Epoch 46/50\n",
      "1398/1398 [==============================] - 1s 534us/step - loss: 0.1259 - MAE: 0.1259\n",
      "Epoch 47/50\n",
      "1398/1398 [==============================] - 1s 494us/step - loss: 0.1253 - MAE: 0.1253\n",
      "Epoch 48/50\n",
      "1398/1398 [==============================] - 1s 580us/step - loss: 0.1211 - MAE: 0.1211\n",
      "Epoch 49/50\n",
      "1398/1398 [==============================] - 1s 432us/step - loss: 0.1187 - MAE: 0.1187\n",
      "Epoch 50/50\n",
      "1398/1398 [==============================] - 1s 417us/step - loss: 0.1221 - MAE: 0.1221\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (1, 14)                   0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (1, 70)                   1050      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (1, 70)                   0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (1, 7)                    497       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (1, 1)                    8         \n",
      "=================================================================\n",
      "Total params: 1,681\n",
      "Trainable params: 1,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.09322276289100566\n"
     ]
    }
   ],
   "source": [
    "ModelDenseAdamax2_1 = Sequential()\n",
    "ModelDenseAdamax2_1.add(Dense(14))\n",
    "ModelDenseAdamax2_1.add(Dropout(0.1))\n",
    "ModelDenseAdamax2_1.add(Dense(70, activation='relu'))\n",
    "ModelDenseAdamax2_1.add(Dropout(0.1))\n",
    "ModelDenseAdamax2_1.add(Dense(7, activation='elu'))\n",
    "ModelDenseAdamax2_1.add(Dense(1, activation='linear'))\n",
    "ModelDenseAdamax2_1.compile(optimizer='Adamax', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseAdamax2_1.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseAdamax2_1.summary()\n",
    "\n",
    "DenseAdamaxPrediction2_1 = ModelDenseAdamax2_1.predict(x_test)\n",
    "score = mean_absolute_error(DenseAdamaxPrediction2_1, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_16 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 644us/step - loss: 0.2922 - MAE: 0.2922\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 622us/step - loss: 0.2319 - MAE: 0.2319\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 531us/step - loss: 0.2231 - MAE: 0.2231\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 598us/step - loss: 0.2197 - MAE: 0.2197\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 467us/step - loss: 0.2145 - MAE: 0.2145\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 441us/step - loss: 0.2121 - MAE: 0.2121\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 414us/step - loss: 0.2051 - MAE: 0.2051\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 456us/step - loss: 0.2034 - MAE: 0.2034\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 581us/step - loss: 0.1974 - MAE: 0.1974\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 563us/step - loss: 0.1901 - MAE: 0.1901\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 435us/step - loss: 0.1779 - MAE: 0.1779\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 513us/step - loss: 0.1652 - MAE: 0.1652\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 462us/step - loss: 0.1542 - MAE: 0.1542\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 486us/step - loss: 0.1436 - MAE: 0.1436\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 641us/step - loss: 0.1321 - MAE: 0.1321\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 619us/step - loss: 0.1243 - MAE: 0.1243\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 457us/step - loss: 0.1190 - MAE: 0.1190\n",
      "Epoch 18/50\n",
      "1398/1398 [==============================] - 1s 571us/step - loss: 0.1169 - MAE: 0.1169\n",
      "Epoch 19/50\n",
      "1398/1398 [==============================] - 1s 616us/step - loss: 0.1151 - MAE: 0.1151\n",
      "Epoch 20/50\n",
      "1398/1398 [==============================] - 1s 581us/step - loss: 0.1135 - MAE: 0.1135\n",
      "Epoch 21/50\n",
      "1398/1398 [==============================] - 1s 496us/step - loss: 0.1125 - MAE: 0.1125 0s - loss: 0.1271 - MA\n",
      "Epoch 22/50\n",
      "1398/1398 [==============================] - 1s 554us/step - loss: 0.1127 - MAE: 0.1127\n",
      "Epoch 23/50\n",
      "1398/1398 [==============================] - 1s 444us/step - loss: 0.1090 - MAE: 0.1090\n",
      "Epoch 24/50\n",
      "1398/1398 [==============================] - 1s 419us/step - loss: 0.1076 - MAE: 0.1076\n",
      "Epoch 25/50\n",
      "1398/1398 [==============================] - 1s 512us/step - loss: 0.1084 - MAE: 0.1084\n",
      "Epoch 26/50\n",
      "1398/1398 [==============================] - 1s 591us/step - loss: 0.1044 - MAE: 0.1044\n",
      "Epoch 27/50\n",
      "1398/1398 [==============================] - 1s 564us/step - loss: 0.1073 - MAE: 0.1073\n",
      "Epoch 28/50\n",
      "1398/1398 [==============================] - 1s 416us/step - loss: 0.1053 - MAE: 0.1053\n",
      "Epoch 29/50\n",
      "1398/1398 [==============================] - 1s 407us/step - loss: 0.1051 - MAE: 0.1051\n",
      "Epoch 30/50\n",
      "1398/1398 [==============================] - 1s 478us/step - loss: 0.1056 - MAE: 0.1056\n",
      "Epoch 31/50\n",
      "1398/1398 [==============================] - 1s 560us/step - loss: 0.1043 - MAE: 0.1043\n",
      "Epoch 32/50\n",
      "1398/1398 [==============================] - 1s 598us/step - loss: 0.1040 - MAE: 0.1040\n",
      "Epoch 33/50\n",
      "1398/1398 [==============================] - 1s 545us/step - loss: 0.1004 - MAE: 0.1004\n",
      "Epoch 34/50\n",
      "1398/1398 [==============================] - 1s 534us/step - loss: 0.1030 - MAE: 0.1030\n",
      "Epoch 35/50\n",
      "1398/1398 [==============================] - 1s 523us/step - loss: 0.1049 - MAE: 0.1049\n",
      "Epoch 36/50\n",
      "1398/1398 [==============================] - 1s 491us/step - loss: 0.1023 - MAE: 0.1023\n",
      "Epoch 37/50\n",
      "1398/1398 [==============================] - 1s 490us/step - loss: 0.1016 - MAE: 0.1016\n",
      "Epoch 38/50\n",
      "1398/1398 [==============================] - 1s 596us/step - loss: 0.1002 - MAE: 0.1002\n",
      "Epoch 39/50\n",
      "1398/1398 [==============================] - 1s 427us/step - loss: 0.1004 - MAE: 0.1004\n",
      "Epoch 40/50\n",
      "1398/1398 [==============================] - 1s 552us/step - loss: 0.1020 - MAE: 0.1020\n",
      "Epoch 41/50\n",
      "1398/1398 [==============================] - 1s 419us/step - loss: 0.1005 - MAE: 0.1005\n",
      "Epoch 42/50\n",
      "1398/1398 [==============================] - 1s 483us/step - loss: 0.0989 - MAE: 0.0989\n",
      "Epoch 43/50\n",
      "1398/1398 [==============================] - 1s 519us/step - loss: 0.0988 - MAE: 0.0988\n",
      "Epoch 44/50\n",
      "1398/1398 [==============================] - 1s 465us/step - loss: 0.1006 - MAE: 0.1006\n",
      "Epoch 45/50\n",
      "1398/1398 [==============================] - 1s 413us/step - loss: 0.0998 - MAE: 0.0998\n",
      "Epoch 46/50\n",
      "1398/1398 [==============================] - 1s 399us/step - loss: 0.0987 - MAE: 0.0987\n",
      "Epoch 47/50\n",
      "1398/1398 [==============================] - 1s 515us/step - loss: 0.0978 - MAE: 0.0978\n",
      "Epoch 48/50\n",
      "1398/1398 [==============================] - 1s 498us/step - loss: 0.0966 - MAE: 0.0966\n",
      "Epoch 49/50\n",
      "1398/1398 [==============================] - 1s 516us/step - loss: 0.0987 - MAE: 0.0987\n",
      "Epoch 50/50\n",
      "1398/1398 [==============================] - 1s 507us/step - loss: 0.0979 - MAE: 0.0979\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (1, 70)                   1050      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (1, 7)                    497       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (1, 1)                    8         \n",
      "=================================================================\n",
      "Total params: 1,681\n",
      "Trainable params: 1,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.08132105667976039\n"
     ]
    }
   ],
   "source": [
    "ModelDenseAdamax3 = Sequential()\n",
    "ModelDenseAdamax3.add(Dense(14))\n",
    "ModelDenseAdamax3.add(Dense(70, activation='relu'))\n",
    "ModelDenseAdamax3.add(Dense(7, activation='selu'))\n",
    "ModelDenseAdamax3.add(Dense(1, activation='linear'))\n",
    "ModelDenseAdamax3.compile(optimizer='Adamax', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseAdamax3.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseAdamax3.summary()\n",
    "\n",
    "DenseAdamaxPrediction3 = ModelDenseAdamax3.predict(x_test)\n",
    "score = mean_absolute_error(DenseAdamaxPrediction3, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_20 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 399us/step - loss: 0.3126 - MAE: 0.3126\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 519us/step - loss: 0.2341 - MAE: 0.2341\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 604us/step - loss: 0.2275 - MAE: 0.2275\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 585us/step - loss: 0.2211 - MAE: 0.2211\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 554us/step - loss: 0.2126 - MAE: 0.2126\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 519us/step - loss: 0.2086 - MAE: 0.2086\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 470us/step - loss: 0.2048 - MAE: 0.2048\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 591us/step - loss: 0.1955 - MAE: 0.1955\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 462us/step - loss: 0.1853 - MAE: 0.1853\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 592us/step - loss: 0.1737 - MAE: 0.1737\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 601us/step - loss: 0.1580 - MAE: 0.1580\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 396us/step - loss: 0.1454 - MAE: 0.1454\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 555us/step - loss: 0.1323 - MAE: 0.1323\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 470us/step - loss: 0.1255 - MAE: 0.1255\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 512us/step - loss: 0.1203 - MAE: 0.1203\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 484us/step - loss: 0.1164 - MAE: 0.1164\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 413us/step - loss: 0.1157 - MAE: 0.1157\n",
      "Epoch 18/50\n",
      "1398/1398 [==============================] - 1s 435us/step - loss: 0.1128 - MAE: 0.1128\n",
      "Epoch 19/50\n",
      "1398/1398 [==============================] - 1s 440us/step - loss: 0.1125 - MAE: 0.1125\n",
      "Epoch 20/50\n",
      "1398/1398 [==============================] - 1s 450us/step - loss: 0.1107 - MAE: 0.1107\n",
      "Epoch 21/50\n",
      "1398/1398 [==============================] - 1s 423us/step - loss: 0.1110 - MAE: 0.1110\n",
      "Epoch 22/50\n",
      "1398/1398 [==============================] - 1s 415us/step - loss: 0.1079 - MAE: 0.1079\n",
      "Epoch 23/50\n",
      "1398/1398 [==============================] - 1s 425us/step - loss: 0.1089 - MAE: 0.1089\n",
      "Epoch 24/50\n",
      "1398/1398 [==============================] - 1s 421us/step - loss: 0.1095 - MAE: 0.1095\n",
      "Epoch 25/50\n",
      "1398/1398 [==============================] - 1s 409us/step - loss: 0.1061 - MAE: 0.1061\n",
      "Epoch 26/50\n",
      "1398/1398 [==============================] - 1s 456us/step - loss: 0.1069 - MAE: 0.1069\n",
      "Epoch 27/50\n",
      "1398/1398 [==============================] - 1s 447us/step - loss: 0.1065 - MAE: 0.1065\n",
      "Epoch 28/50\n",
      "1398/1398 [==============================] - 1s 390us/step - loss: 0.1050 - MAE: 0.1050\n",
      "Epoch 29/50\n",
      "1398/1398 [==============================] - 1s 432us/step - loss: 0.1061 - MAE: 0.1061\n",
      "Epoch 30/50\n",
      "1398/1398 [==============================] - 1s 407us/step - loss: 0.1035 - MAE: 0.1035\n",
      "Epoch 31/50\n",
      "1398/1398 [==============================] - 1s 412us/step - loss: 0.1038 - MAE: 0.1038\n",
      "Epoch 32/50\n",
      "1398/1398 [==============================] - 1s 424us/step - loss: 0.1019 - MAE: 0.1019\n",
      "Epoch 33/50\n",
      "1398/1398 [==============================] - 1s 406us/step - loss: 0.1035 - MAE: 0.1035\n",
      "Epoch 34/50\n",
      "1398/1398 [==============================] - 1s 413us/step - loss: 0.1023 - MAE: 0.1023\n",
      "Epoch 35/50\n",
      "1398/1398 [==============================] - 1s 400us/step - loss: 0.1042 - MAE: 0.1042\n",
      "Epoch 36/50\n",
      "1398/1398 [==============================] - 1s 481us/step - loss: 0.1001 - MAE: 0.1001\n",
      "Epoch 37/50\n",
      "1398/1398 [==============================] - 1s 527us/step - loss: 0.1006 - MAE: 0.1006\n",
      "Epoch 38/50\n",
      "1398/1398 [==============================] - 1s 516us/step - loss: 0.1037 - MAE: 0.1037\n",
      "Epoch 39/50\n",
      "1398/1398 [==============================] - 1s 514us/step - loss: 0.1024 - MAE: 0.1024\n",
      "Epoch 40/50\n",
      "1398/1398 [==============================] - 1s 497us/step - loss: 0.1004 - MAE: 0.1004\n",
      "Epoch 41/50\n",
      "1398/1398 [==============================] - 1s 520us/step - loss: 0.0986 - MAE: 0.0986\n",
      "Epoch 42/50\n",
      "1398/1398 [==============================] - 1s 391us/step - loss: 0.1007 - MAE: 0.1007\n",
      "Epoch 43/50\n",
      "1398/1398 [==============================] - 1s 558us/step - loss: 0.0984 - MAE: 0.0984\n",
      "Epoch 44/50\n",
      "1398/1398 [==============================] - 1s 498us/step - loss: 0.0995 - MAE: 0.0995\n",
      "Epoch 45/50\n",
      "1398/1398 [==============================] - 1s 454us/step - loss: 0.0980 - MAE: 0.0980\n",
      "Epoch 46/50\n",
      "1398/1398 [==============================] - 1s 513us/step - loss: 0.0971 - MAE: 0.0971\n",
      "Epoch 47/50\n",
      "1398/1398 [==============================] - 1s 519us/step - loss: 0.0996 - MAE: 0.0996\n",
      "Epoch 48/50\n",
      "1398/1398 [==============================] - 1s 488us/step - loss: 0.0954 - MAE: 0.0954\n",
      "Epoch 49/50\n",
      "1398/1398 [==============================] - 1s 541us/step - loss: 0.0986 - MAE: 0.0986\n",
      "Epoch 50/50\n",
      "1398/1398 [==============================] - 1s 576us/step - loss: 0.0978 - MAE: 0.0978\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (1, 70)                   1050      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (1, 7)                    497       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (1, 1)                    8         \n",
      "=================================================================\n",
      "Total params: 1,681\n",
      "Trainable params: 1,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.08570132260451226\n"
     ]
    }
   ],
   "source": [
    "ModelDenseAdamax4 = Sequential()\n",
    "ModelDenseAdamax4.add(Dense(14))\n",
    "ModelDenseAdamax4.add(Dense(70, activation='relu'))\n",
    "ModelDenseAdamax4.add(Dense(7, activation='elu'))\n",
    "ModelDenseAdamax4.add(Dense(1, activation='linear'))\n",
    "ModelDenseAdamax4.compile(optimizer='Adamax', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseAdamax4.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseAdamax4.summary()\n",
    "\n",
    "DenseAdamaxPrediction4 = ModelDenseAdamax4.predict(x_test)\n",
    "score = mean_absolute_error(DenseAdamaxPrediction4, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_24 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 420us/step - loss: 0.3185 - MAE: 0.3185\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 515us/step - loss: 0.2342 - MAE: 0.2342\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 417us/step - loss: 0.2220 - MAE: 0.2220\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 407us/step - loss: 0.2180 - MAE: 0.2180\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 408us/step - loss: 0.2119 - MAE: 0.2119\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 405us/step - loss: 0.2078 - MAE: 0.2078\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 490us/step - loss: 0.2032 - MAE: 0.2032\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 504us/step - loss: 0.1974 - MAE: 0.1974\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 523us/step - loss: 0.1883 - MAE: 0.1883\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 508us/step - loss: 0.1766 - MAE: 0.1766\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 419us/step - loss: 0.1622 - MAE: 0.1622\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 415us/step - loss: 0.1482 - MAE: 0.1482\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 440us/step - loss: 0.1332 - MAE: 0.1332\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 527us/step - loss: 0.1250 - MAE: 0.1250\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 455us/step - loss: 0.1175 - MAE: 0.1175\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 391us/step - loss: 0.1152 - MAE: 0.1152\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 459us/step - loss: 0.1097 - MAE: 0.1097\n",
      "Epoch 18/50\n",
      "1398/1398 [==============================] - 1s 449us/step - loss: 0.1125 - MAE: 0.1125\n",
      "Epoch 19/50\n",
      "1398/1398 [==============================] - 1s 539us/step - loss: 0.1068 - MAE: 0.1068\n",
      "Epoch 20/50\n",
      "1398/1398 [==============================] - 1s 561us/step - loss: 0.1099 - MAE: 0.1099\n",
      "Epoch 21/50\n",
      "1398/1398 [==============================] - 1s 539us/step - loss: 0.1080 - MAE: 0.1080\n",
      "Epoch 22/50\n",
      "1398/1398 [==============================] - 1s 555us/step - loss: 0.1076 - MAE: 0.1076\n",
      "Epoch 23/50\n",
      "1398/1398 [==============================] - 1s 486us/step - loss: 0.1079 - MAE: 0.1079\n",
      "Epoch 24/50\n",
      "1398/1398 [==============================] - 1s 511us/step - loss: 0.1060 - MAE: 0.1060\n",
      "Epoch 25/50\n",
      "1398/1398 [==============================] - 1s 550us/step - loss: 0.1056 - MAE: 0.1056\n",
      "Epoch 26/50\n",
      "1398/1398 [==============================] - 1s 510us/step - loss: 0.1051 - MAE: 0.1051\n",
      "Epoch 27/50\n",
      "1398/1398 [==============================] - 1s 495us/step - loss: 0.1064 - MAE: 0.1064\n",
      "Epoch 28/50\n",
      "1398/1398 [==============================] - 1s 694us/step - loss: 0.1044 - MAE: 0.1044\n",
      "Epoch 29/50\n",
      "1398/1398 [==============================] - 1s 515us/step - loss: 0.1056 - MAE: 0.1056\n",
      "Epoch 30/50\n",
      "1398/1398 [==============================] - 1s 498us/step - loss: 0.1048 - MAE: 0.1048\n",
      "Epoch 31/50\n",
      "1398/1398 [==============================] - 1s 436us/step - loss: 0.1027 - MAE: 0.1027\n",
      "Epoch 32/50\n",
      "1398/1398 [==============================] - 1s 424us/step - loss: 0.1022 - MAE: 0.1022\n",
      "Epoch 33/50\n",
      "1398/1398 [==============================] - 1s 716us/step - loss: 0.1054 - MAE: 0.1054\n",
      "Epoch 34/50\n",
      "1398/1398 [==============================] - 1s 595us/step - loss: 0.1005 - MAE: 0.1005\n",
      "Epoch 35/50\n",
      "1398/1398 [==============================] - 1s 580us/step - loss: 0.1021 - MAE: 0.1021\n",
      "Epoch 36/50\n",
      "1398/1398 [==============================] - 1s 599us/step - loss: 0.1017 - MAE: 0.1017\n",
      "Epoch 37/50\n",
      "1398/1398 [==============================] - 1s 610us/step - loss: 0.1014 - MAE: 0.1014\n",
      "Epoch 38/50\n",
      "1398/1398 [==============================] - 1s 645us/step - loss: 0.0994 - MAE: 0.0994\n",
      "Epoch 39/50\n",
      "1398/1398 [==============================] - 1s 598us/step - loss: 0.0988 - MAE: 0.0988\n",
      "Epoch 40/50\n",
      "1398/1398 [==============================] - 1s 492us/step - loss: 0.0996 - MAE: 0.0996\n",
      "Epoch 41/50\n",
      "1398/1398 [==============================] - 1s 415us/step - loss: 0.1014 - MAE: 0.1014\n",
      "Epoch 42/50\n",
      "1398/1398 [==============================] - 1s 428us/step - loss: 0.1003 - MAE: 0.1003\n",
      "Epoch 43/50\n",
      "1398/1398 [==============================] - 1s 519us/step - loss: 0.1003 - MAE: 0.1003\n",
      "Epoch 44/50\n",
      "1398/1398 [==============================] - 1s 495us/step - loss: 0.0992 - MAE: 0.0992\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_24 (Dense)             (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (1, 70)                   1050      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (1, 7)                    497       \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (1, 1)                    8         \n",
      "=================================================================\n",
      "Total params: 1,681\n",
      "Trainable params: 1,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.10658072731022152\n"
     ]
    }
   ],
   "source": [
    "ModelDenseAdamax5 = Sequential()\n",
    "ModelDenseAdamax5.add(Dense(14))\n",
    "ModelDenseAdamax5.add(Dense(70, activation='relu'))\n",
    "ModelDenseAdamax5.add(Dense(7, activation='elu'))\n",
    "ModelDenseAdamax5.add(Dense(1, activation='linear'))\n",
    "ModelDenseAdamax5.compile(optimizer='Adamax', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseAdamax5.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseAdamax5.summary()\n",
    "\n",
    "DenseAdamaxPrediction5 = ModelDenseAdamax5.predict(x_test)\n",
    "score = mean_absolute_error(DenseAdamaxPrediction5, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_28 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 409us/step - loss: 0.3188 - MAE: 0.3188\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 409us/step - loss: 0.2516 - MAE: 0.2516\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 434us/step - loss: 0.2449 - MAE: 0.2449\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 600us/step - loss: 0.2391 - MAE: 0.2391\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 536us/step - loss: 0.2306 - MAE: 0.2306\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 618us/step - loss: 0.2274 - MAE: 0.2274\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 396us/step - loss: 0.2277 - MAE: 0.2277\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 497us/step - loss: 0.2211 - MAE: 0.2211\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 485us/step - loss: 0.2165 - MAE: 0.2165\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 444us/step - loss: 0.2167 - MAE: 0.2167\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 429us/step - loss: 0.2118 - MAE: 0.2118\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 430us/step - loss: 0.2094 - MAE: 0.2094\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 496us/step - loss: 0.2025 - MAE: 0.2025\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 507us/step - loss: 0.2012 - MAE: 0.2012\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 601us/step - loss: 0.1946 - MAE: 0.1946\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 578us/step - loss: 0.1885 - MAE: 0.1885\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 399us/step - loss: 0.1805 - MAE: 0.1805\n",
      "Epoch 18/50\n",
      "1398/1398 [==============================] - 1s 466us/step - loss: 0.1719 - MAE: 0.1719\n",
      "Epoch 19/50\n",
      "1398/1398 [==============================] - 1s 480us/step - loss: 0.1638 - MAE: 0.1638\n",
      "Epoch 20/50\n",
      "1398/1398 [==============================] - 1s 514us/step - loss: 0.1548 - MAE: 0.1548\n",
      "Epoch 21/50\n",
      "1398/1398 [==============================] - 1s 420us/step - loss: 0.1487 - MAE: 0.1487\n",
      "Epoch 22/50\n",
      "1398/1398 [==============================] - 1s 632us/step - loss: 0.1445 - MAE: 0.1445\n",
      "Epoch 23/50\n",
      "1398/1398 [==============================] - 1s 711us/step - loss: 0.1391 - MAE: 0.1391\n",
      "Epoch 24/50\n",
      "1398/1398 [==============================] - 1s 602us/step - loss: 0.1372 - MAE: 0.1372\n",
      "Epoch 25/50\n",
      "1398/1398 [==============================] - 1s 448us/step - loss: 0.1325 - MAE: 0.1325\n",
      "Epoch 26/50\n",
      "1398/1398 [==============================] - 1s 532us/step - loss: 0.1327 - MAE: 0.1327\n",
      "Epoch 27/50\n",
      "1398/1398 [==============================] - 1s 431us/step - loss: 0.1274 - MAE: 0.1274\n",
      "Epoch 28/50\n",
      "1398/1398 [==============================] - 1s 491us/step - loss: 0.1282 - MAE: 0.1282\n",
      "Epoch 29/50\n",
      "1398/1398 [==============================] - 1s 522us/step - loss: 0.1265 - MAE: 0.1265\n",
      "Epoch 30/50\n",
      "1398/1398 [==============================] - 1s 541us/step - loss: 0.1243 - MAE: 0.1243\n",
      "Epoch 31/50\n",
      "1398/1398 [==============================] - 1s 429us/step - loss: 0.1240 - MAE: 0.1240\n",
      "Epoch 32/50\n",
      "1398/1398 [==============================] - 1s 567us/step - loss: 0.1209 - MAE: 0.1209\n",
      "Epoch 33/50\n",
      "1398/1398 [==============================] - 1s 484us/step - loss: 0.1231 - MAE: 0.1231\n",
      "Epoch 34/50\n",
      "1398/1398 [==============================] - 1s 462us/step - loss: 0.1205 - MAE: 0.1205\n",
      "Epoch 35/50\n",
      "1398/1398 [==============================] - 1s 599us/step - loss: 0.1216 - MAE: 0.1216\n",
      "Epoch 36/50\n",
      "1398/1398 [==============================] - 1s 560us/step - loss: 0.1222 - MAE: 0.1222\n",
      "Epoch 37/50\n",
      "1398/1398 [==============================] - 1s 422us/step - loss: 0.1227 - MAE: 0.1227\n",
      "Epoch 38/50\n",
      "1398/1398 [==============================] - 1s 451us/step - loss: 0.1216 - MAE: 0.1216\n",
      "Epoch 39/50\n",
      "1398/1398 [==============================] - 1s 488us/step - loss: 0.1194 - MAE: 0.1194\n",
      "Epoch 40/50\n",
      "1398/1398 [==============================] - 1s 548us/step - loss: 0.1150 - MAE: 0.1150\n",
      "Epoch 41/50\n",
      "1398/1398 [==============================] - 1s 520us/step - loss: 0.1184 - MAE: 0.1184\n",
      "Epoch 42/50\n",
      "1398/1398 [==============================] - 1s 525us/step - loss: 0.1193 - MAE: 0.1193\n",
      "Epoch 43/50\n",
      "1398/1398 [==============================] - 1s 468us/step - loss: 0.1150 - MAE: 0.1150\n",
      "Epoch 44/50\n",
      "1398/1398 [==============================] - 1s 457us/step - loss: 0.1147 - MAE: 0.1147\n",
      "Epoch 45/50\n",
      "1398/1398 [==============================] - 1s 484us/step - loss: 0.1173 - MAE: 0.1173\n",
      "Epoch 46/50\n",
      "1398/1398 [==============================] - 1s 543us/step - loss: 0.1159 - MAE: 0.1159\n",
      "Epoch 47/50\n",
      "1398/1398 [==============================] - 1s 414us/step - loss: 0.1149 - MAE: 0.1149\n",
      "Epoch 48/50\n",
      "1398/1398 [==============================] - 1s 510us/step - loss: 0.1153 - MAE: 0.1153\n",
      "Epoch 49/50\n",
      "1398/1398 [==============================] - 1s 516us/step - loss: 0.1130 - MAE: 0.1130\n",
      "Epoch 50/50\n",
      "1398/1398 [==============================] - 1s 538us/step - loss: 0.1163 - MAE: 0.1163\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_28 (Dense)             (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (1, 70)                   1050      \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (1, 7)                    497       \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (1, 1)                    8         \n",
      "=================================================================\n",
      "Total params: 1,681\n",
      "Trainable params: 1,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.09332039216521465\n"
     ]
    }
   ],
   "source": [
    "ModelDenseAdamax6 = Sequential()\n",
    "ModelDenseAdamax6.add(Dense(14))\n",
    "ModelDenseAdamax6.add(Dense(70))\n",
    "ModelDenseAdamax6.add(Dense(7, activation='relu'))\n",
    "ModelDenseAdamax6.add(Dense(1, activation='linear'))\n",
    "ModelDenseAdamax6.compile(optimizer='Adamax', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseAdamax6.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseAdamax6.summary()\n",
    "\n",
    "DenseAdamaxPrediction6 = ModelDenseAdamax6.predict(x_test)\n",
    "score = mean_absolute_error(DenseAdamaxPrediction6, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_32 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 505us/step - loss: 0.4283 - MAE: 0.4283\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 605us/step - loss: 0.2847 - MAE: 0.2847\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 609us/step - loss: 0.2654 - MAE: 0.2654\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 497us/step - loss: 0.2570 - MAE: 0.2570\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 455us/step - loss: 0.2477 - MAE: 0.2477\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 585us/step - loss: 0.2452 - MAE: 0.2452\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 650us/step - loss: 0.2423 - MAE: 0.2423\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 572us/step - loss: 0.2432 - MAE: 0.2432\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 556us/step - loss: 0.2389 - MAE: 0.2389\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 486us/step - loss: 0.2358 - MAE: 0.2358\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 554us/step - loss: 0.2351 - MAE: 0.2351\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 425us/step - loss: 0.2314 - MAE: 0.2314\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 538us/step - loss: 0.2336 - MAE: 0.2336\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 483us/step - loss: 0.2322 - MAE: 0.2322\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 499us/step - loss: 0.2322 - MAE: 0.2322\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 435us/step - loss: 0.2301 - MAE: 0.2301\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 448us/step - loss: 0.2295 - MAE: 0.2295\n",
      "Epoch 18/50\n",
      "1398/1398 [==============================] - 1s 502us/step - loss: 0.2303 - MAE: 0.2303\n",
      "Epoch 19/50\n",
      "1398/1398 [==============================] - 1s 516us/step - loss: 0.2270 - MAE: 0.2270\n",
      "Epoch 20/50\n",
      "1398/1398 [==============================] - 1s 445us/step - loss: 0.2269 - MAE: 0.2269\n",
      "Epoch 21/50\n",
      "1398/1398 [==============================] - 1s 412us/step - loss: 0.2281 - MAE: 0.2281\n",
      "Epoch 22/50\n",
      "1398/1398 [==============================] - 1s 465us/step - loss: 0.2273 - MAE: 0.2273\n",
      "Epoch 23/50\n",
      "1398/1398 [==============================] - 1s 437us/step - loss: 0.2276 - MAE: 0.2276\n",
      "Epoch 24/50\n",
      "1398/1398 [==============================] - 1s 483us/step - loss: 0.2296 - MAE: 0.2296\n",
      "Epoch 25/50\n",
      "1398/1398 [==============================] - 1s 509us/step - loss: 0.2255 - MAE: 0.2255\n",
      "Epoch 26/50\n",
      "1398/1398 [==============================] - 1s 421us/step - loss: 0.2271 - MAE: 0.2271\n",
      "Epoch 27/50\n",
      "1398/1398 [==============================] - 1s 421us/step - loss: 0.2255 - MAE: 0.2255\n",
      "Epoch 28/50\n",
      "1398/1398 [==============================] - 1s 499us/step - loss: 0.2245 - MAE: 0.2245\n",
      "Epoch 29/50\n",
      "1398/1398 [==============================] - 1s 606us/step - loss: 0.2229 - MAE: 0.2229\n",
      "Epoch 30/50\n",
      "1398/1398 [==============================] - 1s 494us/step - loss: 0.2265 - MAE: 0.2265\n",
      "Epoch 31/50\n",
      "1398/1398 [==============================] - 1s 427us/step - loss: 0.2241 - MAE: 0.2241\n",
      "Epoch 32/50\n",
      "1398/1398 [==============================] - 1s 417us/step - loss: 0.2242 - MAE: 0.2242\n",
      "Epoch 33/50\n",
      "1398/1398 [==============================] - 1s 419us/step - loss: 0.2233 - MAE: 0.2233\n",
      "Epoch 34/50\n",
      "1398/1398 [==============================] - 1s 444us/step - loss: 0.2221 - MAE: 0.2221\n",
      "Epoch 35/50\n",
      "1398/1398 [==============================] - 1s 492us/step - loss: 0.2210 - MAE: 0.2210\n",
      "Epoch 36/50\n",
      "1398/1398 [==============================] - 1s 435us/step - loss: 0.2246 - MAE: 0.2246\n",
      "Epoch 37/50\n",
      "1398/1398 [==============================] - 1s 471us/step - loss: 0.2201 - MAE: 0.2201\n",
      "Epoch 38/50\n",
      "1398/1398 [==============================] - 1s 486us/step - loss: 0.2200 - MAE: 0.2200 0s - loss: 0.2229 - MAE: 0.2\n",
      "Epoch 39/50\n",
      "1398/1398 [==============================] - 1s 557us/step - loss: 0.2211 - MAE: 0.2211\n",
      "Epoch 40/50\n",
      "1398/1398 [==============================] - 1s 429us/step - loss: 0.2220 - MAE: 0.2220\n",
      "Epoch 41/50\n",
      "1398/1398 [==============================] - 1s 519us/step - loss: 0.2208 - MAE: 0.2208\n",
      "Epoch 42/50\n",
      "1398/1398 [==============================] - 1s 442us/step - loss: 0.2225 - MAE: 0.2225\n",
      "Epoch 43/50\n",
      "1398/1398 [==============================] - 1s 432us/step - loss: 0.2194 - MAE: 0.2194\n",
      "Epoch 44/50\n",
      "1398/1398 [==============================] - 1s 493us/step - loss: 0.2143 - MAE: 0.2143\n",
      "Epoch 45/50\n",
      "1398/1398 [==============================] - 1s 538us/step - loss: 0.2094 - MAE: 0.2094\n",
      "Epoch 46/50\n",
      "1398/1398 [==============================] - 1s 606us/step - loss: 0.2065 - MAE: 0.2065\n",
      "Epoch 47/50\n",
      "1398/1398 [==============================] - 1s 440us/step - loss: 0.2034 - MAE: 0.2034\n",
      "Epoch 48/50\n",
      "1398/1398 [==============================] - 1s 454us/step - loss: 0.1978 - MAE: 0.1978\n",
      "Epoch 49/50\n",
      "1398/1398 [==============================] - 1s 427us/step - loss: 0.1888 - MAE: 0.1888\n",
      "Epoch 50/50\n",
      "1398/1398 [==============================] - 1s 506us/step - loss: 0.1791 - MAE: 0.1791\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_32 (Dense)             (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (1, 14)                   0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (1, 70)                   1050      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (1, 70)                   0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (1, 7)                    497       \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (1, 1)                    8         \n",
      "=================================================================\n",
      "Total params: 1,681\n",
      "Trainable params: 1,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.13460581399141594\n"
     ]
    }
   ],
   "source": [
    "ModelDenseAdamax6_1 = Sequential()\n",
    "ModelDenseAdamax6_1.add(Dense(14))\n",
    "ModelDenseAdamax6_1.add(Dropout(0.1))\n",
    "ModelDenseAdamax6_1.add(Dense(70))\n",
    "ModelDenseAdamax6_1.add(Dropout(0.1))\n",
    "ModelDenseAdamax6_1.add(Dense(7, activation='relu'))\n",
    "ModelDenseAdamax6_1.add(Dense(1, activation='linear'))\n",
    "ModelDenseAdamax6_1.compile(optimizer='Adamax', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseAdamax6_1.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseAdamax6_1.summary()\n",
    "\n",
    "DenseAdamaxPrediction6_1 = ModelDenseAdamax6_1.predict(x_test)\n",
    "score = mean_absolute_error(DenseAdamaxPrediction6_1, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_36 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 538us/step - loss: 0.3006 - MAE: 0.3006\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 531us/step - loss: 0.2236 - MAE: 0.2236\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 495us/step - loss: 0.2169 - MAE: 0.2169\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 508us/step - loss: 0.2099 - MAE: 0.2099\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 526us/step - loss: 0.1960 - MAE: 0.1960\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 551us/step - loss: 0.1690 - MAE: 0.1690\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 538us/step - loss: 0.1455 - MAE: 0.1455\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 596us/step - loss: 0.1278 - MAE: 0.1278\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 512us/step - loss: 0.1153 - MAE: 0.1153\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 513us/step - loss: 0.1107 - MAE: 0.1107\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 449us/step - loss: 0.1056 - MAE: 0.1056\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 522us/step - loss: 0.1088 - MAE: 0.1088\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 527us/step - loss: 0.1057 - MAE: 0.1057\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 538us/step - loss: 0.1037 - MAE: 0.1037\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 584us/step - loss: 0.1038 - MAE: 0.1038\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 588us/step - loss: 0.1021 - MAE: 0.1021\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 501us/step - loss: 0.1043 - MAE: 0.1043\n",
      "Epoch 18/50\n",
      "1398/1398 [==============================] - 1s 556us/step - loss: 0.1002 - MAE: 0.1002\n",
      "Epoch 19/50\n",
      "1398/1398 [==============================] - 1s 583us/step - loss: 0.0993 - MAE: 0.0993\n",
      "Epoch 20/50\n",
      "1398/1398 [==============================] - 1s 508us/step - loss: 0.1011 - MAE: 0.1011\n",
      "Epoch 21/50\n",
      "1398/1398 [==============================] - 1s 573us/step - loss: 0.0971 - MAE: 0.0971\n",
      "Epoch 22/50\n",
      "1398/1398 [==============================] - 1s 600us/step - loss: 0.0971 - MAE: 0.0971\n",
      "Epoch 23/50\n",
      "1398/1398 [==============================] - 1s 668us/step - loss: 0.0944 - MAE: 0.0944\n",
      "Epoch 24/50\n",
      "1398/1398 [==============================] - 1s 458us/step - loss: 0.0945 - MAE: 0.0945\n",
      "Epoch 25/50\n",
      "1398/1398 [==============================] - 1s 556us/step - loss: 0.0923 - MAE: 0.0923\n",
      "Epoch 26/50\n",
      "1398/1398 [==============================] - 1s 455us/step - loss: 0.0928 - MAE: 0.0928\n",
      "Epoch 27/50\n",
      "1398/1398 [==============================] - 1s 605us/step - loss: 0.0907 - MAE: 0.0907\n",
      "Epoch 28/50\n",
      "1398/1398 [==============================] - 1s 483us/step - loss: 0.0930 - MAE: 0.0930\n",
      "Epoch 29/50\n",
      "1398/1398 [==============================] - 1s 493us/step - loss: 0.0879 - MAE: 0.0879\n",
      "Epoch 30/50\n",
      "1398/1398 [==============================] - 1s 440us/step - loss: 0.0882 - MAE: 0.0882\n",
      "Epoch 31/50\n",
      "1398/1398 [==============================] - 1s 559us/step - loss: 0.0886 - MAE: 0.0886\n",
      "Epoch 32/50\n",
      "1398/1398 [==============================] - 1s 525us/step - loss: 0.0901 - MAE: 0.0901\n",
      "Epoch 33/50\n",
      "1398/1398 [==============================] - 1s 501us/step - loss: 0.0858 - MAE: 0.0858\n",
      "Epoch 34/50\n",
      "1398/1398 [==============================] - 1s 581us/step - loss: 0.0859 - MAE: 0.0859\n",
      "Epoch 35/50\n",
      "1398/1398 [==============================] - 1s 540us/step - loss: 0.0861 - MAE: 0.0861\n",
      "Epoch 36/50\n",
      "1398/1398 [==============================] - 1s 519us/step - loss: 0.0846 - MAE: 0.0846\n",
      "Epoch 37/50\n",
      "1398/1398 [==============================] - 1s 489us/step - loss: 0.0868 - MAE: 0.0868\n",
      "Epoch 38/50\n",
      "1398/1398 [==============================] - 1s 644us/step - loss: 0.0838 - MAE: 0.0838\n",
      "Epoch 39/50\n",
      "1398/1398 [==============================] - 1s 491us/step - loss: 0.0785 - MAE: 0.0785\n",
      "Epoch 40/50\n",
      "1398/1398 [==============================] - 1s 457us/step - loss: 0.0797 - MAE: 0.0797\n",
      "Epoch 41/50\n",
      "1398/1398 [==============================] - 1s 489us/step - loss: 0.0799 - MAE: 0.0799\n",
      "Epoch 42/50\n",
      "1398/1398 [==============================] - 1s 529us/step - loss: 0.0804 - MAE: 0.0804\n",
      "Epoch 43/50\n",
      "1398/1398 [==============================] - 1s 577us/step - loss: 0.0769 - MAE: 0.0769\n",
      "Epoch 44/50\n",
      "1398/1398 [==============================] - 1s 481us/step - loss: 0.0774 - MAE: 0.0774\n",
      "Epoch 45/50\n",
      "1398/1398 [==============================] - 1s 696us/step - loss: 0.0764 - MAE: 0.0764\n",
      "Epoch 46/50\n",
      "1398/1398 [==============================] - 1s 514us/step - loss: 0.0762 - MAE: 0.0762\n",
      "Epoch 47/50\n",
      "1398/1398 [==============================] - 1s 460us/step - loss: 0.0767 - MAE: 0.0767\n",
      "Epoch 48/50\n",
      "1398/1398 [==============================] - 1s 465us/step - loss: 0.0750 - MAE: 0.0750\n",
      "Epoch 49/50\n",
      "1398/1398 [==============================] - 1s 557us/step - loss: 0.0750 - MAE: 0.0750\n",
      "Epoch 50/50\n",
      "1398/1398 [==============================] - 1s 451us/step - loss: 0.0706 - MAE: 0.0706\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_36 (Dense)             (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (1, 192)                  2880      \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (1, 24)                   4632      \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (1, 9)                    225       \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (1, 1)                    10        \n",
      "=================================================================\n",
      "Total params: 7,873\n",
      "Trainable params: 7,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.07043514246836464\n"
     ]
    }
   ],
   "source": [
    "ModelDenseAdamax7 = Sequential()\n",
    "ModelDenseAdamax7.add(Dense(14))\n",
    "ModelDenseAdamax7.add(Dense(192, activation='relu'))\n",
    "ModelDenseAdamax7.add(Dense(24, activation='relu'))\n",
    "ModelDenseAdamax7.add(Dense(9, activation='relu'))\n",
    "ModelDenseAdamax7.add(Dense(1, activation='linear'))\n",
    "ModelDenseAdamax7.compile(optimizer='Adamax', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseAdamax7.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseAdamax7.summary()\n",
    "\n",
    "DenseAdamaxPrediction7 = ModelDenseAdamax7.predict(x_test)\n",
    "score = mean_absolute_error(DenseAdamaxPrediction7, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense RMSprop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1398/1398 [==============================] - 1s 412us/step - loss: 0.3237 - MAE: 0.3237\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 535us/step - loss: 0.2399 - MAE: 0.2399\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 423us/step - loss: 0.2299 - MAE: 0.2299\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 406us/step - loss: 0.2193 - MAE: 0.2193\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 420us/step - loss: 0.2065 - MAE: 0.2065\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 424us/step - loss: 0.1866 - MAE: 0.1866\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 400us/step - loss: 0.1626 - MAE: 0.1626\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 441us/step - loss: 0.1430 - MAE: 0.1430\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 415us/step - loss: 0.1286 - MAE: 0.1286\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 425us/step - loss: 0.1221 - MAE: 0.1221\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 424us/step - loss: 0.1191 - MAE: 0.1191\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 400us/step - loss: 0.1196 - MAE: 0.1196\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 396us/step - loss: 0.1175 - MAE: 0.1175\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 402us/step - loss: 0.1184 - MAE: 0.1184\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 411us/step - loss: 0.1128 - MAE: 0.1128\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 414us/step - loss: 0.1149 - MAE: 0.1149\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 415us/step - loss: 0.1128 - MAE: 0.1128\n",
      "Epoch 18/50\n",
      "1398/1398 [==============================] - 1s 405us/step - loss: 0.1119 - MAE: 0.1119\n",
      "Epoch 19/50\n",
      "1398/1398 [==============================] - 1s 421us/step - loss: 0.1121 - MAE: 0.1121\n",
      "Epoch 20/50\n",
      "1398/1398 [==============================] - 1s 397us/step - loss: 0.1129 - MAE: 0.1129\n",
      "Epoch 21/50\n",
      "1398/1398 [==============================] - 1s 403us/step - loss: 0.1099 - MAE: 0.1099\n",
      "Epoch 22/50\n",
      "1398/1398 [==============================] - 1s 391us/step - loss: 0.1120 - MAE: 0.1120\n",
      "Epoch 23/50\n",
      "1398/1398 [==============================] - 1s 443us/step - loss: 0.1101 - MAE: 0.1101\n",
      "Epoch 24/50\n",
      "1398/1398 [==============================] - 1s 413us/step - loss: 0.1103 - MAE: 0.1103\n",
      "Epoch 25/50\n",
      "1398/1398 [==============================] - 1s 392us/step - loss: 0.1115 - MAE: 0.1115\n",
      "Epoch 26/50\n",
      "1398/1398 [==============================] - 1s 411us/step - loss: 0.1122 - MAE: 0.1122\n",
      "WARNING:tensorflow:Layer dense_41 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Test score: 0.09369357064518734\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ModelDenseRMSprop1 = Sequential()\n",
    "ModelDenseRMSprop1.add(Dense(14))\n",
    "ModelDenseRMSprop1.add(Dense(70, activation='relu'))\n",
    "ModelDenseRMSprop1.add(Dense(7, activation='relu'))\n",
    "ModelDenseRMSprop1.add(Dense(1, activation='linear'))\n",
    "ModelDenseRMSprop1.compile(optimizer='RMSprop', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseRMSprop1.fit(x_train.values, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "\n",
    "DenseRMSpropPrediction1 = ModelDenseRMSprop1.predict(x_test)\n",
    "score = mean_absolute_error(DenseRMSpropPrediction1, y_test)\n",
    "print('Test score:', score)\n",
    "scores.append(score)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1398/1398 [==============================] - 1s 409us/step - loss: 0.3732 - MAE: 0.3732\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 409us/step - loss: 0.2659 - MAE: 0.2659\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 404us/step - loss: 0.2471 - MAE: 0.2471\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 415us/step - loss: 0.2441 - MAE: 0.2441\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 421us/step - loss: 0.2379 - MAE: 0.2379\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 416us/step - loss: 0.2317 - MAE: 0.2317\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 423us/step - loss: 0.2233 - MAE: 0.2233\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 420us/step - loss: 0.2145 - MAE: 0.2145\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 494us/step - loss: 0.1893 - MAE: 0.1893\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 445us/step - loss: 0.1720 - MAE: 0.1720\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 427us/step - loss: 0.1559 - MAE: 0.1559\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 425us/step - loss: 0.1476 - MAE: 0.1476\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 424us/step - loss: 0.1419 - MAE: 0.1419\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 423us/step - loss: 0.1441 - MAE: 0.1441\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 420us/step - loss: 0.1440 - MAE: 0.1440\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 410us/step - loss: 0.1399 - MAE: 0.1399\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 425us/step - loss: 0.1366 - MAE: 0.1366\n",
      "Epoch 18/50\n",
      "1398/1398 [==============================] - 1s 409us/step - loss: 0.1376 - MAE: 0.1376\n",
      "Epoch 19/50\n",
      "1398/1398 [==============================] - 1s 421us/step - loss: 0.1367 - MAE: 0.1367\n",
      "Epoch 20/50\n",
      "1398/1398 [==============================] - 1s 421us/step - loss: 0.1354 - MAE: 0.1354\n",
      "Epoch 21/50\n",
      "1398/1398 [==============================] - 1s 410us/step - loss: 0.1302 - MAE: 0.1302\n",
      "Epoch 22/50\n",
      "1398/1398 [==============================] - 1s 418us/step - loss: 0.1299 - MAE: 0.1299\n",
      "Epoch 23/50\n",
      "1398/1398 [==============================] - 1s 423us/step - loss: 0.1333 - MAE: 0.1333\n",
      "Epoch 24/50\n",
      "1398/1398 [==============================] - 1s 450us/step - loss: 0.1306 - MAE: 0.1306\n",
      "Epoch 25/50\n",
      "1398/1398 [==============================] - 1s 420us/step - loss: 0.1298 - MAE: 0.1298\n",
      "Epoch 26/50\n",
      "1398/1398 [==============================] - 1s 423us/step - loss: 0.1278 - MAE: 0.1278\n",
      "Epoch 27/50\n",
      "1398/1398 [==============================] - 1s 412us/step - loss: 0.1274 - MAE: 0.1274\n",
      "Epoch 28/50\n",
      "1398/1398 [==============================] - 1s 413us/step - loss: 0.1292 - MAE: 0.1292\n",
      "Epoch 29/50\n",
      "1398/1398 [==============================] - 1s 418us/step - loss: 0.1305 - MAE: 0.1305\n",
      "Epoch 30/50\n",
      "1398/1398 [==============================] - 1s 425us/step - loss: 0.1268 - MAE: 0.1268\n",
      "Epoch 31/50\n",
      "1398/1398 [==============================] - 1s 419us/step - loss: 0.1287 - MAE: 0.1287\n",
      "Epoch 32/50\n",
      "1398/1398 [==============================] - 1s 422us/step - loss: 0.1295 - MAE: 0.1295\n",
      "Epoch 33/50\n",
      "1398/1398 [==============================] - 1s 415us/step - loss: 0.1235 - MAE: 0.1235\n",
      "Epoch 34/50\n",
      "1398/1398 [==============================] - 1s 422us/step - loss: 0.1257 - MAE: 0.1257\n",
      "Epoch 35/50\n",
      "1398/1398 [==============================] - 1s 418us/step - loss: 0.1274 - MAE: 0.1274\n",
      "Epoch 36/50\n",
      "1398/1398 [==============================] - 1s 428us/step - loss: 0.1294 - MAE: 0.1294\n",
      "Epoch 37/50\n",
      "1398/1398 [==============================] - 1s 430us/step - loss: 0.1232 - MAE: 0.1232\n",
      "Epoch 38/50\n",
      "1398/1398 [==============================] - 1s 413us/step - loss: 0.1264 - MAE: 0.1264\n",
      "Epoch 39/50\n",
      "1398/1398 [==============================] - 1s 431us/step - loss: 0.1249 - MAE: 0.1249\n",
      "Epoch 40/50\n",
      "1398/1398 [==============================] - 1s 412us/step - loss: 0.1242 - MAE: 0.1242\n",
      "Epoch 41/50\n",
      "1398/1398 [==============================] - 1s 435us/step - loss: 0.1241 - MAE: 0.1241\n",
      "Epoch 42/50\n",
      "1398/1398 [==============================] - 1s 416us/step - loss: 0.1291 - MAE: 0.1291\n",
      "WARNING:tensorflow:Layer dense_45 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Test score: 0.09922150399487842\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ModelDenseRMSprop1_1 = Sequential()\n",
    "ModelDenseRMSprop1_1.add(Dense(14))\n",
    "ModelDenseRMSprop1_1.add(Dropout(0.1))\n",
    "ModelDenseRMSprop1_1.add(Dense(70, activation='relu'))\n",
    "ModelDenseRMSprop1_1.add(Dropout(0.1))\n",
    "ModelDenseRMSprop1_1.add(Dense(7, activation='relu'))\n",
    "ModelDenseRMSprop1_1.add(Dense(1, activation='linear'))\n",
    "ModelDenseRMSprop1_1.compile(optimizer='RMSprop', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseRMSprop1_1.fit(x_train.values, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "\n",
    "DenseRMSpropPrediction1_1 = ModelDenseRMSprop1_1.predict(x_test)\n",
    "score = mean_absolute_error(DenseRMSpropPrediction1_1, y_test)\n",
    "print('Test score:', score)\n",
    "scores.append(score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_49 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 401us/step - loss: 0.3110 - MAE: 0.3110\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 412us/step - loss: 0.2356 - MAE: 0.2356\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 444us/step - loss: 0.2162 - MAE: 0.2162\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 416us/step - loss: 0.1955 - MAE: 0.1955\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 416us/step - loss: 0.1671 - MAE: 0.1671\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 405us/step - loss: 0.1453 - MAE: 0.1453\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 404us/step - loss: 0.1347 - MAE: 0.1347\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 419us/step - loss: 0.1296 - MAE: 0.1296\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 388us/step - loss: 0.1300 - MAE: 0.1300\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 407us/step - loss: 0.1263 - MAE: 0.1263\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 398us/step - loss: 0.1251 - MAE: 0.1251\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 419us/step - loss: 0.1249 - MAE: 0.1249\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 413us/step - loss: 0.1204 - MAE: 0.1204 0s - loss: 0.1231 - MAE: \n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 408us/step - loss: 0.1256 - MAE: 0.1256\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 396us/step - loss: 0.1222 - MAE: 0.1222\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 414us/step - loss: 0.1196 - MAE: 0.1196\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 424us/step - loss: 0.1181 - MAE: 0.1181\n",
      "Epoch 18/50\n",
      "1398/1398 [==============================] - 1s 410us/step - loss: 0.1158 - MAE: 0.1158\n",
      "Epoch 19/50\n",
      "1398/1398 [==============================] - 1s 424us/step - loss: 0.1167 - MAE: 0.1167\n",
      "Epoch 20/50\n",
      "1398/1398 [==============================] - 1s 408us/step - loss: 0.1153 - MAE: 0.1153\n",
      "Epoch 21/50\n",
      "1398/1398 [==============================] - 1s 412us/step - loss: 0.1150 - MAE: 0.1150\n",
      "Epoch 22/50\n",
      "1398/1398 [==============================] - 1s 419us/step - loss: 0.1144 - MAE: 0.1144\n",
      "Epoch 23/50\n",
      "1398/1398 [==============================] - 1s 416us/step - loss: 0.1111 - MAE: 0.1111\n",
      "Epoch 24/50\n",
      "1398/1398 [==============================] - 1s 413us/step - loss: 0.1112 - MAE: 0.1112\n",
      "Epoch 25/50\n",
      "1398/1398 [==============================] - 1s 405us/step - loss: 0.1099 - MAE: 0.1099\n",
      "Epoch 26/50\n",
      "1398/1398 [==============================] - 1s 432us/step - loss: 0.1128 - MAE: 0.1128\n",
      "Epoch 27/50\n",
      "1398/1398 [==============================] - 1s 427us/step - loss: 0.1107 - MAE: 0.1107\n",
      "Epoch 28/50\n",
      "1398/1398 [==============================] - 1s 422us/step - loss: 0.1088 - MAE: 0.1088\n",
      "Epoch 29/50\n",
      "1398/1398 [==============================] - 1s 400us/step - loss: 0.1061 - MAE: 0.1061\n",
      "Epoch 30/50\n",
      "1398/1398 [==============================] - 1s 422us/step - loss: 0.1057 - MAE: 0.1057\n",
      "Epoch 31/50\n",
      "1398/1398 [==============================] - 1s 407us/step - loss: 0.1073 - MAE: 0.1073\n",
      "Epoch 32/50\n",
      "1398/1398 [==============================] - 1s 397us/step - loss: 0.1075 - MAE: 0.1075\n",
      "Epoch 33/50\n",
      "1398/1398 [==============================] - 1s 417us/step - loss: 0.1077 - MAE: 0.1077\n",
      "Epoch 34/50\n",
      "1398/1398 [==============================] - 1s 401us/step - loss: 0.1040 - MAE: 0.1040 0s - loss: 0.1085 - MAE: 0.\n",
      "Epoch 35/50\n",
      "1398/1398 [==============================] - 1s 416us/step - loss: 0.1051 - MAE: 0.1051\n",
      "Epoch 36/50\n",
      "1398/1398 [==============================] - 1s 408us/step - loss: 0.1037 - MAE: 0.1037\n",
      "Epoch 37/50\n",
      "1398/1398 [==============================] - 1s 409us/step - loss: 0.1037 - MAE: 0.1037\n",
      "Epoch 38/50\n",
      "1398/1398 [==============================] - 1s 420us/step - loss: 0.1040 - MAE: 0.1040\n",
      "Epoch 39/50\n",
      "1398/1398 [==============================] - 1s 427us/step - loss: 0.1004 - MAE: 0.1004\n",
      "Epoch 40/50\n",
      "1398/1398 [==============================] - 1s 409us/step - loss: 0.0975 - MAE: 0.0975\n",
      "Epoch 41/50\n",
      "1398/1398 [==============================] - 1s 405us/step - loss: 0.0953 - MAE: 0.0953\n",
      "Epoch 42/50\n",
      "1398/1398 [==============================] - 1s 421us/step - loss: 0.1007 - MAE: 0.1007\n",
      "Epoch 43/50\n",
      "1398/1398 [==============================] - 1s 408us/step - loss: 0.0972 - MAE: 0.0972\n",
      "Epoch 44/50\n",
      "1398/1398 [==============================] - 1s 407us/step - loss: 0.0960 - MAE: 0.0960\n",
      "Epoch 45/50\n",
      "1398/1398 [==============================] - 1s 395us/step - loss: 0.1011 - MAE: 0.1011\n",
      "Epoch 46/50\n",
      "1398/1398 [==============================] - 1s 413us/step - loss: 0.0956 - MAE: 0.0956\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_49 (Dense)             (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (1, 70)                   1050      \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (1, 7)                    497       \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (1, 1)                    8         \n",
      "=================================================================\n",
      "Total params: 1,681\n",
      "Trainable params: 1,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.09288598429424606\n"
     ]
    }
   ],
   "source": [
    "ModelDenseRMSprop2 = Sequential()\n",
    "ModelDenseRMSprop2.add(Dense(14))\n",
    "ModelDenseRMSprop2.add(Dense(70, activation='relu'))\n",
    "ModelDenseRMSprop2.add(Dense(7, activation='elu'))\n",
    "ModelDenseRMSprop2.add(Dense(1, activation='linear'))\n",
    "ModelDenseRMSprop2.compile(optimizer='RMSprop', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseRMSprop2.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseRMSprop2.summary()\n",
    "\n",
    "DenseRMSpropPrediction2 = ModelDenseRMSprop2.predict(x_test)\n",
    "score = mean_absolute_error(DenseRMSpropPrediction2, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_53 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 423us/step - loss: 0.3465 - MAE: 0.3465\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 438us/step - loss: 0.2519 - MAE: 0.2519\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 411us/step - loss: 0.2405 - MAE: 0.2405\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 441us/step - loss: 0.2299 - MAE: 0.2299\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 442us/step - loss: 0.2110 - MAE: 0.2110\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 418us/step - loss: 0.1833 - MAE: 0.1833\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 416us/step - loss: 0.1701 - MAE: 0.1701\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 412us/step - loss: 0.1628 - MAE: 0.1628\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 421us/step - loss: 0.1549 - MAE: 0.1549\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 422us/step - loss: 0.1454 - MAE: 0.1454\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 421us/step - loss: 0.1387 - MAE: 0.1387\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 419us/step - loss: 0.1421 - MAE: 0.1421\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 445us/step - loss: 0.1390 - MAE: 0.1390\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 428us/step - loss: 0.1405 - MAE: 0.1405 0s - loss: 0.1414 - MAE:\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 430us/step - loss: 0.1381 - MAE: 0.1381\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 432us/step - loss: 0.1318 - MAE: 0.1318\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 436us/step - loss: 0.1346 - MAE: 0.1346\n",
      "Epoch 18/50\n",
      "1398/1398 [==============================] - 1s 429us/step - loss: 0.1339 - MAE: 0.1339\n",
      "Epoch 19/50\n",
      "1398/1398 [==============================] - 1s 437us/step - loss: 0.1315 - MAE: 0.1315\n",
      "Epoch 20/50\n",
      "1398/1398 [==============================] - 1s 414us/step - loss: 0.1342 - MAE: 0.1342\n",
      "Epoch 21/50\n",
      "1398/1398 [==============================] - 1s 423us/step - loss: 0.1331 - MAE: 0.1331\n",
      "Epoch 22/50\n",
      "1398/1398 [==============================] - 1s 430us/step - loss: 0.1317 - MAE: 0.1317\n",
      "Epoch 23/50\n",
      "1398/1398 [==============================] - 1s 446us/step - loss: 0.1317 - MAE: 0.1317\n",
      "Epoch 24/50\n",
      "1398/1398 [==============================] - 1s 418us/step - loss: 0.1291 - MAE: 0.1291\n",
      "Epoch 25/50\n",
      "1398/1398 [==============================] - 1s 424us/step - loss: 0.1291 - MAE: 0.1291\n",
      "Epoch 26/50\n",
      "1398/1398 [==============================] - 1s 438us/step - loss: 0.1274 - MAE: 0.1274\n",
      "Epoch 27/50\n",
      "1398/1398 [==============================] - 1s 419us/step - loss: 0.1282 - MAE: 0.1282\n",
      "Epoch 28/50\n",
      "1398/1398 [==============================] - 1s 422us/step - loss: 0.1287 - MAE: 0.1287\n",
      "Epoch 29/50\n",
      "1398/1398 [==============================] - 1s 428us/step - loss: 0.1252 - MAE: 0.1252\n",
      "Epoch 30/50\n",
      "1398/1398 [==============================] - 1s 413us/step - loss: 0.1189 - MAE: 0.1189\n",
      "Epoch 31/50\n",
      "1398/1398 [==============================] - 1s 409us/step - loss: 0.1238 - MAE: 0.1238\n",
      "Epoch 32/50\n",
      "1398/1398 [==============================] - 1s 422us/step - loss: 0.1325 - MAE: 0.1325\n",
      "Epoch 33/50\n",
      "1398/1398 [==============================] - 1s 424us/step - loss: 0.1183 - MAE: 0.1183\n",
      "Epoch 34/50\n",
      "1398/1398 [==============================] - 1s 422us/step - loss: 0.1218 - MAE: 0.1218\n",
      "Epoch 35/50\n",
      "1398/1398 [==============================] - 1s 420us/step - loss: 0.1225 - MAE: 0.1225\n",
      "Epoch 36/50\n",
      "1398/1398 [==============================] - 1s 416us/step - loss: 0.1219 - MAE: 0.1219\n",
      "Epoch 37/50\n",
      "1398/1398 [==============================] - 1s 434us/step - loss: 0.1207 - MAE: 0.1207\n",
      "Epoch 38/50\n",
      "1398/1398 [==============================] - 1s 409us/step - loss: 0.1225 - MAE: 0.1225\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_53 (Dense)             (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (1, 14)                   0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (1, 70)                   1050      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (1, 70)                   0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (1, 7)                    497       \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (1, 1)                    8         \n",
      "=================================================================\n",
      "Total params: 1,681\n",
      "Trainable params: 1,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.09009214199311218\n"
     ]
    }
   ],
   "source": [
    "ModelDenseRMSprop2_1 = Sequential()\n",
    "ModelDenseRMSprop2_1.add(Dense(14))\n",
    "ModelDenseRMSprop2_1.add(Dropout(0.1))\n",
    "ModelDenseRMSprop2_1.add(Dense(70, activation='relu'))\n",
    "ModelDenseRMSprop2_1.add(Dropout(0.1))\n",
    "ModelDenseRMSprop2_1.add(Dense(7, activation='elu'))\n",
    "ModelDenseRMSprop2_1.add(Dense(1, activation='linear'))\n",
    "ModelDenseRMSprop2_1.compile(optimizer='RMSprop', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseRMSprop2_1.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseRMSprop2_1.summary()\n",
    "\n",
    "DenseRMSpropPrediction2_1 = ModelDenseRMSprop2_1.predict(x_test)\n",
    "score = mean_absolute_error(DenseRMSpropPrediction2_1, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_57 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 413us/step - loss: 0.2980 - MAE: 0.2980\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 407us/step - loss: 0.2452 - MAE: 0.2452\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 397us/step - loss: 0.2289 - MAE: 0.2289\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 399us/step - loss: 0.2088 - MAE: 0.2088\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 415us/step - loss: 0.1707 - MAE: 0.1707\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 408us/step - loss: 0.1448 - MAE: 0.1448\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 412us/step - loss: 0.1333 - MAE: 0.1333\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 402us/step - loss: 0.1302 - MAE: 0.1302\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 405us/step - loss: 0.1255 - MAE: 0.1255\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 421us/step - loss: 0.1225 - MAE: 0.1225\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 413us/step - loss: 0.1248 - MAE: 0.1248\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 401us/step - loss: 0.1255 - MAE: 0.1255\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 411us/step - loss: 0.1177 - MAE: 0.1177\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 416us/step - loss: 0.1207 - MAE: 0.1207\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 406us/step - loss: 0.1182 - MAE: 0.1182\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 417us/step - loss: 0.1180 - MAE: 0.1180\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 424us/step - loss: 0.1185 - MAE: 0.1185\n",
      "Epoch 18/50\n",
      "1398/1398 [==============================] - 1s 417us/step - loss: 0.1122 - MAE: 0.1122\n",
      "Epoch 19/50\n",
      "1398/1398 [==============================] - 1s 425us/step - loss: 0.1154 - MAE: 0.1154\n",
      "Epoch 20/50\n",
      "1398/1398 [==============================] - 1s 408us/step - loss: 0.1161 - MAE: 0.1161\n",
      "Epoch 21/50\n",
      "1398/1398 [==============================] - 1s 419us/step - loss: 0.1136 - MAE: 0.1136 0s - loss: 0.1128 - MAE:\n",
      "Epoch 22/50\n",
      "1398/1398 [==============================] - 1s 466us/step - loss: 0.1116 - MAE: 0.1116\n",
      "Epoch 23/50\n",
      "1398/1398 [==============================] - 1s 422us/step - loss: 0.1094 - MAE: 0.1094\n",
      "Epoch 24/50\n",
      "1398/1398 [==============================] - 1s 403us/step - loss: 0.1092 - MAE: 0.1092\n",
      "Epoch 25/50\n",
      "1398/1398 [==============================] - 1s 417us/step - loss: 0.1090 - MAE: 0.1090\n",
      "Epoch 26/50\n",
      "1398/1398 [==============================] - 1s 404us/step - loss: 0.1078 - MAE: 0.1078\n",
      "Epoch 27/50\n",
      "1398/1398 [==============================] - 1s 419us/step - loss: 0.1086 - MAE: 0.1086\n",
      "Epoch 28/50\n",
      "1398/1398 [==============================] - 1s 408us/step - loss: 0.1065 - MAE: 0.1065\n",
      "Epoch 29/50\n",
      "1398/1398 [==============================] - 1s 394us/step - loss: 0.1066 - MAE: 0.1066\n",
      "Epoch 30/50\n",
      "1398/1398 [==============================] - 1s 415us/step - loss: 0.1073 - MAE: 0.1073 0s - loss: 0.1307 - \n",
      "Epoch 31/50\n",
      "1398/1398 [==============================] - 1s 419us/step - loss: 0.1066 - MAE: 0.1066\n",
      "Epoch 32/50\n",
      "1398/1398 [==============================] - 1s 402us/step - loss: 0.1073 - MAE: 0.1073\n",
      "Epoch 33/50\n",
      "1398/1398 [==============================] - 1s 419us/step - loss: 0.1048 - MAE: 0.1048\n",
      "Epoch 34/50\n",
      "1398/1398 [==============================] - 1s 418us/step - loss: 0.1041 - MAE: 0.1041\n",
      "Epoch 35/50\n",
      "1398/1398 [==============================] - 1s 423us/step - loss: 0.1028 - MAE: 0.1028\n",
      "Epoch 36/50\n",
      "1398/1398 [==============================] - 1s 428us/step - loss: 0.1038 - MAE: 0.1038\n",
      "Epoch 37/50\n",
      "1398/1398 [==============================] - 1s 431us/step - loss: 0.1024 - MAE: 0.1024\n",
      "Epoch 38/50\n",
      "1398/1398 [==============================] - 1s 404us/step - loss: 0.1037 - MAE: 0.1037\n",
      "Epoch 39/50\n",
      "1398/1398 [==============================] - 1s 410us/step - loss: 0.1013 - MAE: 0.1013\n",
      "Epoch 40/50\n",
      "1398/1398 [==============================] - 1s 395us/step - loss: 0.1034 - MAE: 0.1034\n",
      "Epoch 41/50\n",
      "1398/1398 [==============================] - 1s 419us/step - loss: 0.1015 - MAE: 0.1015\n",
      "Epoch 42/50\n",
      "1398/1398 [==============================] - 1s 410us/step - loss: 0.0979 - MAE: 0.0979\n",
      "Epoch 43/50\n",
      "1398/1398 [==============================] - 1s 415us/step - loss: 0.1026 - MAE: 0.1026\n",
      "Epoch 44/50\n",
      "1398/1398 [==============================] - 1s 401us/step - loss: 0.0976 - MAE: 0.0976\n",
      "Epoch 45/50\n",
      "1398/1398 [==============================] - 1s 412us/step - loss: 0.0981 - MAE: 0.0981\n",
      "Epoch 46/50\n",
      "1398/1398 [==============================] - 1s 406us/step - loss: 0.1009 - MAE: 0.1009\n",
      "Epoch 47/50\n",
      "1398/1398 [==============================] - 1s 412us/step - loss: 0.0986 - MAE: 0.0986\n",
      "Epoch 48/50\n",
      "1398/1398 [==============================] - 1s 407us/step - loss: 0.0986 - MAE: 0.0986\n",
      "Epoch 49/50\n",
      "1398/1398 [==============================] - 1s 403us/step - loss: 0.0998 - MAE: 0.0998\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_57 (Dense)             (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (1, 70)                   1050      \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (1, 7)                    497       \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (1, 1)                    8         \n",
      "=================================================================\n",
      "Total params: 1,681\n",
      "Trainable params: 1,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.08425357212293326\n"
     ]
    }
   ],
   "source": [
    "ModelDenseRMSprop3 = Sequential()\n",
    "ModelDenseRMSprop3.add(Dense(14))\n",
    "ModelDenseRMSprop3.add(Dense(70, activation='relu'))\n",
    "ModelDenseRMSprop3.add(Dense(7, activation='selu'))\n",
    "ModelDenseRMSprop3.add(Dense(1, activation='linear'))\n",
    "ModelDenseRMSprop3.compile(optimizer='RMSprop', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseRMSprop3.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseRMSprop3.summary()\n",
    "\n",
    "DenseRMSpropPrediction3 = ModelDenseRMSprop3.predict(x_test)\n",
    "score = mean_absolute_error(DenseRMSpropPrediction3, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_61 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 396us/step - loss: 0.2920 - MAE: 0.2920\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 406us/step - loss: 0.2298 - MAE: 0.2298\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 418us/step - loss: 0.2163 - MAE: 0.2163\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 394us/step - loss: 0.1986 - MAE: 0.1986 0s - loss: 0.2059 - MAE: 0.205\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 411us/step - loss: 0.1722 - MAE: 0.1722\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 402us/step - loss: 0.1485 - MAE: 0.1485\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 414us/step - loss: 0.1344 - MAE: 0.1344\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 404us/step - loss: 0.1273 - MAE: 0.1273\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 397us/step - loss: 0.1264 - MAE: 0.1264\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 408us/step - loss: 0.1238 - MAE: 0.1238\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 410us/step - loss: 0.1202 - MAE: 0.1202\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 412us/step - loss: 0.1171 - MAE: 0.1171\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 411us/step - loss: 0.1192 - MAE: 0.1192\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 412us/step - loss: 0.1161 - MAE: 0.1161\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 429us/step - loss: 0.1140 - MAE: 0.1140\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 409us/step - loss: 0.1155 - MAE: 0.1155\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 405us/step - loss: 0.1135 - MAE: 0.1135\n",
      "Epoch 18/50\n",
      "1398/1398 [==============================] - 1s 420us/step - loss: 0.1101 - MAE: 0.1101\n",
      "Epoch 19/50\n",
      "1398/1398 [==============================] - 1s 452us/step - loss: 0.1098 - MAE: 0.1098\n",
      "Epoch 20/50\n",
      "1398/1398 [==============================] - 1s 421us/step - loss: 0.1095 - MAE: 0.1095\n",
      "Epoch 21/50\n",
      "1398/1398 [==============================] - 1s 388us/step - loss: 0.1112 - MAE: 0.1112\n",
      "Epoch 22/50\n",
      "1398/1398 [==============================] - 1s 399us/step - loss: 0.1086 - MAE: 0.1086\n",
      "Epoch 23/50\n",
      "1398/1398 [==============================] - 1s 465us/step - loss: 0.1050 - MAE: 0.1050\n",
      "Epoch 24/50\n",
      "1398/1398 [==============================] - 1s 460us/step - loss: 0.1103 - MAE: 0.1103\n",
      "Epoch 25/50\n",
      "1398/1398 [==============================] - 1s 403us/step - loss: 0.1076 - MAE: 0.1076\n",
      "Epoch 26/50\n",
      "1398/1398 [==============================] - 1s 414us/step - loss: 0.1068 - MAE: 0.1068\n",
      "Epoch 27/50\n",
      "1398/1398 [==============================] - 1s 406us/step - loss: 0.1043 - MAE: 0.1043\n",
      "Epoch 28/50\n",
      "1398/1398 [==============================] - 1s 426us/step - loss: 0.1054 - MAE: 0.1054\n",
      "Epoch 29/50\n",
      "1398/1398 [==============================] - 1s 409us/step - loss: 0.1060 - MAE: 0.1060\n",
      "Epoch 30/50\n",
      "1398/1398 [==============================] - 1s 416us/step - loss: 0.0995 - MAE: 0.0995\n",
      "Epoch 31/50\n",
      "1398/1398 [==============================] - 1s 430us/step - loss: 0.1055 - MAE: 0.1055\n",
      "Epoch 32/50\n",
      "1398/1398 [==============================] - 1s 407us/step - loss: 0.1023 - MAE: 0.1023\n",
      "Epoch 33/50\n",
      "1398/1398 [==============================] - 1s 393us/step - loss: 0.1030 - MAE: 0.1030\n",
      "Epoch 34/50\n",
      "1398/1398 [==============================] - 1s 417us/step - loss: 0.1011 - MAE: 0.1011\n",
      "Epoch 35/50\n",
      "1398/1398 [==============================] - 1s 416us/step - loss: 0.1021 - MAE: 0.1021\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_61 (Dense)             (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (1, 70)                   1050      \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (1, 7)                    497       \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (1, 1)                    8         \n",
      "=================================================================\n",
      "Total params: 1,681\n",
      "Trainable params: 1,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.08554932917870596\n"
     ]
    }
   ],
   "source": [
    "ModelDenseRMSprop4 = Sequential()\n",
    "ModelDenseRMSprop4.add(Dense(14))\n",
    "ModelDenseRMSprop4.add(Dense(70, activation='relu'))\n",
    "ModelDenseRMSprop4.add(Dense(7, activation='elu'))\n",
    "ModelDenseRMSprop4.add(Dense(1, activation='linear'))\n",
    "ModelDenseRMSprop4.compile(optimizer='RMSprop', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseRMSprop4.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseRMSprop4.summary()\n",
    "\n",
    "DenseRMSpropPrediction4 = ModelDenseRMSprop4.predict(x_test)\n",
    "score = mean_absolute_error(DenseRMSpropPrediction4, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_65 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 397us/step - loss: 0.3178 - MAE: 0.3178\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 401us/step - loss: 0.2427 - MAE: 0.2427\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 397us/step - loss: 0.2198 - MAE: 0.2198\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 411us/step - loss: 0.1853 - MAE: 0.1853\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 427us/step - loss: 0.1517 - MAE: 0.1517\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 407us/step - loss: 0.1362 - MAE: 0.1362\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 410us/step - loss: 0.1310 - MAE: 0.1310\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 402us/step - loss: 0.1288 - MAE: 0.1288\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 420us/step - loss: 0.1260 - MAE: 0.1260\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 408us/step - loss: 0.1232 - MAE: 0.1232\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 412us/step - loss: 0.1247 - MAE: 0.1247\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 393us/step - loss: 0.1222 - MAE: 0.1222\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 413us/step - loss: 0.1203 - MAE: 0.1203\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 403us/step - loss: 0.1197 - MAE: 0.1197\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 409us/step - loss: 0.1167 - MAE: 0.1167\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 437us/step - loss: 0.1203 - MAE: 0.1203\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 423us/step - loss: 0.1141 - MAE: 0.1141\n",
      "Epoch 18/50\n",
      "1398/1398 [==============================] - 1s 408us/step - loss: 0.1151 - MAE: 0.1151\n",
      "Epoch 19/50\n",
      "1398/1398 [==============================] - 1s 418us/step - loss: 0.1118 - MAE: 0.1118\n",
      "Epoch 20/50\n",
      "1398/1398 [==============================] - 1s 409us/step - loss: 0.1154 - MAE: 0.1154\n",
      "Epoch 21/50\n",
      "1398/1398 [==============================] - 1s 413us/step - loss: 0.1142 - MAE: 0.1142\n",
      "Epoch 22/50\n",
      "1398/1398 [==============================] - 1s 428us/step - loss: 0.1135 - MAE: 0.1135\n",
      "Epoch 23/50\n",
      "1398/1398 [==============================] - 1s 403us/step - loss: 0.1104 - MAE: 0.1104\n",
      "Epoch 24/50\n",
      "1398/1398 [==============================] - 1s 411us/step - loss: 0.1123 - MAE: 0.1123\n",
      "Epoch 25/50\n",
      "1398/1398 [==============================] - 1s 396us/step - loss: 0.1133 - MAE: 0.1133\n",
      "Epoch 26/50\n",
      "1398/1398 [==============================] - 1s 410us/step - loss: 0.1103 - MAE: 0.1103\n",
      "Epoch 27/50\n",
      "1398/1398 [==============================] - 1s 414us/step - loss: 0.1084 - MAE: 0.1084\n",
      "Epoch 28/50\n",
      "1398/1398 [==============================] - 1s 408us/step - loss: 0.1091 - MAE: 0.1091\n",
      "Epoch 29/50\n",
      "1398/1398 [==============================] - 1s 393us/step - loss: 0.1131 - MAE: 0.1131\n",
      "Epoch 30/50\n",
      "1398/1398 [==============================] - 1s 424us/step - loss: 0.1076 - MAE: 0.1076\n",
      "Epoch 31/50\n",
      "1398/1398 [==============================] - 1s 449us/step - loss: 0.1130 - MAE: 0.1130\n",
      "Epoch 32/50\n",
      "1398/1398 [==============================] - 1s 421us/step - loss: 0.1078 - MAE: 0.1078\n",
      "Epoch 33/50\n",
      "1398/1398 [==============================] - 1s 405us/step - loss: 0.1058 - MAE: 0.1058\n",
      "Epoch 34/50\n",
      "1398/1398 [==============================] - 1s 413us/step - loss: 0.1084 - MAE: 0.1084\n",
      "Epoch 35/50\n",
      "1398/1398 [==============================] - 1s 405us/step - loss: 0.1084 - MAE: 0.1084\n",
      "Epoch 36/50\n",
      "1398/1398 [==============================] - 1s 410us/step - loss: 0.1066 - MAE: 0.1066\n",
      "Epoch 37/50\n",
      "1398/1398 [==============================] - 1s 411us/step - loss: 0.1066 - MAE: 0.1066\n",
      "Epoch 38/50\n",
      "1398/1398 [==============================] - 1s 412us/step - loss: 0.1036 - MAE: 0.1036\n",
      "Epoch 39/50\n",
      "1398/1398 [==============================] - 1s 401us/step - loss: 0.1055 - MAE: 0.1055\n",
      "Epoch 40/50\n",
      "1398/1398 [==============================] - 1s 425us/step - loss: 0.1050 - MAE: 0.1050\n",
      "Epoch 41/50\n",
      "1398/1398 [==============================] - 1s 408us/step - loss: 0.1051 - MAE: 0.1051\n",
      "Epoch 42/50\n",
      "1398/1398 [==============================] - 1s 401us/step - loss: 0.1027 - MAE: 0.1027\n",
      "Epoch 43/50\n",
      "1398/1398 [==============================] - 1s 407us/step - loss: 0.1056 - MAE: 0.1056\n",
      "Epoch 44/50\n",
      "1398/1398 [==============================] - 1s 408us/step - loss: 0.1046 - MAE: 0.1046\n",
      "Epoch 45/50\n",
      "1398/1398 [==============================] - 1s 393us/step - loss: 0.1067 - MAE: 0.1067\n",
      "Epoch 46/50\n",
      "1398/1398 [==============================] - 1s 397us/step - loss: 0.1054 - MAE: 0.1054\n",
      "Epoch 47/50\n",
      "1398/1398 [==============================] - 1s 397us/step - loss: 0.1007 - MAE: 0.1007\n",
      "Epoch 48/50\n",
      "1398/1398 [==============================] - 1s 400us/step - loss: 0.1026 - MAE: 0.1026\n",
      "Epoch 49/50\n",
      "1398/1398 [==============================] - 1s 406us/step - loss: 0.1011 - MAE: 0.1011\n",
      "Epoch 50/50\n",
      "1398/1398 [==============================] - 1s 415us/step - loss: 0.1012 - MAE: 0.1012\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_65 (Dense)             (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (1, 70)                   1050      \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (1, 7)                    497       \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (1, 1)                    8         \n",
      "=================================================================\n",
      "Total params: 1,681\n",
      "Trainable params: 1,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.09024953439471553\n"
     ]
    }
   ],
   "source": [
    "ModelDenseRMSprop5 = Sequential()\n",
    "ModelDenseRMSprop5.add(Dense(14))\n",
    "ModelDenseRMSprop5.add(Dense(70, activation='relu'))\n",
    "ModelDenseRMSprop5.add(Dense(7, activation='elu'))\n",
    "ModelDenseRMSprop5.add(Dense(1, activation='linear'))\n",
    "ModelDenseRMSprop5.compile(optimizer='RMSprop', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseRMSprop5.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseRMSprop5.summary()\n",
    "\n",
    "DenseRMSpropPrediction5 = ModelDenseRMSprop5.predict(x_test)\n",
    "score = mean_absolute_error(DenseRMSpropPrediction5, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_69 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 403us/step - loss: 0.3210 - MAE: 0.3210\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 420us/step - loss: 0.2414 - MAE: 0.2414\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 417us/step - loss: 0.2336 - MAE: 0.2336\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 428us/step - loss: 0.2275 - MAE: 0.2275\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 404us/step - loss: 0.2184 - MAE: 0.2184\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 427us/step - loss: 0.2088 - MAE: 0.2088\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 407us/step - loss: 0.1946 - MAE: 0.1946\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 436us/step - loss: 0.1690 - MAE: 0.1690\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 413us/step - loss: 0.1518 - MAE: 0.1518\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 402us/step - loss: 0.1441 - MAE: 0.1441\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 409us/step - loss: 0.1418 - MAE: 0.1418\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 406us/step - loss: 0.1373 - MAE: 0.1373\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 408us/step - loss: 0.1377 - MAE: 0.1377\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 414us/step - loss: 0.1362 - MAE: 0.1362\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 413us/step - loss: 0.1370 - MAE: 0.1370\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 388us/step - loss: 0.1352 - MAE: 0.1352\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 412us/step - loss: 0.1296 - MAE: 0.1296\n",
      "Epoch 18/50\n",
      "1398/1398 [==============================] - 1s 397us/step - loss: 0.1307 - MAE: 0.1307\n",
      "Epoch 19/50\n",
      "1398/1398 [==============================] - 1s 410us/step - loss: 0.1330 - MAE: 0.1330\n",
      "Epoch 20/50\n",
      "1398/1398 [==============================] - 1s 401us/step - loss: 0.1307 - MAE: 0.1307\n",
      "Epoch 21/50\n",
      "1398/1398 [==============================] - 1s 488us/step - loss: 0.1281 - MAE: 0.1281\n",
      "Epoch 22/50\n",
      "1398/1398 [==============================] - 1s 418us/step - loss: 0.1300 - MAE: 0.1300\n",
      "Epoch 23/50\n",
      "1398/1398 [==============================] - 1s 444us/step - loss: 0.1293 - MAE: 0.1293\n",
      "Epoch 24/50\n",
      "1398/1398 [==============================] - 1s 394us/step - loss: 0.1290 - MAE: 0.1290\n",
      "Epoch 25/50\n",
      "1398/1398 [==============================] - 1s 411us/step - loss: 0.1259 - MAE: 0.1259\n",
      "Epoch 26/50\n",
      "1398/1398 [==============================] - 1s 413us/step - loss: 0.1272 - MAE: 0.1272\n",
      "Epoch 27/50\n",
      "1398/1398 [==============================] - 1s 417us/step - loss: 0.1239 - MAE: 0.1239\n",
      "Epoch 28/50\n",
      "1398/1398 [==============================] - 1s 403us/step - loss: 0.1258 - MAE: 0.1258\n",
      "Epoch 29/50\n",
      "1398/1398 [==============================] - 1s 412us/step - loss: 0.1226 - MAE: 0.1226\n",
      "Epoch 30/50\n",
      "1398/1398 [==============================] - 1s 407us/step - loss: 0.1297 - MAE: 0.1297\n",
      "Epoch 31/50\n",
      "1398/1398 [==============================] - 1s 469us/step - loss: 0.1287 - MAE: 0.1287\n",
      "Epoch 32/50\n",
      "1398/1398 [==============================] - 1s 409us/step - loss: 0.1294 - MAE: 0.1294\n",
      "Epoch 33/50\n",
      "1398/1398 [==============================] - 1s 388us/step - loss: 0.1240 - MAE: 0.1240\n",
      "Epoch 34/50\n",
      "1398/1398 [==============================] - 1s 403us/step - loss: 0.1213 - MAE: 0.1213\n",
      "Epoch 35/50\n",
      "1398/1398 [==============================] - 1s 403us/step - loss: 0.1255 - MAE: 0.1255\n",
      "Epoch 36/50\n",
      "1398/1398 [==============================] - 1s 423us/step - loss: 0.1214 - MAE: 0.1214\n",
      "Epoch 37/50\n",
      "1398/1398 [==============================] - 1s 406us/step - loss: 0.1187 - MAE: 0.1187\n",
      "Epoch 38/50\n",
      "1398/1398 [==============================] - 1s 411us/step - loss: 0.1248 - MAE: 0.1248\n",
      "Epoch 39/50\n",
      "1398/1398 [==============================] - 1s 405us/step - loss: 0.1191 - MAE: 0.1191\n",
      "Epoch 40/50\n",
      "1398/1398 [==============================] - 1s 408us/step - loss: 0.1220 - MAE: 0.1220\n",
      "Epoch 41/50\n",
      "1398/1398 [==============================] - 1s 413us/step - loss: 0.1210 - MAE: 0.1210\n",
      "Epoch 42/50\n",
      "1398/1398 [==============================] - 1s 411us/step - loss: 0.1182 - MAE: 0.1182\n",
      "Epoch 43/50\n",
      "1398/1398 [==============================] - 1s 402us/step - loss: 0.1208 - MAE: 0.1208\n",
      "Epoch 44/50\n",
      "1398/1398 [==============================] - 1s 406us/step - loss: 0.1166 - MAE: 0.1166\n",
      "Epoch 45/50\n",
      "1398/1398 [==============================] - 1s 404us/step - loss: 0.1191 - MAE: 0.1191\n",
      "Epoch 46/50\n",
      "1398/1398 [==============================] - 1s 396us/step - loss: 0.1186 - MAE: 0.1186\n",
      "Epoch 47/50\n",
      "1398/1398 [==============================] - 1s 444us/step - loss: 0.1196 - MAE: 0.1196\n",
      "Epoch 48/50\n",
      "1398/1398 [==============================] - 1s 433us/step - loss: 0.1200 - MAE: 0.1200\n",
      "Epoch 49/50\n",
      "1398/1398 [==============================] - 1s 427us/step - loss: 0.1171 - MAE: 0.1171\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_69 (Dense)             (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (1, 70)                   1050      \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (1, 7)                    497       \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (1, 1)                    8         \n",
      "=================================================================\n",
      "Total params: 1,681\n",
      "Trainable params: 1,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.10191311485866408\n"
     ]
    }
   ],
   "source": [
    "ModelDenseRMSprop6 = Sequential()\n",
    "ModelDenseRMSprop6.add(Dense(14))\n",
    "ModelDenseRMSprop6.add(Dense(70))\n",
    "ModelDenseRMSprop6.add(Dense(7, activation='relu'))\n",
    "ModelDenseRMSprop6.add(Dense(1, activation='linear'))\n",
    "ModelDenseRMSprop6.compile(optimizer='RMSprop', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseRMSprop6.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseRMSprop6.summary()\n",
    "\n",
    "DenseRMSpropPrediction6 = ModelDenseRMSprop6.predict(x_test)\n",
    "score = mean_absolute_error(DenseRMSpropPrediction6, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_73 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 417us/step - loss: 0.3387 - MAE: 0.3387\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 422us/step - loss: 0.2714 - MAE: 0.2714\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 420us/step - loss: 0.2630 - MAE: 0.2630\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 437us/step - loss: 0.2547 - MAE: 0.2547\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 423us/step - loss: 0.2478 - MAE: 0.2478\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 436us/step - loss: 0.2345 - MAE: 0.2345\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 431us/step - loss: 0.2331 - MAE: 0.2331\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 421us/step - loss: 0.2262 - MAE: 0.2262\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 428us/step - loss: 0.2206 - MAE: 0.2206\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 432us/step - loss: 0.2052 - MAE: 0.2052\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 444us/step - loss: 0.1896 - MAE: 0.1896\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 415us/step - loss: 0.1770 - MAE: 0.1770\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 425us/step - loss: 0.1765 - MAE: 0.1765\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 417us/step - loss: 0.1754 - MAE: 0.1754\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 424us/step - loss: 0.1738 - MAE: 0.1738\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 415us/step - loss: 0.1662 - MAE: 0.1662\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 415us/step - loss: 0.1613 - MAE: 0.1613\n",
      "Epoch 18/50\n",
      "1398/1398 [==============================] - 1s 422us/step - loss: 0.1546 - MAE: 0.1546\n",
      "Epoch 19/50\n",
      "1398/1398 [==============================] - 1s 430us/step - loss: 0.1578 - MAE: 0.1578\n",
      "Epoch 20/50\n",
      "1398/1398 [==============================] - 1s 418us/step - loss: 0.1454 - MAE: 0.1454\n",
      "Epoch 21/50\n",
      "1398/1398 [==============================] - 1s 415us/step - loss: 0.1502 - MAE: 0.1502\n",
      "Epoch 22/50\n",
      "1398/1398 [==============================] - 1s 411us/step - loss: 0.1558 - MAE: 0.1558\n",
      "Epoch 23/50\n",
      "1398/1398 [==============================] - 1s 413us/step - loss: 0.1508 - MAE: 0.1508\n",
      "Epoch 24/50\n",
      "1398/1398 [==============================] - 1s 424us/step - loss: 0.1468 - MAE: 0.1468\n",
      "Epoch 25/50\n",
      "1398/1398 [==============================] - 1s 419us/step - loss: 0.1468 - MAE: 0.1468\n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_73 (Dense)             (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (1, 14)                   0         \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (1, 70)                   1050      \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (1, 70)                   0         \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (1, 7)                    497       \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (1, 1)                    8         \n",
      "=================================================================\n",
      "Total params: 1,681\n",
      "Trainable params: 1,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.10906423976314145\n"
     ]
    }
   ],
   "source": [
    "ModelDenseRMSprop6_1 = Sequential()\n",
    "ModelDenseRMSprop6_1.add(Dense(14))\n",
    "ModelDenseRMSprop6_1.add(Dropout(0.1))\n",
    "ModelDenseRMSprop6_1.add(Dense(70))\n",
    "ModelDenseRMSprop6_1.add(Dropout(0.1))\n",
    "ModelDenseRMSprop6_1.add(Dense(7, activation='relu'))\n",
    "ModelDenseRMSprop6_1.add(Dense(1, activation='linear'))\n",
    "ModelDenseRMSprop6_1.compile(optimizer='RMSprop', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseRMSprop6_1.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseRMSprop6_1.summary()\n",
    "\n",
    "DenseRMSpropPrediction6_1 = ModelDenseRMSprop6_1.predict(x_test)\n",
    "score = mean_absolute_error(DenseRMSpropPrediction6_1, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_77 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 435us/step - loss: 0.2971 - MAE: 0.2971\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 515us/step - loss: 0.2350 - MAE: 0.2350\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 497us/step - loss: 0.1870 - MAE: 0.1870\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 467us/step - loss: 0.1538 - MAE: 0.1538\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 445us/step - loss: 0.1422 - MAE: 0.1422\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 454us/step - loss: 0.1392 - MAE: 0.1392\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 435us/step - loss: 0.1327 - MAE: 0.1327\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 470us/step - loss: 0.1290 - MAE: 0.1290\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 440us/step - loss: 0.1263 - MAE: 0.1263\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 463us/step - loss: 0.1285 - MAE: 0.1285\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 468us/step - loss: 0.1241 - MAE: 0.1241\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 474us/step - loss: 0.1228 - MAE: 0.1228\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 442us/step - loss: 0.1199 - MAE: 0.1199\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 431us/step - loss: 0.1181 - MAE: 0.1181\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 506us/step - loss: 0.1163 - MAE: 0.1163\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 467us/step - loss: 0.1184 - MAE: 0.1184\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 453us/step - loss: 0.1162 - MAE: 0.1162\n",
      "Epoch 18/50\n",
      "1398/1398 [==============================] - 1s 483us/step - loss: 0.1129 - MAE: 0.1129\n",
      "Epoch 19/50\n",
      "1398/1398 [==============================] - 1s 453us/step - loss: 0.1137 - MAE: 0.1137\n",
      "Epoch 20/50\n",
      "1398/1398 [==============================] - 1s 456us/step - loss: 0.1134 - MAE: 0.1134\n",
      "Epoch 21/50\n",
      "1398/1398 [==============================] - 1s 474us/step - loss: 0.1115 - MAE: 0.1115\n",
      "Epoch 22/50\n",
      "1398/1398 [==============================] - 1s 438us/step - loss: 0.1110 - MAE: 0.1110\n",
      "Epoch 23/50\n",
      "1398/1398 [==============================] - 1s 464us/step - loss: 0.1073 - MAE: 0.1073\n",
      "Epoch 24/50\n",
      "1398/1398 [==============================] - 1s 441us/step - loss: 0.1080 - MAE: 0.1080\n",
      "Epoch 25/50\n",
      "1398/1398 [==============================] - 1s 447us/step - loss: 0.1067 - MAE: 0.1067\n",
      "Epoch 26/50\n",
      "1398/1398 [==============================] - 1s 463us/step - loss: 0.1034 - MAE: 0.1034\n",
      "Epoch 27/50\n",
      "1398/1398 [==============================] - 1s 430us/step - loss: 0.1050 - MAE: 0.1050\n",
      "Epoch 28/50\n",
      "1398/1398 [==============================] - 1s 452us/step - loss: 0.1029 - MAE: 0.1029\n",
      "Epoch 29/50\n",
      "1398/1398 [==============================] - 1s 466us/step - loss: 0.1054 - MAE: 0.1054 0s - loss: 0.1042 - MAE: 0.10\n",
      "Epoch 30/50\n",
      "1398/1398 [==============================] - 1s 435us/step - loss: 0.1047 - MAE: 0.1047\n",
      "Epoch 31/50\n",
      "1398/1398 [==============================] - 1s 421us/step - loss: 0.1023 - MAE: 0.1023\n",
      "Epoch 32/50\n",
      "1398/1398 [==============================] - 1s 430us/step - loss: 0.0984 - MAE: 0.0984\n",
      "Epoch 33/50\n",
      "1398/1398 [==============================] - 1s 480us/step - loss: 0.0989 - MAE: 0.0989\n",
      "Epoch 34/50\n",
      "1398/1398 [==============================] - 1s 434us/step - loss: 0.0968 - MAE: 0.0968\n",
      "Epoch 35/50\n",
      "1398/1398 [==============================] - 1s 487us/step - loss: 0.0953 - MAE: 0.0953\n",
      "Epoch 36/50\n",
      "1398/1398 [==============================] - 1s 486us/step - loss: 0.0932 - MAE: 0.0932\n",
      "Epoch 37/50\n",
      "1398/1398 [==============================] - 1s 448us/step - loss: 0.0914 - MAE: 0.0914\n",
      "Epoch 38/50\n",
      "1398/1398 [==============================] - 1s 443us/step - loss: 0.0936 - MAE: 0.0936\n",
      "Epoch 39/50\n",
      "1398/1398 [==============================] - 1s 435us/step - loss: 0.0944 - MAE: 0.0944\n",
      "Epoch 40/50\n",
      "1398/1398 [==============================] - 1s 452us/step - loss: 0.0868 - MAE: 0.0868\n",
      "Epoch 41/50\n",
      "1398/1398 [==============================] - 1s 518us/step - loss: 0.0851 - MAE: 0.0851\n",
      "Epoch 42/50\n",
      "1398/1398 [==============================] - 1s 428us/step - loss: 0.0889 - MAE: 0.0889\n",
      "Epoch 43/50\n",
      "1398/1398 [==============================] - 1s 443us/step - loss: 0.0879 - MAE: 0.0879\n",
      "Epoch 44/50\n",
      "1398/1398 [==============================] - 1s 465us/step - loss: 0.0854 - MAE: 0.0854\n",
      "Epoch 45/50\n",
      "1398/1398 [==============================] - 1s 446us/step - loss: 0.0871 - MAE: 0.0871\n",
      "Epoch 46/50\n",
      "1398/1398 [==============================] - 1s 453us/step - loss: 0.0849 - MAE: 0.0849\n",
      "Epoch 47/50\n",
      "1398/1398 [==============================] - 1s 469us/step - loss: 0.0844 - MAE: 0.0844\n",
      "Epoch 48/50\n",
      "1398/1398 [==============================] - 1s 478us/step - loss: 0.0818 - MAE: 0.0818\n",
      "Epoch 49/50\n",
      "1398/1398 [==============================] - 1s 460us/step - loss: 0.0798 - MAE: 0.0798\n",
      "Epoch 50/50\n",
      "1398/1398 [==============================] - 1s 435us/step - loss: 0.0798 - MAE: 0.0798\n",
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_77 (Dense)             (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (1, 192)                  2880      \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (1, 24)                   4632      \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (1, 9)                    225       \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (1, 1)                    10        \n",
      "=================================================================\n",
      "Total params: 7,873\n",
      "Trainable params: 7,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.10830603585751716\n"
     ]
    }
   ],
   "source": [
    "ModelDenseRMSprop7 = Sequential()\n",
    "ModelDenseRMSprop7.add(Dense(14))\n",
    "ModelDenseRMSprop7.add(Dense(192, activation='relu'))\n",
    "ModelDenseRMSprop7.add(Dense(24, activation='relu'))\n",
    "ModelDenseRMSprop7.add(Dense(9, activation='relu'))\n",
    "ModelDenseRMSprop7.add(Dense(1, activation='linear'))\n",
    "ModelDenseRMSprop7.compile(optimizer='RMSprop', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseRMSprop7.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseRMSprop7.summary()\n",
    "\n",
    "DenseRMSpropPrediction7 = ModelDenseRMSprop7.predict(x_test)\n",
    "score = mean_absolute_error(DenseRMSpropPrediction7, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1398/1398 [==============================] - 1s 434us/step - loss: 0.2964 - MAE: 0.2964\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 447us/step - loss: 0.2293 - MAE: 0.2293\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 433us/step - loss: 0.2139 - MAE: 0.2139\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 441us/step - loss: 0.1925 - MAE: 0.1925\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 455us/step - loss: 0.1534 - MAE: 0.1534\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 461us/step - loss: 0.1371 - MAE: 0.1371\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 435us/step - loss: 0.1243 - MAE: 0.1243\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 454us/step - loss: 0.1172 - MAE: 0.1172\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 439us/step - loss: 0.1170 - MAE: 0.1170\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 436us/step - loss: 0.1163 - MAE: 0.1163\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 426us/step - loss: 0.1111 - MAE: 0.1111\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 424us/step - loss: 0.1166 - MAE: 0.1166\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 430us/step - loss: 0.1130 - MAE: 0.1130\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 419us/step - loss: 0.1091 - MAE: 0.1091\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 414us/step - loss: 0.1062 - MAE: 0.1062\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 437us/step - loss: 0.1067 - MAE: 0.1067\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 440us/step - loss: 0.1064 - MAE: 0.1064\n",
      "Epoch 18/50\n",
      "1398/1398 [==============================] - 1s 441us/step - loss: 0.1074 - MAE: 0.1074\n",
      "Epoch 19/50\n",
      "1398/1398 [==============================] - 1s 427us/step - loss: 0.1032 - MAE: 0.1032\n",
      "Epoch 20/50\n",
      "1398/1398 [==============================] - 1s 429us/step - loss: 0.1044 - MAE: 0.1044\n",
      "Epoch 21/50\n",
      "1398/1398 [==============================] - 1s 425us/step - loss: 0.1065 - MAE: 0.1065\n",
      "Epoch 22/50\n",
      "1398/1398 [==============================] - 1s 453us/step - loss: 0.1026 - MAE: 0.1026\n",
      "Epoch 23/50\n",
      "1398/1398 [==============================] - 1s 460us/step - loss: 0.1037 - MAE: 0.1037\n",
      "Epoch 24/50\n",
      "1398/1398 [==============================] - 1s 455us/step - loss: 0.0984 - MAE: 0.0984\n",
      "Epoch 25/50\n",
      "1398/1398 [==============================] - 1s 431us/step - loss: 0.0990 - MAE: 0.0990\n",
      "Epoch 26/50\n",
      "1398/1398 [==============================] - 1s 423us/step - loss: 0.0998 - MAE: 0.0998\n",
      "Epoch 27/50\n",
      "1398/1398 [==============================] - 1s 455us/step - loss: 0.0961 - MAE: 0.0961\n",
      "Epoch 28/50\n",
      "1398/1398 [==============================] - 1s 456us/step - loss: 0.0969 - MAE: 0.0969\n",
      "Epoch 29/50\n",
      "1398/1398 [==============================] - 1s 455us/step - loss: 0.0971 - MAE: 0.0971\n",
      "Epoch 30/50\n",
      "1398/1398 [==============================] - 1s 449us/step - loss: 0.0921 - MAE: 0.0921\n",
      "Epoch 31/50\n",
      "1398/1398 [==============================] - 1s 442us/step - loss: 0.0901 - MAE: 0.0901\n",
      "Epoch 32/50\n",
      "1398/1398 [==============================] - 1s 469us/step - loss: 0.0891 - MAE: 0.0891\n",
      "Epoch 33/50\n",
      "1398/1398 [==============================] - 1s 474us/step - loss: 0.0880 - MAE: 0.0880\n",
      "Epoch 34/50\n",
      "1398/1398 [==============================] - 1s 476us/step - loss: 0.0849 - MAE: 0.0849\n",
      "Epoch 35/50\n",
      "1398/1398 [==============================] - 1s 470us/step - loss: 0.0872 - MAE: 0.0872\n",
      "Epoch 36/50\n",
      "1398/1398 [==============================] - 1s 469us/step - loss: 0.0866 - MAE: 0.0866\n",
      "Epoch 37/50\n",
      "1398/1398 [==============================] - 1s 457us/step - loss: 0.0855 - MAE: 0.0855\n",
      "Epoch 38/50\n",
      "1398/1398 [==============================] - 1s 435us/step - loss: 0.0820 - MAE: 0.0820\n",
      "Epoch 39/50\n",
      "1398/1398 [==============================] - 1s 431us/step - loss: 0.0823 - MAE: 0.0823\n",
      "Epoch 40/50\n",
      "1398/1398 [==============================] - 1s 458us/step - loss: 0.0808 - MAE: 0.0808\n",
      "Epoch 41/50\n",
      "1398/1398 [==============================] - 1s 434us/step - loss: 0.0788 - MAE: 0.0788\n",
      "Epoch 42/50\n",
      "1398/1398 [==============================] - 1s 433us/step - loss: 0.0780 - MAE: 0.0780\n",
      "Epoch 43/50\n",
      "1398/1398 [==============================] - 1s 431us/step - loss: 0.0771 - MAE: 0.0771\n",
      "Epoch 44/50\n",
      "1398/1398 [==============================] - 1s 430us/step - loss: 0.0791 - MAE: 0.0791\n",
      "Epoch 45/50\n",
      "1398/1398 [==============================] - 1s 453us/step - loss: 0.0701 - MAE: 0.0701\n",
      "Epoch 46/50\n",
      "1398/1398 [==============================] - 1s 439us/step - loss: 0.0773 - MAE: 0.0773\n",
      "Epoch 47/50\n",
      "1398/1398 [==============================] - 1s 442us/step - loss: 0.0761 - MAE: 0.0761\n",
      "Epoch 48/50\n",
      "1398/1398 [==============================] - 1s 447us/step - loss: 0.0733 - MAE: 0.0733 0s - loss: 0.0713 - MAE: 0.071\n",
      "Epoch 49/50\n",
      "1398/1398 [==============================] - 1s 454us/step - loss: 0.0765 - MAE: 0.0765\n",
      "Epoch 50/50\n",
      "1398/1398 [==============================] - 1s 432us/step - loss: 0.0747 - MAE: 0.0747\n",
      "WARNING:tensorflow:Layer dense_82 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Test score: 0.07388324603642335\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ModelDenseAdam1 = Sequential()\n",
    "ModelDenseAdam1.add(Dense(14))\n",
    "ModelDenseAdam1.add(Dense(70, activation='relu'))\n",
    "ModelDenseAdam1.add(Dense(7, activation='relu'))\n",
    "ModelDenseAdam1.add(Dense(1, activation='linear'))\n",
    "ModelDenseAdam1.compile(optimizer='Adam', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseAdam1.fit(x_train.values, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "\n",
    "DenseAdamPrediction1 = ModelDenseAdam1.predict(x_test)\n",
    "score = mean_absolute_error(DenseAdamPrediction1, y_test)\n",
    "print('Test score:', score)\n",
    "scores.append(score)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1398/1398 [==============================] - 1s 438us/step - loss: 0.3388 - MAE: 0.3388\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 450us/step - loss: 0.2521 - MAE: 0.2521\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 461us/step - loss: 0.2404 - MAE: 0.2404\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 447us/step - loss: 0.2303 - MAE: 0.2303\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 476us/step - loss: 0.2022 - MAE: 0.2022\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 437us/step - loss: 0.1729 - MAE: 0.1729\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 459us/step - loss: 0.1627 - MAE: 0.1627\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 448us/step - loss: 0.1492 - MAE: 0.1492\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 466us/step - loss: 0.1482 - MAE: 0.1482\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 450us/step - loss: 0.1355 - MAE: 0.1355\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 458us/step - loss: 0.1401 - MAE: 0.1401\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 459us/step - loss: 0.1357 - MAE: 0.1357\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 451us/step - loss: 0.1380 - MAE: 0.1380\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 465us/step - loss: 0.1342 - MAE: 0.1342\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 448us/step - loss: 0.1330 - MAE: 0.1330\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 458us/step - loss: 0.1324 - MAE: 0.1324\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 460us/step - loss: 0.1318 - MAE: 0.1318\n",
      "Epoch 18/50\n",
      "1398/1398 [==============================] - 1s 474us/step - loss: 0.1260 - MAE: 0.1260\n",
      "Epoch 19/50\n",
      "1398/1398 [==============================] - 1s 450us/step - loss: 0.1341 - MAE: 0.1341\n",
      "Epoch 20/50\n",
      "1398/1398 [==============================] - 1s 463us/step - loss: 0.1275 - MAE: 0.1275\n",
      "Epoch 21/50\n",
      "1398/1398 [==============================] - 1s 441us/step - loss: 0.1284 - MAE: 0.1284\n",
      "Epoch 22/50\n",
      "1398/1398 [==============================] - 1s 457us/step - loss: 0.1272 - MAE: 0.1272\n",
      "Epoch 23/50\n",
      "1398/1398 [==============================] - 1s 492us/step - loss: 0.1242 - MAE: 0.1242\n",
      "Epoch 24/50\n",
      "1398/1398 [==============================] - 1s 443us/step - loss: 0.1273 - MAE: 0.1273\n",
      "Epoch 25/50\n",
      "1398/1398 [==============================] - 1s 464us/step - loss: 0.1261 - MAE: 0.1261\n",
      "Epoch 26/50\n",
      "1398/1398 [==============================] - 1s 442us/step - loss: 0.1258 - MAE: 0.1258\n",
      "Epoch 27/50\n",
      "1398/1398 [==============================] - 1s 462us/step - loss: 0.1238 - MAE: 0.1238\n",
      "Epoch 28/50\n",
      "1398/1398 [==============================] - 1s 461us/step - loss: 0.1235 - MAE: 0.1235\n",
      "Epoch 29/50\n",
      "1398/1398 [==============================] - 1s 437us/step - loss: 0.1212 - MAE: 0.1212\n",
      "Epoch 30/50\n",
      "1398/1398 [==============================] - 1s 474us/step - loss: 0.1172 - MAE: 0.1172\n",
      "Epoch 31/50\n",
      "1398/1398 [==============================] - 1s 477us/step - loss: 0.1167 - MAE: 0.1167\n",
      "Epoch 32/50\n",
      "1398/1398 [==============================] - 1s 446us/step - loss: 0.1169 - MAE: 0.1169\n",
      "Epoch 33/50\n",
      "1398/1398 [==============================] - 1s 453us/step - loss: 0.1214 - MAE: 0.1214\n",
      "Epoch 34/50\n",
      "1398/1398 [==============================] - 1s 458us/step - loss: 0.1166 - MAE: 0.1166\n",
      "Epoch 35/50\n",
      "1398/1398 [==============================] - 1s 457us/step - loss: 0.1160 - MAE: 0.1160\n",
      "Epoch 36/50\n",
      "1398/1398 [==============================] - 1s 444us/step - loss: 0.1152 - MAE: 0.1152\n",
      "Epoch 37/50\n",
      "1398/1398 [==============================] - 1s 457us/step - loss: 0.1166 - MAE: 0.1166\n",
      "Epoch 38/50\n",
      "1398/1398 [==============================] - 1s 448us/step - loss: 0.1091 - MAE: 0.1091\n",
      "Epoch 39/50\n",
      "1398/1398 [==============================] - 1s 459us/step - loss: 0.1139 - MAE: 0.1139\n",
      "Epoch 40/50\n",
      "1398/1398 [==============================] - 1s 466us/step - loss: 0.1103 - MAE: 0.1103\n",
      "Epoch 41/50\n",
      "1398/1398 [==============================] - 1s 444us/step - loss: 0.1089 - MAE: 0.1089\n",
      "Epoch 42/50\n",
      "1398/1398 [==============================] - 1s 451us/step - loss: 0.1092 - MAE: 0.1092\n",
      "Epoch 43/50\n",
      "1398/1398 [==============================] - 1s 455us/step - loss: 0.1077 - MAE: 0.1077\n",
      "Epoch 44/50\n",
      "1398/1398 [==============================] - 1s 460us/step - loss: 0.1111 - MAE: 0.1111\n",
      "Epoch 45/50\n",
      "1398/1398 [==============================] - 1s 449us/step - loss: 0.1078 - MAE: 0.1078\n",
      "Epoch 46/50\n",
      "1398/1398 [==============================] - 1s 464us/step - loss: 0.1034 - MAE: 0.1034\n",
      "Epoch 47/50\n",
      "1398/1398 [==============================] - 1s 433us/step - loss: 0.1104 - MAE: 0.1104\n",
      "Epoch 48/50\n",
      "1398/1398 [==============================] - 1s 455us/step - loss: 0.1057 - MAE: 0.1057\n",
      "Epoch 49/50\n",
      "1398/1398 [==============================] - 1s 448us/step - loss: 0.1015 - MAE: 0.1015\n",
      "Epoch 50/50\n",
      "1398/1398 [==============================] - 1s 463us/step - loss: 0.1044 - MAE: 0.1044\n",
      "WARNING:tensorflow:Layer dense_86 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Test score: 0.08921726516335628\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ModelDenseAdam1_1 = Sequential()\n",
    "ModelDenseAdam1_1.add(Dense(14))\n",
    "ModelDenseAdam1_1.add(Dropout(0.1))\n",
    "ModelDenseAdam1_1.add(Dense(70, activation='relu'))\n",
    "ModelDenseAdam1_1.add(Dropout(0.1))\n",
    "ModelDenseAdam1_1.add(Dense(7, activation='relu'))\n",
    "ModelDenseAdam1_1.add(Dense(1, activation='linear'))\n",
    "ModelDenseAdam1_1.compile(optimizer='Adam', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseAdam1_1.fit(x_train.values, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "\n",
    "DenseAdamPrediction1_1 = ModelDenseAdam1_1.predict(x_test)\n",
    "score = mean_absolute_error(DenseAdamPrediction1_1, y_test)\n",
    "print('Test score:', score)\n",
    "scores.append(score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_90 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 430us/step - loss: 0.2973 - MAE: 0.2973\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 455us/step - loss: 0.2328 - MAE: 0.2328\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 447us/step - loss: 0.2181 - MAE: 0.2181\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 459us/step - loss: 0.2024 - MAE: 0.2024\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 444us/step - loss: 0.1709 - MAE: 0.1709\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 440us/step - loss: 0.1458 - MAE: 0.1458\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 468us/step - loss: 0.1309 - MAE: 0.1309\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 478us/step - loss: 0.1235 - MAE: 0.1235\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 419us/step - loss: 0.1232 - MAE: 0.1232\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 426us/step - loss: 0.1191 - MAE: 0.1191\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 448us/step - loss: 0.1180 - MAE: 0.1180\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 450us/step - loss: 0.1202 - MAE: 0.1202\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 450us/step - loss: 0.1146 - MAE: 0.1146\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 460us/step - loss: 0.1159 - MAE: 0.1159\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 446us/step - loss: 0.1125 - MAE: 0.1125\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 472us/step - loss: 0.1087 - MAE: 0.1087\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 443us/step - loss: 0.1130 - MAE: 0.1130\n",
      "Epoch 18/50\n",
      "1398/1398 [==============================] - 1s 467us/step - loss: 0.1109 - MAE: 0.1109\n",
      "Epoch 19/50\n",
      "1398/1398 [==============================] - 1s 450us/step - loss: 0.1103 - MAE: 0.1103\n",
      "Epoch 20/50\n",
      "1398/1398 [==============================] - 1s 461us/step - loss: 0.1096 - MAE: 0.1096\n",
      "Epoch 21/50\n",
      "1398/1398 [==============================] - 1s 449us/step - loss: 0.1093 - MAE: 0.1093\n",
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_90 (Dense)             (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (1, 70)                   1050      \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (1, 7)                    497       \n",
      "_________________________________________________________________\n",
      "dense_93 (Dense)             (1, 1)                    8         \n",
      "=================================================================\n",
      "Total params: 1,681\n",
      "Trainable params: 1,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.08751290208853287\n"
     ]
    }
   ],
   "source": [
    "ModelDenseAdam2 = Sequential()\n",
    "ModelDenseAdam2.add(Dense(14))\n",
    "ModelDenseAdam2.add(Dense(70, activation='relu'))\n",
    "ModelDenseAdam2.add(Dense(7, activation='elu'))\n",
    "ModelDenseAdam2.add(Dense(1, activation='linear'))\n",
    "ModelDenseAdam2.compile(optimizer='Adam', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseAdam2.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseAdam2.summary()\n",
    "\n",
    "DenseAdamPrediction2 = ModelDenseAdam2.predict(x_test)\n",
    "score = mean_absolute_error(DenseAdamPrediction2, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_94 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 440us/step - loss: 0.3309 - MAE: 0.3309\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 465us/step - loss: 0.2542 - MAE: 0.2542\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 474us/step - loss: 0.2457 - MAE: 0.2457\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 464us/step - loss: 0.2331 - MAE: 0.2331\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 462us/step - loss: 0.2206 - MAE: 0.2206\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 461us/step - loss: 0.1967 - MAE: 0.1967\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 459us/step - loss: 0.1723 - MAE: 0.1723\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 469us/step - loss: 0.1566 - MAE: 0.1566\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 453us/step - loss: 0.1487 - MAE: 0.1487\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 460us/step - loss: 0.1456 - MAE: 0.1456\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 465us/step - loss: 0.1391 - MAE: 0.1391\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 461us/step - loss: 0.1326 - MAE: 0.1326\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 477us/step - loss: 0.1365 - MAE: 0.1365\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 451us/step - loss: 0.1327 - MAE: 0.1327\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 471us/step - loss: 0.1302 - MAE: 0.1302\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 466us/step - loss: 0.1315 - MAE: 0.1315\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 445us/step - loss: 0.1312 - MAE: 0.1312\n",
      "Epoch 18/50\n",
      "1398/1398 [==============================] - 1s 471us/step - loss: 0.1273 - MAE: 0.1273\n",
      "Epoch 19/50\n",
      "1398/1398 [==============================] - 1s 460us/step - loss: 0.1289 - MAE: 0.1289\n",
      "Epoch 20/50\n",
      "1398/1398 [==============================] - 1s 467us/step - loss: 0.1289 - MAE: 0.1289\n",
      "Epoch 21/50\n",
      "1398/1398 [==============================] - 1s 454us/step - loss: 0.1227 - MAE: 0.1227\n",
      "Epoch 22/50\n",
      "1398/1398 [==============================] - 1s 508us/step - loss: 0.1205 - MAE: 0.1205\n",
      "Epoch 23/50\n",
      "1398/1398 [==============================] - 1s 471us/step - loss: 0.1235 - MAE: 0.1235\n",
      "Epoch 24/50\n",
      "1398/1398 [==============================] - 1s 448us/step - loss: 0.1205 - MAE: 0.1205\n",
      "Epoch 25/50\n",
      "1398/1398 [==============================] - 1s 472us/step - loss: 0.1195 - MAE: 0.1195\n",
      "Epoch 26/50\n",
      "1398/1398 [==============================] - 1s 453us/step - loss: 0.1170 - MAE: 0.1170\n",
      "Epoch 27/50\n",
      "1398/1398 [==============================] - 1s 451us/step - loss: 0.1171 - MAE: 0.1171\n",
      "Epoch 28/50\n",
      "1398/1398 [==============================] - 1s 469us/step - loss: 0.1143 - MAE: 0.1143\n",
      "Epoch 29/50\n",
      "1398/1398 [==============================] - 1s 456us/step - loss: 0.1134 - MAE: 0.1134\n",
      "Epoch 30/50\n",
      "1398/1398 [==============================] - 1s 446us/step - loss: 0.1084 - MAE: 0.1084\n",
      "Epoch 31/50\n",
      "1398/1398 [==============================] - 1s 492us/step - loss: 0.1065 - MAE: 0.1065\n",
      "Epoch 32/50\n",
      "1398/1398 [==============================] - 1s 493us/step - loss: 0.1026 - MAE: 0.1026\n",
      "Epoch 33/50\n",
      "1398/1398 [==============================] - 1s 448us/step - loss: 0.1073 - MAE: 0.1073\n",
      "Epoch 34/50\n",
      "1398/1398 [==============================] - 1s 465us/step - loss: 0.1010 - MAE: 0.1010\n",
      "Epoch 35/50\n",
      "1398/1398 [==============================] - 1s 477us/step - loss: 0.0985 - MAE: 0.0985\n",
      "Epoch 36/50\n",
      "1398/1398 [==============================] - 1s 522us/step - loss: 0.1050 - MAE: 0.1050\n",
      "Epoch 37/50\n",
      "1398/1398 [==============================] - 1s 482us/step - loss: 0.1000 - MAE: 0.1000\n",
      "Epoch 38/50\n",
      "1398/1398 [==============================] - 1s 453us/step - loss: 0.0996 - MAE: 0.0996\n",
      "Epoch 39/50\n",
      "1398/1398 [==============================] - 1s 465us/step - loss: 0.0978 - MAE: 0.0978\n",
      "Epoch 40/50\n",
      "1398/1398 [==============================] - 1s 463us/step - loss: 0.0985 - MAE: 0.0985\n",
      "Epoch 41/50\n",
      "1398/1398 [==============================] - 1s 457us/step - loss: 0.1004 - MAE: 0.1004\n",
      "Epoch 42/50\n",
      "1398/1398 [==============================] - 1s 466us/step - loss: 0.0984 - MAE: 0.0984\n",
      "Epoch 43/50\n",
      "1398/1398 [==============================] - 1s 457us/step - loss: 0.0952 - MAE: 0.0952\n",
      "Epoch 44/50\n",
      "1398/1398 [==============================] - 1s 521us/step - loss: 0.0968 - MAE: 0.0968\n",
      "Epoch 45/50\n",
      "1398/1398 [==============================] - 1s 458us/step - loss: 0.0968 - MAE: 0.0968\n",
      "Epoch 46/50\n",
      "1398/1398 [==============================] - 1s 452us/step - loss: 0.0959 - MAE: 0.0959\n",
      "Epoch 47/50\n",
      "1398/1398 [==============================] - 1s 469us/step - loss: 0.0952 - MAE: 0.0952\n",
      "Epoch 48/50\n",
      "1398/1398 [==============================] - 1s 459us/step - loss: 0.0952 - MAE: 0.0952\n",
      "Epoch 49/50\n",
      "1398/1398 [==============================] - 1s 457us/step - loss: 0.0947 - MAE: 0.0947\n",
      "Epoch 50/50\n",
      "1398/1398 [==============================] - 1s 454us/step - loss: 0.0934 - MAE: 0.0934\n",
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_94 (Dense)             (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (1, 14)                   0         \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (1, 70)                   1050      \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (1, 70)                   0         \n",
      "_________________________________________________________________\n",
      "dense_96 (Dense)             (1, 7)                    497       \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (1, 1)                    8         \n",
      "=================================================================\n",
      "Total params: 1,681\n",
      "Trainable params: 1,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.07623685948712923\n"
     ]
    }
   ],
   "source": [
    "ModelDenseAdam2_1 = Sequential()\n",
    "ModelDenseAdam2_1.add(Dense(14))\n",
    "ModelDenseAdam2_1.add(Dropout(0.1))\n",
    "ModelDenseAdam2_1.add(Dense(70, activation='relu'))\n",
    "ModelDenseAdam2_1.add(Dropout(0.1))\n",
    "ModelDenseAdam2_1.add(Dense(7, activation='elu'))\n",
    "ModelDenseAdam2_1.add(Dense(1, activation='linear'))\n",
    "ModelDenseAdam2_1.compile(optimizer='Adam', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseAdam2_1.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseAdam2_1.summary()\n",
    "\n",
    "DenseAdamPrediction2_1 = ModelDenseAdam2_1.predict(x_test)\n",
    "score = mean_absolute_error(DenseAdamPrediction2_1, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_98 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 427us/step - loss: 0.2767 - MAE: 0.2767\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 454us/step - loss: 0.2255 - MAE: 0.2255\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 462us/step - loss: 0.1915 - MAE: 0.1915\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 447us/step - loss: 0.1503 - MAE: 0.1503\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 464us/step - loss: 0.1332 - MAE: 0.1332\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 445us/step - loss: 0.1274 - MAE: 0.1274\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 480us/step - loss: 0.1237 - MAE: 0.1237\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 475us/step - loss: 0.1166 - MAE: 0.1166\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 480us/step - loss: 0.1201 - MAE: 0.1201\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 441us/step - loss: 0.1196 - MAE: 0.1196\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 455us/step - loss: 0.1130 - MAE: 0.1130\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 441us/step - loss: 0.1158 - MAE: 0.1158\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 445us/step - loss: 0.1098 - MAE: 0.1098\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 448us/step - loss: 0.1158 - MAE: 0.1158\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 454us/step - loss: 0.1112 - MAE: 0.1112\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 444us/step - loss: 0.1106 - MAE: 0.1106\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 462us/step - loss: 0.1074 - MAE: 0.1074\n",
      "Epoch 18/50\n",
      "1398/1398 [==============================] - 1s 479us/step - loss: 0.1082 - MAE: 0.1082\n",
      "Epoch 19/50\n",
      "1398/1398 [==============================] - 1s 448us/step - loss: 0.1068 - MAE: 0.1068\n",
      "Epoch 20/50\n",
      "1398/1398 [==============================] - 1s 452us/step - loss: 0.1023 - MAE: 0.1023\n",
      "Epoch 21/50\n",
      "1398/1398 [==============================] - 1s 463us/step - loss: 0.1043 - MAE: 0.1043\n",
      "Epoch 22/50\n",
      "1398/1398 [==============================] - 1s 471us/step - loss: 0.1014 - MAE: 0.1014\n",
      "Epoch 23/50\n",
      "1398/1398 [==============================] - 1s 450us/step - loss: 0.0971 - MAE: 0.0971\n",
      "Epoch 24/50\n",
      "1398/1398 [==============================] - 1s 464us/step - loss: 0.1001 - MAE: 0.1001\n",
      "Epoch 25/50\n",
      "1398/1398 [==============================] - 1s 445us/step - loss: 0.0979 - MAE: 0.0979\n",
      "Epoch 26/50\n",
      "1398/1398 [==============================] - 1s 463us/step - loss: 0.0934 - MAE: 0.0934\n",
      "Epoch 27/50\n",
      "1398/1398 [==============================] - 1s 453us/step - loss: 0.0951 - MAE: 0.0951\n",
      "Epoch 28/50\n",
      "1398/1398 [==============================] - 1s 445us/step - loss: 0.0905 - MAE: 0.0905\n",
      "Epoch 29/50\n",
      "1398/1398 [==============================] - 1s 448us/step - loss: 0.0939 - MAE: 0.0939\n",
      "Epoch 30/50\n",
      "1398/1398 [==============================] - 1s 466us/step - loss: 0.0918 - MAE: 0.0918\n",
      "Epoch 31/50\n",
      "1398/1398 [==============================] - 1s 452us/step - loss: 0.0903 - MAE: 0.0903\n",
      "Epoch 32/50\n",
      "1398/1398 [==============================] - 1s 467us/step - loss: 0.0867 - MAE: 0.0867\n",
      "Epoch 33/50\n",
      "1398/1398 [==============================] - 1s 445us/step - loss: 0.0868 - MAE: 0.0868\n",
      "Epoch 34/50\n",
      "1398/1398 [==============================] - 1s 464us/step - loss: 0.0882 - MAE: 0.0882\n",
      "Epoch 35/50\n",
      "1398/1398 [==============================] - 1s 451us/step - loss: 0.0845 - MAE: 0.0845\n",
      "Epoch 36/50\n",
      "1398/1398 [==============================] - 1s 463us/step - loss: 0.0847 - MAE: 0.0847\n",
      "Epoch 37/50\n",
      "1398/1398 [==============================] - 1s 463us/step - loss: 0.0801 - MAE: 0.0801\n",
      "Epoch 38/50\n",
      "1398/1398 [==============================] - 1s 445us/step - loss: 0.0802 - MAE: 0.0802\n",
      "Epoch 39/50\n",
      "1398/1398 [==============================] - 1s 449us/step - loss: 0.0803 - MAE: 0.0803\n",
      "Epoch 40/50\n",
      "1398/1398 [==============================] - 1s 455us/step - loss: 0.0806 - MAE: 0.0806\n",
      "Epoch 41/50\n",
      "1398/1398 [==============================] - 1s 449us/step - loss: 0.0805 - MAE: 0.0805\n",
      "Epoch 42/50\n",
      "1398/1398 [==============================] - 1s 462us/step - loss: 0.0790 - MAE: 0.0790\n",
      "Epoch 43/50\n",
      "1398/1398 [==============================] - 1s 448us/step - loss: 0.0762 - MAE: 0.0762 0s - loss: 0.0750 - MAE: 0\n",
      "Epoch 44/50\n",
      "1398/1398 [==============================] - 1s 527us/step - loss: 0.0797 - MAE: 0.0797\n",
      "Epoch 45/50\n",
      "1398/1398 [==============================] - 1s 480us/step - loss: 0.0791 - MAE: 0.0791\n",
      "Epoch 46/50\n",
      "1398/1398 [==============================] - 1s 453us/step - loss: 0.0761 - MAE: 0.0761\n",
      "Epoch 47/50\n",
      "1398/1398 [==============================] - 1s 464us/step - loss: 0.0774 - MAE: 0.0774\n",
      "Epoch 48/50\n",
      "1398/1398 [==============================] - 1s 450us/step - loss: 0.0773 - MAE: 0.0773\n",
      "Epoch 49/50\n",
      "1398/1398 [==============================] - 1s 461us/step - loss: 0.0791 - MAE: 0.0791\n",
      "Epoch 50/50\n",
      "1398/1398 [==============================] - 1s 446us/step - loss: 0.0742 - MAE: 0.0742\n",
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_98 (Dense)             (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (1, 70)                   1050      \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (1, 7)                    497       \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            (1, 1)                    8         \n",
      "=================================================================\n",
      "Total params: 1,681\n",
      "Trainable params: 1,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.06780479414921502\n"
     ]
    }
   ],
   "source": [
    "ModelDenseAdam3 = Sequential()\n",
    "ModelDenseAdam3.add(Dense(14))\n",
    "ModelDenseAdam3.add(Dense(70, activation='relu'))\n",
    "ModelDenseAdam3.add(Dense(7, activation='selu'))\n",
    "ModelDenseAdam3.add(Dense(1, activation='linear'))\n",
    "ModelDenseAdam3.compile(optimizer='Adam', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseAdam3.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseAdam3.summary()\n",
    "\n",
    "DenseAdamPrediction3 = ModelDenseAdam3.predict(x_test)\n",
    "score = mean_absolute_error(DenseAdamPrediction3, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_102 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 437us/step - loss: 0.2790 - MAE: 0.2790\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 461us/step - loss: 0.2289 - MAE: 0.2289\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 447us/step - loss: 0.2106 - MAE: 0.2106\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 463us/step - loss: 0.1909 - MAE: 0.1909\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 446us/step - loss: 0.1505 - MAE: 0.1505\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 466us/step - loss: 0.1313 - MAE: 0.1313\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 449us/step - loss: 0.1250 - MAE: 0.1250\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 456us/step - loss: 0.1223 - MAE: 0.1223\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 457us/step - loss: 0.1210 - MAE: 0.1210\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 450us/step - loss: 0.1148 - MAE: 0.1148\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 460us/step - loss: 0.1159 - MAE: 0.1159\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 451us/step - loss: 0.1159 - MAE: 0.1159\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 460us/step - loss: 0.1152 - MAE: 0.1152\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 445us/step - loss: 0.1125 - MAE: 0.1125\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 462us/step - loss: 0.1124 - MAE: 0.1124\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 439us/step - loss: 0.1106 - MAE: 0.1106\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 457us/step - loss: 0.1125 - MAE: 0.1125\n",
      "Epoch 18/50\n",
      "1398/1398 [==============================] - 1s 476us/step - loss: 0.1089 - MAE: 0.1089\n",
      "Epoch 19/50\n",
      "1398/1398 [==============================] - 1s 445us/step - loss: 0.1052 - MAE: 0.1052\n",
      "Epoch 20/50\n",
      "1398/1398 [==============================] - 1s 461us/step - loss: 0.1092 - MAE: 0.1092\n",
      "Epoch 21/50\n",
      "1398/1398 [==============================] - 1s 447us/step - loss: 0.1073 - MAE: 0.1073\n",
      "Epoch 22/50\n",
      "1398/1398 [==============================] - 1s 461us/step - loss: 0.1011 - MAE: 0.1011\n",
      "Epoch 23/50\n",
      "1398/1398 [==============================] - 1s 449us/step - loss: 0.1070 - MAE: 0.1070\n",
      "Epoch 24/50\n",
      "1398/1398 [==============================] - 1s 461us/step - loss: 0.1034 - MAE: 0.1034\n",
      "Epoch 25/50\n",
      "1398/1398 [==============================] - 1s 451us/step - loss: 0.1039 - MAE: 0.1039\n",
      "Epoch 26/50\n",
      "1398/1398 [==============================] - 1s 452us/step - loss: 0.1016 - MAE: 0.1016\n",
      "Epoch 27/50\n",
      "1398/1398 [==============================] - 1s 463us/step - loss: 0.0965 - MAE: 0.0965\n",
      "Epoch 28/50\n",
      "1398/1398 [==============================] - 1s 444us/step - loss: 0.0961 - MAE: 0.0961\n",
      "Epoch 29/50\n",
      "1398/1398 [==============================] - 1s 467us/step - loss: 0.0951 - MAE: 0.0951\n",
      "Epoch 30/50\n",
      "1398/1398 [==============================] - 1s 445us/step - loss: 0.0917 - MAE: 0.0917\n",
      "Epoch 31/50\n",
      "1398/1398 [==============================] - 1s 459us/step - loss: 0.0902 - MAE: 0.0902\n",
      "Epoch 32/50\n",
      "1398/1398 [==============================] - 1s 463us/step - loss: 0.0893 - MAE: 0.0893\n",
      "Epoch 33/50\n",
      "1398/1398 [==============================] - 1s 453us/step - loss: 0.0887 - MAE: 0.0887\n",
      "Epoch 34/50\n",
      "1398/1398 [==============================] - 1s 470us/step - loss: 0.0868 - MAE: 0.0868\n",
      "Epoch 35/50\n",
      "1398/1398 [==============================] - 1s 449us/step - loss: 0.0864 - MAE: 0.0864\n",
      "Epoch 36/50\n",
      "1398/1398 [==============================] - 1s 481us/step - loss: 0.0815 - MAE: 0.0815\n",
      "Epoch 37/50\n",
      "1398/1398 [==============================] - 1s 430us/step - loss: 0.0849 - MAE: 0.0849\n",
      "Epoch 38/50\n",
      "1398/1398 [==============================] - 1s 477us/step - loss: 0.0890 - MAE: 0.0890\n",
      "Epoch 39/50\n",
      "1398/1398 [==============================] - 1s 448us/step - loss: 0.0827 - MAE: 0.0827\n",
      "Epoch 40/50\n",
      "1398/1398 [==============================] - 1s 463us/step - loss: 0.0802 - MAE: 0.0802\n",
      "Epoch 41/50\n",
      "1398/1398 [==============================] - 1s 456us/step - loss: 0.0809 - MAE: 0.0809\n",
      "Epoch 42/50\n",
      "1398/1398 [==============================] - 1s 474us/step - loss: 0.0800 - MAE: 0.0800\n",
      "Epoch 43/50\n",
      "1398/1398 [==============================] - 1s 448us/step - loss: 0.0755 - MAE: 0.0755\n",
      "Epoch 44/50\n",
      "1398/1398 [==============================] - 1s 475us/step - loss: 0.0790 - MAE: 0.0790\n",
      "Epoch 45/50\n",
      "1398/1398 [==============================] - 1s 453us/step - loss: 0.0791 - MAE: 0.0791\n",
      "Epoch 46/50\n",
      "1398/1398 [==============================] - 1s 487us/step - loss: 0.0758 - MAE: 0.0758\n",
      "Epoch 47/50\n",
      "1398/1398 [==============================] - 1s 473us/step - loss: 0.0766 - MAE: 0.0766\n",
      "Epoch 48/50\n",
      "1398/1398 [==============================] - 1s 460us/step - loss: 0.0786 - MAE: 0.0786\n",
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_102 (Dense)            (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dense_103 (Dense)            (1, 70)                   1050      \n",
      "_________________________________________________________________\n",
      "dense_104 (Dense)            (1, 7)                    497       \n",
      "_________________________________________________________________\n",
      "dense_105 (Dense)            (1, 1)                    8         \n",
      "=================================================================\n",
      "Total params: 1,681\n",
      "Trainable params: 1,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.06934848968330237\n"
     ]
    }
   ],
   "source": [
    "ModelDenseAdam4 = Sequential()\n",
    "ModelDenseAdam4.add(Dense(14))\n",
    "ModelDenseAdam4.add(Dense(70, activation='relu'))\n",
    "ModelDenseAdam4.add(Dense(7, activation='elu'))\n",
    "ModelDenseAdam4.add(Dense(1, activation='linear'))\n",
    "ModelDenseAdam4.compile(optimizer='Adam', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseAdam4.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseAdam4.summary()\n",
    "\n",
    "DenseAdamPrediction4 = ModelDenseAdam4.predict(x_test)\n",
    "score = mean_absolute_error(DenseAdamPrediction4, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_106 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 437us/step - loss: 0.2891 - MAE: 0.2891\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 456us/step - loss: 0.2259 - MAE: 0.2259\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 474us/step - loss: 0.2050 - MAE: 0.2050\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 446us/step - loss: 0.1706 - MAE: 0.1706\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 462us/step - loss: 0.1361 - MAE: 0.1361\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 453us/step - loss: 0.1279 - MAE: 0.1279\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 461us/step - loss: 0.1259 - MAE: 0.1259\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 451us/step - loss: 0.1244 - MAE: 0.1244\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 464us/step - loss: 0.1190 - MAE: 0.1190\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 442us/step - loss: 0.1166 - MAE: 0.1166\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 467us/step - loss: 0.1158 - MAE: 0.1158\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 460us/step - loss: 0.1132 - MAE: 0.1132\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 445us/step - loss: 0.1144 - MAE: 0.1144\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 461us/step - loss: 0.1177 - MAE: 0.1177\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 455us/step - loss: 0.1134 - MAE: 0.1134\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 454us/step - loss: 0.1121 - MAE: 0.1121\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 461us/step - loss: 0.1111 - MAE: 0.1111\n",
      "Epoch 18/50\n",
      "1398/1398 [==============================] - 1s 472us/step - loss: 0.1126 - MAE: 0.1126\n",
      "Epoch 19/50\n",
      "1398/1398 [==============================] - 1s 445us/step - loss: 0.1063 - MAE: 0.1063\n",
      "Epoch 20/50\n",
      "1398/1398 [==============================] - 1s 471us/step - loss: 0.1090 - MAE: 0.1090\n",
      "Epoch 21/50\n",
      "1398/1398 [==============================] - 1s 449us/step - loss: 0.1096 - MAE: 0.1096\n",
      "Epoch 22/50\n",
      "1398/1398 [==============================] - 1s 484us/step - loss: 0.1081 - MAE: 0.1081\n",
      "Epoch 23/50\n",
      "1398/1398 [==============================] - 1s 494us/step - loss: 0.1040 - MAE: 0.1040\n",
      "Epoch 24/50\n",
      "1398/1398 [==============================] - 1s 455us/step - loss: 0.1032 - MAE: 0.1032 0s - loss: 0.0950 - MAE\n",
      "Epoch 25/50\n",
      "1398/1398 [==============================] - 1s 487us/step - loss: 0.1037 - MAE: 0.1037\n",
      "Epoch 26/50\n",
      "1398/1398 [==============================] - 1s 470us/step - loss: 0.1009 - MAE: 0.1009\n",
      "Epoch 27/50\n",
      "1398/1398 [==============================] - 1s 441us/step - loss: 0.1059 - MAE: 0.1059\n",
      "Epoch 28/50\n",
      "1398/1398 [==============================] - 1s 462us/step - loss: 0.1012 - MAE: 0.1012\n",
      "Epoch 29/50\n",
      "1398/1398 [==============================] - 1s 458us/step - loss: 0.0997 - MAE: 0.0997\n",
      "Epoch 30/50\n",
      "1398/1398 [==============================] - 1s 473us/step - loss: 0.0973 - MAE: 0.0973\n",
      "Epoch 31/50\n",
      "1398/1398 [==============================] - 1s 456us/step - loss: 0.0950 - MAE: 0.0950\n",
      "Epoch 32/50\n",
      "1398/1398 [==============================] - 1s 444us/step - loss: 0.0940 - MAE: 0.0940\n",
      "Epoch 33/50\n",
      "1398/1398 [==============================] - 1s 446us/step - loss: 0.0898 - MAE: 0.0898 0s - loss: 0.0874 - MAE: 0\n",
      "Epoch 34/50\n",
      "1398/1398 [==============================] - 1s 478us/step - loss: 0.0868 - MAE: 0.0868\n",
      "Epoch 35/50\n",
      "1398/1398 [==============================] - 1s 445us/step - loss: 0.0912 - MAE: 0.0912\n",
      "Epoch 36/50\n",
      "1398/1398 [==============================] - 1s 464us/step - loss: 0.0868 - MAE: 0.0868\n",
      "Epoch 37/50\n",
      "1398/1398 [==============================] - 1s 455us/step - loss: 0.0822 - MAE: 0.0822\n",
      "Epoch 38/50\n",
      "1398/1398 [==============================] - 1s 456us/step - loss: 0.0793 - MAE: 0.0793\n",
      "Epoch 39/50\n",
      "1398/1398 [==============================] - 1s 485us/step - loss: 0.0870 - MAE: 0.0870: 0s - loss: 0.0858 - MAE: 0\n",
      "Epoch 40/50\n",
      "1398/1398 [==============================] - 1s 468us/step - loss: 0.0855 - MAE: 0.0855\n",
      "Epoch 41/50\n",
      "1398/1398 [==============================] - 1s 454us/step - loss: 0.0833 - MAE: 0.0833\n",
      "Epoch 42/50\n",
      "1398/1398 [==============================] - 1s 452us/step - loss: 0.0802 - MAE: 0.0802\n",
      "Epoch 43/50\n",
      "1398/1398 [==============================] - 1s 462us/step - loss: 0.0800 - MAE: 0.0800\n",
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_106 (Dense)            (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dense_107 (Dense)            (1, 70)                   1050      \n",
      "_________________________________________________________________\n",
      "dense_108 (Dense)            (1, 7)                    497       \n",
      "_________________________________________________________________\n",
      "dense_109 (Dense)            (1, 1)                    8         \n",
      "=================================================================\n",
      "Total params: 1,681\n",
      "Trainable params: 1,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.06681451894065808\n"
     ]
    }
   ],
   "source": [
    "ModelDenseAdam5 = Sequential()\n",
    "ModelDenseAdam5.add(Dense(14))\n",
    "ModelDenseAdam5.add(Dense(70, activation='relu'))\n",
    "ModelDenseAdam5.add(Dense(7, activation='elu'))\n",
    "ModelDenseAdam5.add(Dense(1, activation='linear'))\n",
    "ModelDenseAdam5.compile(optimizer='Adam', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseAdam5.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseAdam5.summary()\n",
    "\n",
    "DenseAdamPrediction5 = ModelDenseAdam5.predict(x_test)\n",
    "score = mean_absolute_error(DenseAdamPrediction5, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_110 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 436us/step - loss: 0.3258 - MAE: 0.3258\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 447us/step - loss: 0.2449 - MAE: 0.2449\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 476us/step - loss: 0.2312 - MAE: 0.2312\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 433us/step - loss: 0.2302 - MAE: 0.2302\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 445us/step - loss: 0.2268 - MAE: 0.2268\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 460us/step - loss: 0.2267 - MAE: 0.2267\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 435us/step - loss: 0.2204 - MAE: 0.2204\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 456us/step - loss: 0.2087 - MAE: 0.2087\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 461us/step - loss: 0.1916 - MAE: 0.1916\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 454us/step - loss: 0.1758 - MAE: 0.1758\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 461us/step - loss: 0.1654 - MAE: 0.1654\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 450us/step - loss: 0.1597 - MAE: 0.1597\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 456us/step - loss: 0.1617 - MAE: 0.1617\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 455us/step - loss: 0.1628 - MAE: 0.1628\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 457us/step - loss: 0.1557 - MAE: 0.1557\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 453us/step - loss: 0.1577 - MAE: 0.1577\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 464us/step - loss: 0.1528 - MAE: 0.1528\n",
      "Epoch 18/50\n",
      "1398/1398 [==============================] - 1s 464us/step - loss: 0.1522 - MAE: 0.1522\n",
      "Epoch 19/50\n",
      "1398/1398 [==============================] - 1s 459us/step - loss: 0.1545 - MAE: 0.1545\n",
      "Epoch 20/50\n",
      "1398/1398 [==============================] - 1s 451us/step - loss: 0.1496 - MAE: 0.1496\n",
      "Epoch 21/50\n",
      "1398/1398 [==============================] - 1s 454us/step - loss: 0.1492 - MAE: 0.1492\n",
      "Epoch 22/50\n",
      "1398/1398 [==============================] - 1s 471us/step - loss: 0.1454 - MAE: 0.1454\n",
      "Epoch 23/50\n",
      "1398/1398 [==============================] - 1s 447us/step - loss: 0.1497 - MAE: 0.1497\n",
      "Epoch 24/50\n",
      "1398/1398 [==============================] - 1s 465us/step - loss: 0.1448 - MAE: 0.1448\n",
      "Epoch 25/50\n",
      "1398/1398 [==============================] - 1s 493us/step - loss: 0.1488 - MAE: 0.1488\n",
      "Epoch 26/50\n",
      "1398/1398 [==============================] - 1s 487us/step - loss: 0.1455 - MAE: 0.1455\n",
      "Epoch 27/50\n",
      "1398/1398 [==============================] - 1s 442us/step - loss: 0.1470 - MAE: 0.1470\n",
      "Epoch 28/50\n",
      "1398/1398 [==============================] - 1s 446us/step - loss: 0.1397 - MAE: 0.1397\n",
      "Epoch 29/50\n",
      "1398/1398 [==============================] - 1s 471us/step - loss: 0.1333 - MAE: 0.1333\n",
      "Epoch 30/50\n",
      "1398/1398 [==============================] - 1s 438us/step - loss: 0.1297 - MAE: 0.1297\n",
      "Epoch 31/50\n",
      "1398/1398 [==============================] - 1s 475us/step - loss: 0.1303 - MAE: 0.1303\n",
      "Epoch 32/50\n",
      "1398/1398 [==============================] - 1s 459us/step - loss: 0.1307 - MAE: 0.1307\n",
      "Epoch 33/50\n",
      "1398/1398 [==============================] - 1s 437us/step - loss: 0.1332 - MAE: 0.1332\n",
      "Epoch 34/50\n",
      "1398/1398 [==============================] - 1s 455us/step - loss: 0.1245 - MAE: 0.1245\n",
      "Epoch 35/50\n",
      "1398/1398 [==============================] - 1s 447us/step - loss: 0.1290 - MAE: 0.1290\n",
      "Epoch 36/50\n",
      "1398/1398 [==============================] - 1s 459us/step - loss: 0.1287 - MAE: 0.1287\n",
      "Epoch 37/50\n",
      "1398/1398 [==============================] - 1s 447us/step - loss: 0.1238 - MAE: 0.1238\n",
      "Epoch 38/50\n",
      "1398/1398 [==============================] - 1s 512us/step - loss: 0.1270 - MAE: 0.1270\n",
      "Epoch 39/50\n",
      "1398/1398 [==============================] - 1s 449us/step - loss: 0.1257 - MAE: 0.1257\n",
      "Epoch 40/50\n",
      "1398/1398 [==============================] - 1s 454us/step - loss: 0.1206 - MAE: 0.1206\n",
      "Epoch 41/50\n",
      "1398/1398 [==============================] - 1s 458us/step - loss: 0.1234 - MAE: 0.1234\n",
      "Epoch 42/50\n",
      "1398/1398 [==============================] - 1s 457us/step - loss: 0.1227 - MAE: 0.1227\n",
      "Epoch 43/50\n",
      "1398/1398 [==============================] - 1s 466us/step - loss: 0.1211 - MAE: 0.1211\n",
      "Epoch 44/50\n",
      "1398/1398 [==============================] - 1s 488us/step - loss: 0.1185 - MAE: 0.1185\n",
      "Epoch 45/50\n",
      "1398/1398 [==============================] - 1s 444us/step - loss: 0.1183 - MAE: 0.1183\n",
      "Epoch 46/50\n",
      "1398/1398 [==============================] - 1s 459us/step - loss: 0.1152 - MAE: 0.1152\n",
      "Epoch 47/50\n",
      "1398/1398 [==============================] - 1s 463us/step - loss: 0.1188 - MAE: 0.1188\n",
      "Epoch 48/50\n",
      "1398/1398 [==============================] - 1s 472us/step - loss: 0.1172 - MAE: 0.1172\n",
      "Epoch 49/50\n",
      "1398/1398 [==============================] - 1s 463us/step - loss: 0.1161 - MAE: 0.1161\n",
      "Epoch 50/50\n",
      "1398/1398 [==============================] - 1s 477us/step - loss: 0.1151 - MAE: 0.1151\n",
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_110 (Dense)            (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dense_111 (Dense)            (1, 70)                   1050      \n",
      "_________________________________________________________________\n",
      "dense_112 (Dense)            (1, 7)                    497       \n",
      "_________________________________________________________________\n",
      "dense_113 (Dense)            (1, 1)                    8         \n",
      "=================================================================\n",
      "Total params: 1,681\n",
      "Trainable params: 1,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.0938743970841565\n"
     ]
    }
   ],
   "source": [
    "ModelDenseAdam6 = Sequential()\n",
    "ModelDenseAdam6.add(Dense(14))\n",
    "ModelDenseAdam6.add(Dense(70))\n",
    "ModelDenseAdam6.add(Dense(7, activation='relu'))\n",
    "ModelDenseAdam6.add(Dense(1, activation='linear'))\n",
    "ModelDenseAdam6.compile(optimizer='Adam', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseAdam6.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseAdam6.summary()\n",
    "\n",
    "DenseAdamPrediction6 = ModelDenseAdam6.predict(x_test)\n",
    "score = mean_absolute_error(DenseAdamPrediction6, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_114 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 486us/step - loss: 0.3480 - MAE: 0.3480\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 478us/step - loss: 0.2657 - MAE: 0.2657\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 477us/step - loss: 0.2528 - MAE: 0.2528\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 513us/step - loss: 0.2470 - MAE: 0.2470\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 514us/step - loss: 0.2439 - MAE: 0.2439\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 495us/step - loss: 0.2316 - MAE: 0.2316\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 472us/step - loss: 0.2203 - MAE: 0.2203\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 472us/step - loss: 0.2020 - MAE: 0.2020\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 477us/step - loss: 0.1863 - MAE: 0.1863\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 466us/step - loss: 0.1815 - MAE: 0.1815\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 464us/step - loss: 0.1696 - MAE: 0.1696\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 464us/step - loss: 0.1585 - MAE: 0.1585\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 489us/step - loss: 0.1602 - MAE: 0.1602\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 450us/step - loss: 0.1499 - MAE: 0.1499\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 477us/step - loss: 0.1522 - MAE: 0.1522\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 508us/step - loss: 0.1479 - MAE: 0.1479\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 520us/step - loss: 0.1444 - MAE: 0.1444\n",
      "Epoch 18/50\n",
      "1398/1398 [==============================] - 1s 460us/step - loss: 0.1424 - MAE: 0.1424\n",
      "Epoch 19/50\n",
      "1398/1398 [==============================] - 1s 462us/step - loss: 0.1380 - MAE: 0.1380\n",
      "Epoch 20/50\n",
      "1398/1398 [==============================] - 1s 464us/step - loss: 0.1373 - MAE: 0.1373\n",
      "Epoch 21/50\n",
      "1398/1398 [==============================] - 1s 484us/step - loss: 0.1386 - MAE: 0.1386\n",
      "Epoch 22/50\n",
      "1398/1398 [==============================] - 1s 487us/step - loss: 0.1316 - MAE: 0.1316\n",
      "Epoch 23/50\n",
      "1398/1398 [==============================] - 1s 478us/step - loss: 0.1332 - MAE: 0.1332\n",
      "Epoch 24/50\n",
      "1398/1398 [==============================] - 1s 469us/step - loss: 0.1306 - MAE: 0.1306\n",
      "Epoch 25/50\n",
      "1398/1398 [==============================] - 1s 453us/step - loss: 0.1297 - MAE: 0.1297\n",
      "Epoch 26/50\n",
      "1398/1398 [==============================] - 1s 465us/step - loss: 0.1321 - MAE: 0.1321\n",
      "Epoch 27/50\n",
      "1398/1398 [==============================] - 1s 471us/step - loss: 0.1275 - MAE: 0.1275\n",
      "Epoch 28/50\n",
      "1398/1398 [==============================] - 1s 483us/step - loss: 0.1259 - MAE: 0.1259\n",
      "Epoch 29/50\n",
      "1398/1398 [==============================] - 1s 512us/step - loss: 0.1253 - MAE: 0.1253\n",
      "Epoch 30/50\n",
      "1398/1398 [==============================] - 1s 440us/step - loss: 0.1283 - MAE: 0.1283\n",
      "Epoch 31/50\n",
      "1398/1398 [==============================] - 1s 450us/step - loss: 0.1216 - MAE: 0.1216\n",
      "Epoch 32/50\n",
      "1398/1398 [==============================] - 1s 463us/step - loss: 0.1231 - MAE: 0.1231\n",
      "Epoch 33/50\n",
      "1398/1398 [==============================] - 1s 477us/step - loss: 0.1250 - MAE: 0.1250\n",
      "Epoch 34/50\n",
      "1398/1398 [==============================] - 1s 463us/step - loss: 0.1231 - MAE: 0.1231\n",
      "Epoch 35/50\n",
      "1398/1398 [==============================] - 1s 465us/step - loss: 0.1289 - MAE: 0.1289\n",
      "Epoch 36/50\n",
      "1398/1398 [==============================] - 1s 474us/step - loss: 0.1218 - MAE: 0.1218\n",
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_114 (Dense)            (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (1, 14)                   0         \n",
      "_________________________________________________________________\n",
      "dense_115 (Dense)            (1, 70)                   1050      \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (1, 70)                   0         \n",
      "_________________________________________________________________\n",
      "dense_116 (Dense)            (1, 7)                    497       \n",
      "_________________________________________________________________\n",
      "dense_117 (Dense)            (1, 1)                    8         \n",
      "=================================================================\n",
      "Total params: 1,681\n",
      "Trainable params: 1,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.08206490427462397\n"
     ]
    }
   ],
   "source": [
    "ModelDenseAdam6_1 = Sequential()\n",
    "ModelDenseAdam6_1.add(Dense(14))\n",
    "ModelDenseAdam6_1.add(Dropout(0.1))\n",
    "ModelDenseAdam6_1.add(Dense(70))\n",
    "ModelDenseAdam6_1.add(Dropout(0.1))\n",
    "ModelDenseAdam6_1.add(Dense(7, activation='relu'))\n",
    "ModelDenseAdam6_1.add(Dense(1, activation='linear'))\n",
    "ModelDenseAdam6_1.compile(optimizer='Adam', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseAdam6_1.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseAdam6_1.summary()\n",
    "\n",
    "DenseAdamPrediction6_1 = ModelDenseAdam6_1.predict(x_test)\n",
    "score = mean_absolute_error(DenseAdamPrediction6_1, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_118 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1398/1398 [==============================] - 1s 480us/step - loss: 0.2812 - MAE: 0.2812\n",
      "Epoch 2/50\n",
      "1398/1398 [==============================] - 1s 468us/step - loss: 0.2212 - MAE: 0.2212\n",
      "Epoch 3/50\n",
      "1398/1398 [==============================] - 1s 473us/step - loss: 0.1805 - MAE: 0.1805\n",
      "Epoch 4/50\n",
      "1398/1398 [==============================] - 1s 485us/step - loss: 0.1503 - MAE: 0.1503\n",
      "Epoch 5/50\n",
      "1398/1398 [==============================] - 1s 483us/step - loss: 0.1370 - MAE: 0.1370\n",
      "Epoch 6/50\n",
      "1398/1398 [==============================] - 1s 484us/step - loss: 0.1299 - MAE: 0.1299\n",
      "Epoch 7/50\n",
      "1398/1398 [==============================] - 1s 541us/step - loss: 0.1219 - MAE: 0.1219\n",
      "Epoch 8/50\n",
      "1398/1398 [==============================] - 1s 517us/step - loss: 0.1220 - MAE: 0.1220\n",
      "Epoch 9/50\n",
      "1398/1398 [==============================] - 1s 505us/step - loss: 0.1237 - MAE: 0.1237\n",
      "Epoch 10/50\n",
      "1398/1398 [==============================] - 1s 488us/step - loss: 0.1206 - MAE: 0.1206\n",
      "Epoch 11/50\n",
      "1398/1398 [==============================] - 1s 482us/step - loss: 0.1167 - MAE: 0.1167\n",
      "Epoch 12/50\n",
      "1398/1398 [==============================] - 1s 484us/step - loss: 0.1165 - MAE: 0.1165\n",
      "Epoch 13/50\n",
      "1398/1398 [==============================] - 1s 482us/step - loss: 0.1184 - MAE: 0.1184\n",
      "Epoch 14/50\n",
      "1398/1398 [==============================] - 1s 477us/step - loss: 0.1090 - MAE: 0.1090\n",
      "Epoch 15/50\n",
      "1398/1398 [==============================] - 1s 511us/step - loss: 0.1135 - MAE: 0.1135\n",
      "Epoch 16/50\n",
      "1398/1398 [==============================] - 1s 553us/step - loss: 0.1115 - MAE: 0.1115\n",
      "Epoch 17/50\n",
      "1398/1398 [==============================] - 1s 553us/step - loss: 0.1096 - MAE: 0.1096\n",
      "Epoch 18/50\n",
      "1398/1398 [==============================] - 1s 520us/step - loss: 0.1076 - MAE: 0.1076\n",
      "Epoch 19/50\n",
      "1398/1398 [==============================] - 1s 504us/step - loss: 0.1060 - MAE: 0.1060\n",
      "Epoch 20/50\n",
      "1398/1398 [==============================] - 1s 508us/step - loss: 0.1048 - MAE: 0.1048\n",
      "Epoch 21/50\n",
      "1398/1398 [==============================] - 1s 521us/step - loss: 0.1000 - MAE: 0.1000\n",
      "Epoch 22/50\n",
      "1398/1398 [==============================] - 1s 512us/step - loss: 0.1037 - MAE: 0.1037\n",
      "Epoch 23/50\n",
      "1398/1398 [==============================] - 1s 493us/step - loss: 0.0989 - MAE: 0.0989\n",
      "Epoch 24/50\n",
      "1398/1398 [==============================] - 1s 464us/step - loss: 0.1000 - MAE: 0.1000\n",
      "Epoch 25/50\n",
      "1398/1398 [==============================] - 1s 482us/step - loss: 0.0981 - MAE: 0.0981\n",
      "Epoch 26/50\n",
      "1398/1398 [==============================] - 1s 473us/step - loss: 0.0993 - MAE: 0.0993\n",
      "Epoch 27/50\n",
      "1398/1398 [==============================] - 1s 478us/step - loss: 0.0913 - MAE: 0.0913\n",
      "Epoch 28/50\n",
      "1398/1398 [==============================] - 1s 480us/step - loss: 0.0918 - MAE: 0.0918\n",
      "Epoch 29/50\n",
      "1398/1398 [==============================] - 1s 479us/step - loss: 0.0953 - MAE: 0.0953\n",
      "Epoch 30/50\n",
      "1398/1398 [==============================] - 1s 543us/step - loss: 0.0937 - MAE: 0.0937\n",
      "Epoch 31/50\n",
      "1398/1398 [==============================] - 1s 522us/step - loss: 0.0924 - MAE: 0.0924\n",
      "Epoch 32/50\n",
      "1398/1398 [==============================] - 1s 500us/step - loss: 0.0876 - MAE: 0.0876\n",
      "Epoch 33/50\n",
      "1398/1398 [==============================] - 1s 519us/step - loss: 0.0882 - MAE: 0.0882\n",
      "Epoch 34/50\n",
      "1398/1398 [==============================] - 1s 492us/step - loss: 0.0935 - MAE: 0.0935\n",
      "Epoch 35/50\n",
      "1398/1398 [==============================] - 1s 485us/step - loss: 0.0900 - MAE: 0.0900\n",
      "Epoch 36/50\n",
      "1398/1398 [==============================] - 1s 481us/step - loss: 0.0848 - MAE: 0.0848\n",
      "Epoch 37/50\n",
      "1398/1398 [==============================] - 1s 478us/step - loss: 0.0873 - MAE: 0.0873\n",
      "Epoch 38/50\n",
      "1398/1398 [==============================] - 1s 491us/step - loss: 0.0887 - MAE: 0.0887\n",
      "Epoch 39/50\n",
      "1398/1398 [==============================] - 1s 500us/step - loss: 0.0840 - MAE: 0.0840\n",
      "Epoch 40/50\n",
      "1398/1398 [==============================] - 1s 519us/step - loss: 0.0887 - MAE: 0.0887\n",
      "Epoch 41/50\n",
      "1398/1398 [==============================] - 1s 475us/step - loss: 0.0898 - MAE: 0.0898\n",
      "Epoch 42/50\n",
      "1398/1398 [==============================] - 1s 539us/step - loss: 0.0872 - MAE: 0.0872\n",
      "Epoch 43/50\n",
      "1398/1398 [==============================] - 1s 509us/step - loss: 0.0859 - MAE: 0.0859\n",
      "Epoch 44/50\n",
      "1398/1398 [==============================] - 1s 500us/step - loss: 0.0870 - MAE: 0.0870\n",
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_118 (Dense)            (1, 14)                   126       \n",
      "_________________________________________________________________\n",
      "dense_119 (Dense)            (1, 192)                  2880      \n",
      "_________________________________________________________________\n",
      "dense_120 (Dense)            (1, 24)                   4632      \n",
      "_________________________________________________________________\n",
      "dense_121 (Dense)            (1, 9)                    225       \n",
      "_________________________________________________________________\n",
      "dense_122 (Dense)            (1, 1)                    10        \n",
      "=================================================================\n",
      "Total params: 7,873\n",
      "Trainable params: 7,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test score: 0.07904013545557059\n"
     ]
    }
   ],
   "source": [
    "ModelDenseAdam7 = Sequential()\n",
    "ModelDenseAdam7.add(Dense(14))\n",
    "ModelDenseAdam7.add(Dense(192, activation='relu'))\n",
    "ModelDenseAdam7.add(Dense(24, activation='relu'))\n",
    "ModelDenseAdam7.add(Dense(9, activation='relu'))\n",
    "ModelDenseAdam7.add(Dense(1, activation='linear'))\n",
    "ModelDenseAdam7.compile(optimizer='Adam', loss='MAE', metrics = ['MAE'])\n",
    "\n",
    "\n",
    "ModelDenseAdam7.fit(x_train, y_train.values, epochs=50, batch_size=1, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "ModelDenseAdam7.summary()\n",
    "\n",
    "DenseAdamPrediction7 = ModelDenseAdam7.predict(x_test)\n",
    "score = mean_absolute_error(DenseAdamPrediction7, y_test)\n",
    "print('Test score:', score)\n",
    "\n",
    "scores.append(score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 1)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(scores).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.066815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.067805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.069348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.070435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.073883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.076237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.079040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.081321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.082065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.083750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.084254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.085549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.085701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.086701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.087513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.088283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.089217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.090092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.090250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.092886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.093223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.093320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.093694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.093874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.099222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.101913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.106581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.108306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.109064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.134606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "26  0.066815\n",
       "24  0.067805\n",
       "25  0.069348\n",
       "9   0.070435\n",
       "20  0.073883\n",
       "23  0.076237\n",
       "29  0.079040\n",
       "4   0.081321\n",
       "28  0.082065\n",
       "0   0.083750\n",
       "14  0.084254\n",
       "15  0.085549\n",
       "5   0.085701\n",
       "2   0.086701\n",
       "22  0.087513\n",
       "1   0.088283\n",
       "21  0.089217\n",
       "13  0.090092\n",
       "16  0.090250\n",
       "12  0.092886\n",
       "3   0.093223\n",
       "7   0.093320\n",
       "10  0.093694\n",
       "27  0.093874\n",
       "11  0.099222\n",
       "17  0.101913\n",
       "6   0.106581\n",
       "19  0.108306\n",
       "18  0.109064\n",
       "8   0.134606"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(scores).sort_values(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r</th>\n",
       "      <th>Area</th>\n",
       "      <th>In</th>\n",
       "      <th>Z</th>\n",
       "      <th>tplus</th>\n",
       "      <th>tminus</th>\n",
       "      <th>tminus_</th>\n",
       "      <th>analytic 0.259000000000006</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>0.017656</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.738067</td>\n",
       "      <td>0.608110</td>\n",
       "      <td>0.733957</td>\n",
       "      <td>8.424850e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0.653277</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.349666</td>\n",
       "      <td>0.400344</td>\n",
       "      <td>0.733957</td>\n",
       "      <td>1.503308e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>0.375193</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.381512</td>\n",
       "      <td>0.373447</td>\n",
       "      <td>0.733957</td>\n",
       "      <td>1.503308e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1391</th>\n",
       "      <td>0.015228</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.101948</td>\n",
       "      <td>0.067172</td>\n",
       "      <td>0.137838</td>\n",
       "      <td>4.878460e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1570</th>\n",
       "      <td>0.111675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.007982</td>\n",
       "      <td>0.018392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.739937e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0.344295</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.081755</td>\n",
       "      <td>0.114293</td>\n",
       "      <td>0.176597</td>\n",
       "      <td>2.405293e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>0.150077</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.034517</td>\n",
       "      <td>0.059596</td>\n",
       "      <td>0.037257</td>\n",
       "      <td>2.156762e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>0.011256</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.026482</td>\n",
       "      <td>0.003774</td>\n",
       "      <td>0.006579</td>\n",
       "      <td>2.576022e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1781</th>\n",
       "      <td>0.172589</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.563224</td>\n",
       "      <td>0.501488</td>\n",
       "      <td>0.578920</td>\n",
       "      <td>1.779062e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1480</th>\n",
       "      <td>0.233503</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.331250</td>\n",
       "      <td>0.295633</td>\n",
       "      <td>0.578920</td>\n",
       "      <td>3.049038e-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1398 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             r      Area   In    Z     tplus    tminus   tminus_  \\\n",
       "547   0.017656  1.000000  1.0  0.5  0.738067  0.608110  0.733957   \n",
       "193   0.653277  1.000000  0.0  0.5  0.349666  0.400344  0.733957   \n",
       "214   0.375193  1.000000  0.0  0.5  0.381512  0.373447  0.733957   \n",
       "1391  0.015228  0.238095  0.0  1.0  0.101948  0.067172  0.137838   \n",
       "1570  0.111675  0.000000  1.0  1.0  0.007982  0.018392  0.000000   \n",
       "...        ...       ...  ...  ...       ...       ...       ...   \n",
       "156   0.344295  0.238095  0.0  0.5  0.081755  0.114293  0.176597   \n",
       "384   0.150077  0.047619  1.0  0.5  0.034517  0.059596  0.037257   \n",
       "645   0.011256  0.000000  0.0  0.0  0.026482  0.003774  0.006579   \n",
       "1781  0.172589  1.000000  1.0  1.0  0.563224  0.501488  0.578920   \n",
       "1480  0.233503  1.000000  0.0  1.0  0.331250  0.295633  0.578920   \n",
       "\n",
       "      analytic 0.259000000000006  \n",
       "547                 8.424850e-07  \n",
       "193                 1.503308e-06  \n",
       "214                 1.503308e-06  \n",
       "1391                4.878460e-05  \n",
       "1570                8.739937e-03  \n",
       "...                          ...  \n",
       "156                 2.405293e-05  \n",
       "384                 2.156762e-04  \n",
       "645                 2.576022e-03  \n",
       "1781                1.779062e-06  \n",
       "1480                3.049038e-06  \n",
       "\n",
       "[1398 rows x 8 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "variable=analytic 0.259000000000006<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "analytic 0.259000000000006",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "mode": "lines",
         "name": "analytic 0.259000000000006",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          2,
          8,
          10,
          15,
          23,
          24,
          28,
          38,
          42,
          43,
          44,
          47,
          51,
          64,
          80,
          84,
          87,
          90,
          92,
          94,
          99,
          100,
          103,
          109,
          117,
          120,
          128,
          136,
          146,
          148,
          159,
          164,
          170,
          175,
          177,
          179,
          184,
          187,
          189,
          194,
          200,
          204,
          208,
          209,
          218,
          222,
          225,
          226,
          227,
          228,
          230,
          233,
          238,
          239,
          242,
          243,
          249,
          251,
          252,
          256,
          257,
          262,
          263,
          267,
          270,
          272,
          273,
          274,
          276,
          277,
          285,
          289,
          290,
          291,
          293,
          295,
          302,
          303,
          306,
          307,
          309,
          310,
          313,
          316,
          328,
          335,
          346,
          349,
          350,
          361,
          367,
          376,
          380,
          396,
          400,
          405,
          408,
          411,
          412,
          415,
          423,
          424,
          430,
          431,
          432,
          443,
          446,
          449,
          451,
          452,
          453,
          456,
          465,
          471,
          482,
          490,
          498,
          512,
          513,
          514,
          516,
          518,
          520,
          532,
          550,
          552,
          557,
          574,
          579,
          583,
          589,
          591,
          607,
          612,
          616,
          620,
          626,
          629,
          642,
          643,
          644,
          652,
          657,
          659,
          665,
          669,
          670,
          680,
          683,
          686,
          688,
          696,
          700,
          702,
          708,
          709,
          710,
          712,
          714,
          720,
          722,
          725,
          727,
          733,
          740,
          741,
          744,
          745,
          746,
          756,
          765,
          766,
          767,
          770,
          771,
          773,
          777,
          780,
          782,
          783,
          785,
          789,
          791,
          793,
          794,
          801,
          805,
          808,
          811,
          817,
          827,
          831,
          832,
          834,
          842,
          843,
          849,
          858,
          862,
          865,
          867,
          870,
          871,
          879,
          883,
          888,
          889,
          890,
          893,
          896,
          897,
          899,
          904,
          906,
          911,
          912,
          913,
          914,
          917,
          919,
          922,
          925,
          926,
          942,
          944,
          947,
          957,
          958,
          959,
          963,
          969,
          971,
          972,
          979,
          983,
          986,
          995,
          996,
          998,
          1001,
          1005,
          1007,
          1009,
          1016,
          1019,
          1023,
          1027,
          1029,
          1033,
          1036,
          1041,
          1042,
          1046,
          1049,
          1052,
          1054,
          1061,
          1065,
          1067,
          1074,
          1078,
          1082,
          1084,
          1087,
          1088,
          1104,
          1106,
          1107,
          1108,
          1114,
          1116,
          1119,
          1123,
          1126,
          1127,
          1138,
          1140,
          1149,
          1160,
          1161,
          1162,
          1171,
          1173,
          1179,
          1180,
          1182,
          1183,
          1185,
          1189,
          1196,
          1199,
          1204,
          1209,
          1215,
          1220,
          1224,
          1228,
          1229,
          1234,
          1235,
          1243,
          1254,
          1255,
          1258,
          1262,
          1264,
          1273,
          1274,
          1278,
          1290,
          1295,
          1298,
          1302,
          1304,
          1316,
          1318,
          1327,
          1328,
          1331,
          1334,
          1339,
          1344,
          1345,
          1346,
          1347,
          1353,
          1354,
          1358,
          1360,
          1362,
          1363,
          1367,
          1376,
          1385,
          1394,
          1396,
          1404,
          1405,
          1407,
          1415,
          1423,
          1424,
          1426,
          1432,
          1434,
          1438,
          1439,
          1441,
          1446,
          1452,
          1468,
          1470,
          1479,
          1482,
          1491,
          1492,
          1493,
          1502,
          1508,
          1511,
          1520,
          1522,
          1524,
          1528,
          1536,
          1537,
          1544,
          1545,
          1546,
          1551,
          1552,
          1557,
          1560,
          1561,
          1562,
          1563,
          1565,
          1567,
          1572,
          1576,
          1583,
          1588,
          1594,
          1597,
          1606,
          1609,
          1611,
          1616,
          1617,
          1629,
          1630,
          1631,
          1633,
          1635,
          1650,
          1652,
          1656,
          1657,
          1660,
          1668,
          1677,
          1678,
          1681,
          1692,
          1695,
          1697,
          1701,
          1704,
          1705,
          1706,
          1707,
          1710,
          1711,
          1712,
          1713,
          1714,
          1716,
          1717,
          1720,
          1721,
          1722,
          1723,
          1733,
          1737,
          1740,
          1741,
          1744,
          1745,
          1750,
          1752,
          1753,
          1756,
          1760,
          1769,
          1771,
          1772,
          1774,
          1780,
          1784,
          1785,
          1786,
          1792,
          1795,
          1797,
          1799,
          1800,
          1807,
          1808,
          1810,
          1814,
          1820,
          1821,
          1822,
          1827,
          1830,
          1831,
          1834,
          1835,
          1836,
          1839,
          1844,
          1846,
          1851,
          1852,
          1857,
          1861,
          1864
         ],
         "xaxis": "x",
         "y": [
          0.01293106720683201,
          0.009389698731687771,
          0.008604232314894628,
          0.007116053334397756,
          0.0061575502544891645,
          0.006157550254489165,
          0.00687812662363765,
          0.0003967217316923803,
          0.0003848468909055728,
          0.00038484689090557283,
          0.0003848468909055728,
          0.0003848468909055728,
          0.0003848468909055728,
          0.0003848468909055728,
          0.0003848468909055728,
          0.00038484689090557283,
          2.4052930681598302e-05,
          2.40529306815983e-05,
          2.40529306815983e-05,
          2.4052930681598302e-05,
          2.40529306815983e-05,
          2.40529306815983e-05,
          2.4052930681598302e-05,
          2.40529306815983e-05,
          2.40529306815983e-05,
          2.40529306815983e-05,
          2.40529306815983e-05,
          2.40529306815983e-05,
          2.40529306815983e-05,
          2.40529306815983e-05,
          2.40529306815983e-05,
          2.40529306815983e-05,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          1.5033081675998937e-06,
          0.004389116871955973,
          0.004155351649716951,
          0.003450818464485599,
          0.0035828773986630764,
          0.004308326524757415,
          0.00021567615403034996,
          0.00021567615403034993,
          0.00021567615403034993,
          0.00021567615403034993,
          0.00021567615403034993,
          0.00021567615403034993,
          0.00021567615403034993,
          1.347975962689687e-05,
          1.347975962689687e-05,
          1.347975962689687e-05,
          1.347975962689687e-05,
          1.347975962689687e-05,
          1.347975962689687e-05,
          1.347975962689687e-05,
          1.347975962689687e-05,
          1.347975962689687e-05,
          1.347975962689687e-05,
          1.347975962689687e-05,
          1.347975962689687e-05,
          1.347975962689687e-05,
          1.347975962689687e-05,
          1.347975962689687e-05,
          1.347975962689687e-05,
          1.347975962689687e-05,
          1.347975962689687e-05,
          1.347975962689687e-05,
          1.347975962689687e-05,
          8.424849766810544e-07,
          8.424849766810544e-07,
          8.424849766810544e-07,
          8.424849766810544e-07,
          8.424849766810544e-07,
          8.424849766810544e-07,
          8.424849766810544e-07,
          8.424849766810544e-07,
          8.424849766810544e-07,
          8.424849766810544e-07,
          8.424849766810544e-07,
          8.424849766810544e-07,
          8.424849766810544e-07,
          8.424849766810544e-07,
          8.424849766810544e-07,
          8.424849766810544e-07,
          8.424849766810544e-07,
          8.424849766810544e-07,
          8.424849766810544e-07,
          8.424849766810544e-07,
          8.424849766810544e-07,
          8.424849766810544e-07,
          0.004664720103241818,
          0.0040246200233920735,
          0.002576021827306264,
          0.002576021827306264,
          0.002576021827306264,
          0.002677465104403587,
          0.0031579441103787094,
          0.0034021547931627873,
          0.00016100136420664154,
          0.00016100136420664154,
          0.0001610013642066415,
          0.0001610013642066415,
          0.0001610013642066415,
          0.00016100136420664154,
          0.0001610013642066415,
          0.0001610013642066415,
          0.00016100136420664154,
          0.00016100136420664154,
          0.00016100136420664154,
          0.00016100136420664154,
          0.00016100136420664154,
          0.0001610013642066415,
          0.0001610013642066415,
          1.0062585262915093e-05,
          1.0062585262915096e-05,
          1.0062585262915093e-05,
          1.0062585262915093e-05,
          1.0062585262915096e-05,
          1.0062585262915096e-05,
          1.0062585262915093e-05,
          1.0062585262915093e-05,
          1.0062585262915093e-05,
          1.0062585262915093e-05,
          1.0062585262915093e-05,
          1.0062585262915093e-05,
          1.0062585262915093e-05,
          1.0062585262915093e-05,
          1.0062585262915093e-05,
          1.0062585262915093e-05,
          1.0062585262915093e-05,
          1.0062585262915093e-05,
          1.0062585262915096e-05,
          1.0062585262915093e-05,
          1.0062585262915093e-05,
          1.0062585262915093e-05,
          1.0062585262915093e-05,
          1.0062585262915093e-05,
          1.0062585262915093e-05,
          1.0062585262915096e-05,
          6.289115789321935e-07,
          6.289115789321933e-07,
          6.289115789321935e-07,
          6.289115789321933e-07,
          6.289115789321933e-07,
          6.289115789321933e-07,
          6.289115789321933e-07,
          6.289115789321933e-07,
          6.289115789321933e-07,
          6.289115789321933e-07,
          6.289115789321933e-07,
          6.289115789321933e-07,
          6.289115789321933e-07,
          6.289115789321933e-07,
          6.289115789321933e-07,
          6.289115789321933e-07,
          6.289115789321933e-07,
          6.289115789321933e-07,
          6.289115789321933e-07,
          6.289115789321933e-07,
          6.289115789321933e-07,
          6.289115789321933e-07,
          6.289115789321933e-07,
          6.289115789321933e-07,
          6.289115789321933e-07,
          6.289115789321933e-07,
          6.289115789321933e-07,
          6.289115789321933e-07,
          6.289115789321933e-07,
          6.289115789321933e-07,
          6.289115789321933e-07,
          6.289115789321933e-07,
          6.289115789321933e-07,
          6.289115789321933e-07,
          6.289115789321933e-07,
          6.289115789321933e-07,
          6.289115789321933e-07,
          6.289115789321933e-07,
          6.289115789321933e-07,
          6.289115789321935e-07,
          6.289115789321933e-07,
          0.001447279551006036,
          0.0014239003671427358,
          0.0014012645049286453,
          0.0013519588646268988,
          0.0013519588646268988,
          0.0013580853151781169,
          0.0013793370691580504,
          0.0015490132582423298,
          0.0016661306809216344,
          8.449742903918118e-05,
          8.449742903918118e-05,
          8.449742903918116e-05,
          8.449742903918116e-05,
          8.449742903918118e-05,
          8.449742903918118e-05,
          8.449742903918118e-05,
          8.449742903918118e-05,
          8.449742903918118e-05,
          8.449742903918118e-05,
          8.449742903918118e-05,
          8.449742903918118e-05,
          8.449742903918118e-05,
          8.449742903918118e-05,
          5.2810893149488236e-06,
          5.2810893149488236e-06,
          5.281089314948823e-06,
          5.281089314948823e-06,
          5.2810893149488236e-06,
          5.2810893149488236e-06,
          5.2810893149488236e-06,
          5.2810893149488236e-06,
          5.2810893149488236e-06,
          5.2810893149488236e-06,
          5.2810893149488236e-06,
          5.2810893149488236e-06,
          5.2810893149488236e-06,
          5.2810893149488236e-06,
          5.2810893149488236e-06,
          5.2810893149488236e-06,
          5.281089314948823e-06,
          5.2810893149488236e-06,
          5.2810893149488236e-06,
          5.281089314948823e-06,
          5.281089314948823e-06,
          5.2810893149488236e-06,
          3.3006808218430147e-07,
          3.3006808218430147e-07,
          3.3006808218430147e-07,
          3.3006808218430147e-07,
          3.300680821843014e-07,
          3.3006808218430147e-07,
          3.3006808218430147e-07,
          3.3006808218430147e-07,
          3.3006808218430147e-07,
          3.300680821843014e-07,
          3.3006808218430147e-07,
          3.3006808218430147e-07,
          3.3006808218430147e-07,
          3.3006808218430147e-07,
          3.3006808218430147e-07,
          3.3006808218430147e-07,
          3.3006808218430147e-07,
          3.3006808218430147e-07,
          3.3006808218430147e-07,
          3.3006808218430147e-07,
          3.3006808218430147e-07,
          3.3006808218430147e-07,
          3.3006808218430147e-07,
          3.3006808218430147e-07,
          3.300680821843014e-07,
          3.3006808218430147e-07,
          3.3006808218430147e-07,
          3.3006808218430147e-07,
          3.3006808218430147e-07,
          3.300680821843014e-07,
          3.3006808218430147e-07,
          3.3006808218430147e-07,
          3.3006808218430147e-07,
          3.3006808218430147e-07,
          3.3006808218430147e-07,
          0.023781793635460402,
          0.022440670966857992,
          0.018310375356983655,
          0.01248885870968785,
          0.014885666893902378,
          0.016767328302397102,
          0.020166211458739272,
          0.0008364825242530825,
          0.0007805536693554907,
          0.0007805536693554907,
          0.0007805536693554907,
          0.0007805536693554907,
          0.0007805536693554907,
          0.0007805536693554907,
          0.0007805536693554907,
          0.0007805536693554907,
          0.0007805536693554907,
          0.0007805536693554905,
          0.0007805536693554905,
          4.8784604334718166e-05,
          4.8784604334718166e-05,
          4.878460433471816e-05,
          4.8784604334718166e-05,
          4.8784604334718166e-05,
          4.8784604334718166e-05,
          4.8784604334718166e-05,
          4.8784604334718166e-05,
          4.8784604334718166e-05,
          4.8784604334718166e-05,
          4.8784604334718166e-05,
          4.8784604334718166e-05,
          4.8784604334718166e-05,
          4.8784604334718166e-05,
          4.8784604334718166e-05,
          4.8784604334718166e-05,
          4.8784604334718166e-05,
          4.878460433471816e-05,
          3.0490377709198854e-06,
          3.0490377709198854e-06,
          3.0490377709198854e-06,
          3.0490377709198854e-06,
          3.0490377709198854e-06,
          3.0490377709198854e-06,
          3.0490377709198854e-06,
          3.0490377709198854e-06,
          3.0490377709198854e-06,
          3.0490377709198854e-06,
          3.0490377709198854e-06,
          3.0490377709198854e-06,
          3.0490377709198854e-06,
          3.0490377709198854e-06,
          3.0490377709198854e-06,
          3.0490377709198854e-06,
          3.0490377709198854e-06,
          3.0490377709198854e-06,
          3.0490377709198854e-06,
          3.0490377709198854e-06,
          3.0490377709198854e-06,
          3.0490377709198854e-06,
          3.0490377709198854e-06,
          3.0490377709198854e-06,
          3.0490377709198854e-06,
          3.0490377709198854e-06,
          3.0490377709198854e-06,
          3.0490377709198854e-06,
          3.0490377709198854e-06,
          3.0490377709198854e-06,
          3.0490377709198854e-06,
          3.0490377709198854e-06,
          3.0490377709198854e-06,
          3.0490377709198854e-06,
          0.009236290570580783,
          0.008437647206407502,
          0.007891740906991681,
          0.007287038127444467,
          0.007891740906991681,
          0.008739937079085902,
          0.009236290570580783,
          0.0004554398829652793,
          0.0004554398829652793,
          0.0004554398829652793,
          0.0004554398829652792,
          0.0004554398829652793,
          0.0004554398829652792,
          0.0004554398829652793,
          0.0004554398829652793,
          0.0004554398829652793,
          0.0004554398829652793,
          2.8464992685329957e-05,
          2.846499268532995e-05,
          2.8464992685329957e-05,
          2.846499268532995e-05,
          2.846499268532995e-05,
          2.8464992685329957e-05,
          2.846499268532995e-05,
          2.846499268532995e-05,
          2.8464992685329957e-05,
          2.846499268532995e-05,
          2.846499268532995e-05,
          2.846499268532995e-05,
          2.846499268532995e-05,
          2.846499268532995e-05,
          2.846499268532995e-05,
          2.846499268532995e-05,
          2.846499268532995e-05,
          2.8464992685329957e-05,
          2.8464992685329957e-05,
          2.8464992685329957e-05,
          2.8464992685329957e-05,
          2.846499268532995e-05,
          2.8464992685329957e-05,
          2.846499268532995e-05,
          2.8464992685329957e-05,
          2.8464992685329957e-05,
          1.779062042833122e-06,
          1.7790620428331223e-06,
          1.7790620428331223e-06,
          1.7790620428331223e-06,
          1.7790620428331223e-06,
          1.7790620428331223e-06,
          1.779062042833122e-06,
          1.7790620428331223e-06,
          1.779062042833122e-06,
          1.7790620428331223e-06,
          1.7790620428331223e-06,
          1.7790620428331223e-06,
          1.7790620428331223e-06,
          1.7790620428331223e-06,
          1.779062042833122e-06,
          1.7790620428331223e-06,
          1.7790620428331223e-06,
          1.7790620428331223e-06,
          1.7790620428331223e-06,
          1.7790620428331223e-06,
          1.7790620428331223e-06,
          1.7790620428331223e-06,
          1.7790620428331223e-06,
          1.7790620428331223e-06,
          1.7790620428331223e-06,
          1.7790620428331223e-06,
          1.7790620428331223e-06,
          1.7790620428331223e-06,
          1.7790620428331223e-06,
          1.7790620428331223e-06,
          1.779062042833122e-06,
          1.7790620428331223e-06,
          1.7790620428331223e-06,
          1.7790620428331223e-06,
          1.7790620428331223e-06,
          1.7790620428331223e-06,
          1.7790620428331223e-06,
          1.7790620428331223e-06,
          1.779062042833122e-06,
          1.7790620428331223e-06,
          1.779062042833122e-06,
          1.7790620428331223e-06,
          1.7790620428331223e-06,
          1.779062042833122e-06,
          1.779062042833122e-06,
          1.7790620428331223e-06,
          1.779062042833122e-06
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "variable"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "index"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "value"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"83fdc6e5-d4df-478b-b428-e71e1147d91b\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"83fdc6e5-d4df-478b-b428-e71e1147d91b\")) {                    Plotly.newPlot(                        \"83fdc6e5-d4df-478b-b428-e71e1147d91b\",                        [{\"hovertemplate\": \"variable=analytic 0.259000000000006<br>index=%{x}<br>value=%{y}<extra></extra>\", \"legendgroup\": \"analytic 0.259000000000006\", \"line\": {\"color\": \"#636efa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"analytic 0.259000000000006\", \"orientation\": \"v\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [2, 8, 10, 15, 23, 24, 28, 38, 42, 43, 44, 47, 51, 64, 80, 84, 87, 90, 92, 94, 99, 100, 103, 109, 117, 120, 128, 136, 146, 148, 159, 164, 170, 175, 177, 179, 184, 187, 189, 194, 200, 204, 208, 209, 218, 222, 225, 226, 227, 228, 230, 233, 238, 239, 242, 243, 249, 251, 252, 256, 257, 262, 263, 267, 270, 272, 273, 274, 276, 277, 285, 289, 290, 291, 293, 295, 302, 303, 306, 307, 309, 310, 313, 316, 328, 335, 346, 349, 350, 361, 367, 376, 380, 396, 400, 405, 408, 411, 412, 415, 423, 424, 430, 431, 432, 443, 446, 449, 451, 452, 453, 456, 465, 471, 482, 490, 498, 512, 513, 514, 516, 518, 520, 532, 550, 552, 557, 574, 579, 583, 589, 591, 607, 612, 616, 620, 626, 629, 642, 643, 644, 652, 657, 659, 665, 669, 670, 680, 683, 686, 688, 696, 700, 702, 708, 709, 710, 712, 714, 720, 722, 725, 727, 733, 740, 741, 744, 745, 746, 756, 765, 766, 767, 770, 771, 773, 777, 780, 782, 783, 785, 789, 791, 793, 794, 801, 805, 808, 811, 817, 827, 831, 832, 834, 842, 843, 849, 858, 862, 865, 867, 870, 871, 879, 883, 888, 889, 890, 893, 896, 897, 899, 904, 906, 911, 912, 913, 914, 917, 919, 922, 925, 926, 942, 944, 947, 957, 958, 959, 963, 969, 971, 972, 979, 983, 986, 995, 996, 998, 1001, 1005, 1007, 1009, 1016, 1019, 1023, 1027, 1029, 1033, 1036, 1041, 1042, 1046, 1049, 1052, 1054, 1061, 1065, 1067, 1074, 1078, 1082, 1084, 1087, 1088, 1104, 1106, 1107, 1108, 1114, 1116, 1119, 1123, 1126, 1127, 1138, 1140, 1149, 1160, 1161, 1162, 1171, 1173, 1179, 1180, 1182, 1183, 1185, 1189, 1196, 1199, 1204, 1209, 1215, 1220, 1224, 1228, 1229, 1234, 1235, 1243, 1254, 1255, 1258, 1262, 1264, 1273, 1274, 1278, 1290, 1295, 1298, 1302, 1304, 1316, 1318, 1327, 1328, 1331, 1334, 1339, 1344, 1345, 1346, 1347, 1353, 1354, 1358, 1360, 1362, 1363, 1367, 1376, 1385, 1394, 1396, 1404, 1405, 1407, 1415, 1423, 1424, 1426, 1432, 1434, 1438, 1439, 1441, 1446, 1452, 1468, 1470, 1479, 1482, 1491, 1492, 1493, 1502, 1508, 1511, 1520, 1522, 1524, 1528, 1536, 1537, 1544, 1545, 1546, 1551, 1552, 1557, 1560, 1561, 1562, 1563, 1565, 1567, 1572, 1576, 1583, 1588, 1594, 1597, 1606, 1609, 1611, 1616, 1617, 1629, 1630, 1631, 1633, 1635, 1650, 1652, 1656, 1657, 1660, 1668, 1677, 1678, 1681, 1692, 1695, 1697, 1701, 1704, 1705, 1706, 1707, 1710, 1711, 1712, 1713, 1714, 1716, 1717, 1720, 1721, 1722, 1723, 1733, 1737, 1740, 1741, 1744, 1745, 1750, 1752, 1753, 1756, 1760, 1769, 1771, 1772, 1774, 1780, 1784, 1785, 1786, 1792, 1795, 1797, 1799, 1800, 1807, 1808, 1810, 1814, 1820, 1821, 1822, 1827, 1830, 1831, 1834, 1835, 1836, 1839, 1844, 1846, 1851, 1852, 1857, 1861, 1864], \"xaxis\": \"x\", \"y\": [0.01293106720683201, 0.009389698731687771, 0.008604232314894628, 0.007116053334397756, 0.0061575502544891645, 0.006157550254489165, 0.00687812662363765, 0.0003967217316923803, 0.0003848468909055728, 0.00038484689090557283, 0.0003848468909055728, 0.0003848468909055728, 0.0003848468909055728, 0.0003848468909055728, 0.0003848468909055728, 0.00038484689090557283, 2.4052930681598302e-05, 2.40529306815983e-05, 2.40529306815983e-05, 2.4052930681598302e-05, 2.40529306815983e-05, 2.40529306815983e-05, 2.4052930681598302e-05, 2.40529306815983e-05, 2.40529306815983e-05, 2.40529306815983e-05, 2.40529306815983e-05, 2.40529306815983e-05, 2.40529306815983e-05, 2.40529306815983e-05, 2.40529306815983e-05, 2.40529306815983e-05, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 1.5033081675998937e-06, 0.004389116871955973, 0.004155351649716951, 0.003450818464485599, 0.0035828773986630764, 0.004308326524757415, 0.00021567615403034996, 0.00021567615403034993, 0.00021567615403034993, 0.00021567615403034993, 0.00021567615403034993, 0.00021567615403034993, 0.00021567615403034993, 1.347975962689687e-05, 1.347975962689687e-05, 1.347975962689687e-05, 1.347975962689687e-05, 1.347975962689687e-05, 1.347975962689687e-05, 1.347975962689687e-05, 1.347975962689687e-05, 1.347975962689687e-05, 1.347975962689687e-05, 1.347975962689687e-05, 1.347975962689687e-05, 1.347975962689687e-05, 1.347975962689687e-05, 1.347975962689687e-05, 1.347975962689687e-05, 1.347975962689687e-05, 1.347975962689687e-05, 1.347975962689687e-05, 1.347975962689687e-05, 8.424849766810544e-07, 8.424849766810544e-07, 8.424849766810544e-07, 8.424849766810544e-07, 8.424849766810544e-07, 8.424849766810544e-07, 8.424849766810544e-07, 8.424849766810544e-07, 8.424849766810544e-07, 8.424849766810544e-07, 8.424849766810544e-07, 8.424849766810544e-07, 8.424849766810544e-07, 8.424849766810544e-07, 8.424849766810544e-07, 8.424849766810544e-07, 8.424849766810544e-07, 8.424849766810544e-07, 8.424849766810544e-07, 8.424849766810544e-07, 8.424849766810544e-07, 8.424849766810544e-07, 0.004664720103241818, 0.0040246200233920735, 0.002576021827306264, 0.002576021827306264, 0.002576021827306264, 0.002677465104403587, 0.0031579441103787094, 0.0034021547931627873, 0.00016100136420664154, 0.00016100136420664154, 0.0001610013642066415, 0.0001610013642066415, 0.0001610013642066415, 0.00016100136420664154, 0.0001610013642066415, 0.0001610013642066415, 0.00016100136420664154, 0.00016100136420664154, 0.00016100136420664154, 0.00016100136420664154, 0.00016100136420664154, 0.0001610013642066415, 0.0001610013642066415, 1.0062585262915093e-05, 1.0062585262915096e-05, 1.0062585262915093e-05, 1.0062585262915093e-05, 1.0062585262915096e-05, 1.0062585262915096e-05, 1.0062585262915093e-05, 1.0062585262915093e-05, 1.0062585262915093e-05, 1.0062585262915093e-05, 1.0062585262915093e-05, 1.0062585262915093e-05, 1.0062585262915093e-05, 1.0062585262915093e-05, 1.0062585262915093e-05, 1.0062585262915093e-05, 1.0062585262915093e-05, 1.0062585262915093e-05, 1.0062585262915096e-05, 1.0062585262915093e-05, 1.0062585262915093e-05, 1.0062585262915093e-05, 1.0062585262915093e-05, 1.0062585262915093e-05, 1.0062585262915093e-05, 1.0062585262915096e-05, 6.289115789321935e-07, 6.289115789321933e-07, 6.289115789321935e-07, 6.289115789321933e-07, 6.289115789321933e-07, 6.289115789321933e-07, 6.289115789321933e-07, 6.289115789321933e-07, 6.289115789321933e-07, 6.289115789321933e-07, 6.289115789321933e-07, 6.289115789321933e-07, 6.289115789321933e-07, 6.289115789321933e-07, 6.289115789321933e-07, 6.289115789321933e-07, 6.289115789321933e-07, 6.289115789321933e-07, 6.289115789321933e-07, 6.289115789321933e-07, 6.289115789321933e-07, 6.289115789321933e-07, 6.289115789321933e-07, 6.289115789321933e-07, 6.289115789321933e-07, 6.289115789321933e-07, 6.289115789321933e-07, 6.289115789321933e-07, 6.289115789321933e-07, 6.289115789321933e-07, 6.289115789321933e-07, 6.289115789321933e-07, 6.289115789321933e-07, 6.289115789321933e-07, 6.289115789321933e-07, 6.289115789321933e-07, 6.289115789321933e-07, 6.289115789321933e-07, 6.289115789321933e-07, 6.289115789321935e-07, 6.289115789321933e-07, 0.001447279551006036, 0.0014239003671427358, 0.0014012645049286453, 0.0013519588646268988, 0.0013519588646268988, 0.0013580853151781169, 0.0013793370691580504, 0.0015490132582423298, 0.0016661306809216344, 8.449742903918118e-05, 8.449742903918118e-05, 8.449742903918116e-05, 8.449742903918116e-05, 8.449742903918118e-05, 8.449742903918118e-05, 8.449742903918118e-05, 8.449742903918118e-05, 8.449742903918118e-05, 8.449742903918118e-05, 8.449742903918118e-05, 8.449742903918118e-05, 8.449742903918118e-05, 8.449742903918118e-05, 5.2810893149488236e-06, 5.2810893149488236e-06, 5.281089314948823e-06, 5.281089314948823e-06, 5.2810893149488236e-06, 5.2810893149488236e-06, 5.2810893149488236e-06, 5.2810893149488236e-06, 5.2810893149488236e-06, 5.2810893149488236e-06, 5.2810893149488236e-06, 5.2810893149488236e-06, 5.2810893149488236e-06, 5.2810893149488236e-06, 5.2810893149488236e-06, 5.2810893149488236e-06, 5.281089314948823e-06, 5.2810893149488236e-06, 5.2810893149488236e-06, 5.281089314948823e-06, 5.281089314948823e-06, 5.2810893149488236e-06, 3.3006808218430147e-07, 3.3006808218430147e-07, 3.3006808218430147e-07, 3.3006808218430147e-07, 3.300680821843014e-07, 3.3006808218430147e-07, 3.3006808218430147e-07, 3.3006808218430147e-07, 3.3006808218430147e-07, 3.300680821843014e-07, 3.3006808218430147e-07, 3.3006808218430147e-07, 3.3006808218430147e-07, 3.3006808218430147e-07, 3.3006808218430147e-07, 3.3006808218430147e-07, 3.3006808218430147e-07, 3.3006808218430147e-07, 3.3006808218430147e-07, 3.3006808218430147e-07, 3.3006808218430147e-07, 3.3006808218430147e-07, 3.3006808218430147e-07, 3.3006808218430147e-07, 3.300680821843014e-07, 3.3006808218430147e-07, 3.3006808218430147e-07, 3.3006808218430147e-07, 3.3006808218430147e-07, 3.300680821843014e-07, 3.3006808218430147e-07, 3.3006808218430147e-07, 3.3006808218430147e-07, 3.3006808218430147e-07, 3.3006808218430147e-07, 0.023781793635460402, 0.022440670966857992, 0.018310375356983655, 0.01248885870968785, 0.014885666893902378, 0.016767328302397102, 0.020166211458739272, 0.0008364825242530825, 0.0007805536693554907, 0.0007805536693554907, 0.0007805536693554907, 0.0007805536693554907, 0.0007805536693554907, 0.0007805536693554907, 0.0007805536693554907, 0.0007805536693554907, 0.0007805536693554907, 0.0007805536693554905, 0.0007805536693554905, 4.8784604334718166e-05, 4.8784604334718166e-05, 4.878460433471816e-05, 4.8784604334718166e-05, 4.8784604334718166e-05, 4.8784604334718166e-05, 4.8784604334718166e-05, 4.8784604334718166e-05, 4.8784604334718166e-05, 4.8784604334718166e-05, 4.8784604334718166e-05, 4.8784604334718166e-05, 4.8784604334718166e-05, 4.8784604334718166e-05, 4.8784604334718166e-05, 4.8784604334718166e-05, 4.8784604334718166e-05, 4.878460433471816e-05, 3.0490377709198854e-06, 3.0490377709198854e-06, 3.0490377709198854e-06, 3.0490377709198854e-06, 3.0490377709198854e-06, 3.0490377709198854e-06, 3.0490377709198854e-06, 3.0490377709198854e-06, 3.0490377709198854e-06, 3.0490377709198854e-06, 3.0490377709198854e-06, 3.0490377709198854e-06, 3.0490377709198854e-06, 3.0490377709198854e-06, 3.0490377709198854e-06, 3.0490377709198854e-06, 3.0490377709198854e-06, 3.0490377709198854e-06, 3.0490377709198854e-06, 3.0490377709198854e-06, 3.0490377709198854e-06, 3.0490377709198854e-06, 3.0490377709198854e-06, 3.0490377709198854e-06, 3.0490377709198854e-06, 3.0490377709198854e-06, 3.0490377709198854e-06, 3.0490377709198854e-06, 3.0490377709198854e-06, 3.0490377709198854e-06, 3.0490377709198854e-06, 3.0490377709198854e-06, 3.0490377709198854e-06, 3.0490377709198854e-06, 0.009236290570580783, 0.008437647206407502, 0.007891740906991681, 0.007287038127444467, 0.007891740906991681, 0.008739937079085902, 0.009236290570580783, 0.0004554398829652793, 0.0004554398829652793, 0.0004554398829652793, 0.0004554398829652792, 0.0004554398829652793, 0.0004554398829652792, 0.0004554398829652793, 0.0004554398829652793, 0.0004554398829652793, 0.0004554398829652793, 2.8464992685329957e-05, 2.846499268532995e-05, 2.8464992685329957e-05, 2.846499268532995e-05, 2.846499268532995e-05, 2.8464992685329957e-05, 2.846499268532995e-05, 2.846499268532995e-05, 2.8464992685329957e-05, 2.846499268532995e-05, 2.846499268532995e-05, 2.846499268532995e-05, 2.846499268532995e-05, 2.846499268532995e-05, 2.846499268532995e-05, 2.846499268532995e-05, 2.846499268532995e-05, 2.8464992685329957e-05, 2.8464992685329957e-05, 2.8464992685329957e-05, 2.8464992685329957e-05, 2.846499268532995e-05, 2.8464992685329957e-05, 2.846499268532995e-05, 2.8464992685329957e-05, 2.8464992685329957e-05, 1.779062042833122e-06, 1.7790620428331223e-06, 1.7790620428331223e-06, 1.7790620428331223e-06, 1.7790620428331223e-06, 1.7790620428331223e-06, 1.779062042833122e-06, 1.7790620428331223e-06, 1.779062042833122e-06, 1.7790620428331223e-06, 1.7790620428331223e-06, 1.7790620428331223e-06, 1.7790620428331223e-06, 1.7790620428331223e-06, 1.779062042833122e-06, 1.7790620428331223e-06, 1.7790620428331223e-06, 1.7790620428331223e-06, 1.7790620428331223e-06, 1.7790620428331223e-06, 1.7790620428331223e-06, 1.7790620428331223e-06, 1.7790620428331223e-06, 1.7790620428331223e-06, 1.7790620428331223e-06, 1.7790620428331223e-06, 1.7790620428331223e-06, 1.7790620428331223e-06, 1.7790620428331223e-06, 1.7790620428331223e-06, 1.779062042833122e-06, 1.7790620428331223e-06, 1.7790620428331223e-06, 1.7790620428331223e-06, 1.7790620428331223e-06, 1.7790620428331223e-06, 1.7790620428331223e-06, 1.7790620428331223e-06, 1.779062042833122e-06, 1.7790620428331223e-06, 1.779062042833122e-06, 1.7790620428331223e-06, 1.7790620428331223e-06, 1.779062042833122e-06, 1.779062042833122e-06, 1.7790620428331223e-06, 1.779062042833122e-06], \"yaxis\": \"y\"}],                        {\"legend\": {\"title\": {\"text\": \"variable\"}, \"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"index\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"value\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('83fdc6e5-d4df-478b-b428-e71e1147d91b');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_test.iloc[:,-1:].sort_index().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "variable=Data<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "Data",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "mode": "lines",
         "name": "Data",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          2,
          8,
          10,
          15,
          23,
          24,
          28,
          38,
          42,
          43,
          44,
          47,
          51,
          64,
          80,
          84,
          87,
          90,
          92,
          94,
          99,
          100,
          103,
          109,
          117,
          120,
          128,
          136,
          146,
          148,
          159,
          164,
          170,
          175,
          177,
          179,
          184,
          187,
          189,
          194,
          200,
          204,
          208,
          209,
          218,
          222,
          225,
          226,
          227,
          228,
          230,
          233,
          238,
          239,
          242,
          243,
          249,
          251,
          252,
          256,
          257,
          262,
          263,
          267,
          270,
          272,
          273,
          274,
          276,
          277,
          285,
          289,
          290,
          291,
          293,
          295,
          302,
          303,
          306,
          307,
          309,
          310,
          313,
          316,
          328,
          335,
          346,
          349,
          350,
          361,
          367,
          376,
          380,
          396,
          400,
          405,
          408,
          411,
          412,
          415,
          423,
          424,
          430,
          431,
          432,
          443,
          446,
          449,
          451,
          452,
          453,
          456,
          465,
          471,
          482,
          490,
          498,
          512,
          513,
          514,
          516,
          518,
          520,
          532,
          550,
          552,
          557,
          574,
          579,
          583,
          589,
          591,
          607,
          612,
          616,
          620,
          626,
          629,
          642,
          643,
          644,
          652,
          657,
          659,
          665,
          669,
          670,
          680,
          683,
          686,
          688,
          696,
          700,
          702,
          708,
          709,
          710,
          712,
          714,
          720,
          722,
          725,
          727,
          733,
          740,
          741,
          744,
          745,
          746,
          756,
          765,
          766,
          767,
          770,
          771,
          773,
          777,
          780,
          782,
          783,
          785,
          789,
          791,
          793,
          794,
          801,
          805,
          808,
          811,
          817,
          827,
          831,
          832,
          834,
          842,
          843,
          849,
          858,
          862,
          865,
          867,
          870,
          871,
          879,
          883,
          888,
          889,
          890,
          893,
          896,
          897,
          899,
          904,
          906,
          911,
          912,
          913,
          914,
          917,
          919,
          922,
          925,
          926,
          942,
          944,
          947,
          957,
          958,
          959,
          963,
          969,
          971,
          972,
          979,
          983,
          986,
          995,
          996,
          998,
          1001,
          1005,
          1007,
          1009,
          1016,
          1019,
          1023,
          1027,
          1029,
          1033,
          1036,
          1041,
          1042,
          1046,
          1049,
          1052,
          1054,
          1061,
          1065,
          1067,
          1074,
          1078,
          1082,
          1084,
          1087,
          1088,
          1104,
          1106,
          1107,
          1108,
          1114,
          1116,
          1119,
          1123,
          1126,
          1127,
          1138,
          1140,
          1149,
          1160,
          1161,
          1162,
          1171,
          1173,
          1179,
          1180,
          1182,
          1183,
          1185,
          1189,
          1196,
          1199,
          1204,
          1209,
          1215,
          1220,
          1224,
          1228,
          1229,
          1234,
          1235,
          1243,
          1254,
          1255,
          1258,
          1262,
          1264,
          1273,
          1274,
          1278,
          1290,
          1295,
          1298,
          1302,
          1304,
          1316,
          1318,
          1327,
          1328,
          1331,
          1334,
          1339,
          1344,
          1345,
          1346,
          1347,
          1353,
          1354,
          1358,
          1360,
          1362,
          1363,
          1367,
          1376,
          1385,
          1394,
          1396,
          1404,
          1405,
          1407,
          1415,
          1423,
          1424,
          1426,
          1432,
          1434,
          1438,
          1439,
          1441,
          1446,
          1452,
          1468,
          1470,
          1479,
          1482,
          1491,
          1492,
          1493,
          1502,
          1508,
          1511,
          1520,
          1522,
          1524,
          1528,
          1536,
          1537,
          1544,
          1545,
          1546,
          1551,
          1552,
          1557,
          1560,
          1561,
          1562,
          1563,
          1565,
          1567,
          1572,
          1576,
          1583,
          1588,
          1594,
          1597,
          1606,
          1609,
          1611,
          1616,
          1617,
          1629,
          1630,
          1631,
          1633,
          1635,
          1650,
          1652,
          1656,
          1657,
          1660,
          1668,
          1677,
          1678,
          1681,
          1692,
          1695,
          1697,
          1701,
          1704,
          1705,
          1706,
          1707,
          1710,
          1711,
          1712,
          1713,
          1714,
          1716,
          1717,
          1720,
          1721,
          1722,
          1723,
          1733,
          1737,
          1740,
          1741,
          1744,
          1745,
          1750,
          1752,
          1753,
          1756,
          1760,
          1769,
          1771,
          1772,
          1774,
          1780,
          1784,
          1785,
          1786,
          1792,
          1795,
          1797,
          1799,
          1800,
          1807,
          1808,
          1810,
          1814,
          1820,
          1821,
          1822,
          1827,
          1830,
          1831,
          1834,
          1835,
          1836,
          1839,
          1844,
          1846,
          1851,
          1852,
          1857,
          1861,
          1864
         ],
         "xaxis": "x",
         "y": [
          1.4552,
          2.9538,
          3.05,
          3.0553,
          3.0621,
          3.0519,
          3.0134,
          0.50015,
          0.98244,
          1.0598,
          1.0898,
          1.1008,
          1.1025,
          1.0968,
          1.0976,
          0.8627,
          0.535,
          0.82124,
          0.8921600000000001,
          0.9006200000000001,
          0.90507,
          0.904,
          0.90781,
          0.90636,
          0.8959,
          0.89243,
          0.88476,
          0.8936700000000001,
          0.9057200000000001,
          0.9067799999999999,
          0.90138,
          0.72556,
          0.7548699999999999,
          0.7842399999999999,
          0.7855,
          0.78492,
          0.7858,
          0.7858,
          0.7857,
          0.78364,
          0.7803800000000001,
          0.78008,
          0.77841,
          0.77761,
          0.77318,
          0.7723,
          0.7702899999999999,
          0.76764,
          0.7659699999999999,
          0.7648699999999999,
          0.76059,
          0.7593,
          0.75542,
          0.75364,
          0.7520899999999999,
          0.75271,
          0.75937,
          0.75942,
          0.76017,
          0.76658,
          0.76707,
          0.7723,
          0.77279,
          0.77496,
          0.7762399999999999,
          0.77674,
          0.77685,
          0.77772,
          0.77822,
          0.77969,
          0.7819699999999999,
          0.78452,
          0.78432,
          0.78492,
          0.78561,
          0.786,
          0.78601,
          0.7864899999999999,
          0.78432,
          0.78128,
          0.75536,
          0.7089,
          0.6539699999999999,
          1.6153,
          2.9172,
          2.9248,
          0.97016,
          0.64904,
          0.77644,
          1.1536,
          1.154,
          1.1538,
          1.1566,
          0.52267,
          0.84915,
          0.9469799999999999,
          0.95011,
          0.9528,
          0.9538399999999999,
          0.9538399999999999,
          0.9451,
          0.9436899999999999,
          0.93891,
          0.93606,
          0.9362299999999999,
          0.93855,
          0.93976,
          0.9451200000000001,
          0.9490700000000001,
          0.9499,
          0.95134,
          0.95321,
          0.9476100000000001,
          0.89731,
          0.81921,
          0.8252200000000001,
          0.82481,
          0.8186399999999999,
          0.81892,
          0.81788,
          0.81657,
          0.8157399999999999,
          0.8156,
          0.80937,
          0.79105,
          0.7925800000000001,
          0.79771,
          0.81415,
          0.8154600000000001,
          0.81686,
          0.81886,
          0.82004,
          0.8242799999999999,
          0.82155,
          0.81386,
          0.6339899999999999,
          1.9573,
          2.8378,
          3.6409,
          3.6437,
          3.6377,
          3.5357,
          2.3861,
          1.7347,
          0.92106,
          1.4611,
          1.5067,
          1.5303,
          1.5293,
          1.5263,
          1.5251,
          1.5312,
          1.5335,
          1.5278,
          1.3705,
          1.2324,
          1.0894,
          0.80479,
          0.5012,
          1.1299,
          1.2371,
          1.2497,
          1.2534,
          1.258,
          1.2562,
          1.2554,
          1.2487,
          1.2475,
          1.2448,
          1.232,
          1.2389,
          1.2393,
          1.2395,
          1.2429,
          1.2454,
          1.2499,
          1.2566,
          1.2589,
          1.26,
          1.2607,
          1.2585,
          1.2507,
          1.2117,
          1.0347,
          0.91755,
          0.85883,
          1.0777,
          1.0829,
          1.0834,
          1.0846,
          1.0811,
          1.0789,
          1.0787,
          1.0774,
          1.0732,
          1.0728,
          1.0686,
          1.0603,
          1.0513,
          1.0492,
          1.0452,
          1.0433,
          1.0418,
          1.042,
          1.0469,
          1.0546,
          1.0578,
          1.059,
          1.0649,
          1.0676,
          1.0674,
          1.0693,
          1.0735,
          1.074,
          1.0765,
          1.0774,
          1.0779,
          1.0776,
          1.0796,
          1.0806,
          1.0823,
          1.0841,
          1.0846,
          0.99729,
          0.8058,
          0.5017199999999999,
          3.9577,
          4.0259,
          4.041,
          4.0382,
          4.0465,
          4.0533,
          4.0506,
          2.5568,
          0.98345,
          0.7747,
          1.6006,
          1.6005,
          1.6001,
          1.6032,
          1.6016,
          1.5998,
          1.5966,
          1.6052,
          1.603,
          1.6017,
          1.5853,
          1.4478,
          0.86634,
          0.54188,
          1.1485,
          1.2399,
          1.3105,
          1.3151,
          1.3205,
          1.3212,
          1.3173,
          1.3067,
          1.3034,
          1.2945,
          1.2924,
          1.2997,
          1.3026,
          1.304,
          1.3052,
          1.3183,
          1.3138,
          1.3111,
          1.31,
          0.95988,
          0.7228,
          0.60868,
          1.0207,
          1.1282,
          1.1324,
          1.139,
          1.1388,
          1.1337,
          1.1275,
          1.1269,
          1.1266,
          1.1216,
          1.1199,
          1.1096,
          1.1069,
          1.1023,
          1.1025,
          1.1013,
          1.0946,
          1.0944,
          1.0994,
          1.1024,
          1.1146,
          1.121,
          1.1245,
          1.1268,
          1.1301,
          1.1301,
          1.132,
          1.1321,
          1.1364,
          1.1354,
          1.1347,
          1.1299,
          1.0221,
          0.83091,
          2.2026,
          2.394,
          2.614,
          2.6169,
          2.5994,
          2.3007,
          1.2188,
          0.56183,
          0.8327700000000001,
          0.8318399999999999,
          0.82934,
          0.82965,
          0.8302200000000001,
          0.8342700000000001,
          0.8345799999999999,
          0.83265,
          0.8269200000000001,
          0.81481,
          0.77994,
          0.66274,
          0.6765100000000001,
          0.68219,
          0.68376,
          0.68455,
          0.68533,
          0.68791,
          0.68258,
          0.67374,
          0.67237,
          0.67375,
          0.68083,
          0.68219,
          0.68555,
          0.68693,
          0.67788,
          0.66804,
          0.58855,
          0.59442,
          0.59665,
          0.59665,
          0.5965,
          0.5967899999999999,
          0.59531,
          0.59334,
          0.58934,
          0.58841,
          0.58368,
          0.57904,
          0.57326,
          0.5716399999999999,
          0.57213,
          0.57587,
          0.57953,
          0.58305,
          0.58885,
          0.58905,
          0.58919,
          0.59102,
          0.59299,
          0.59285,
          0.5956600000000001,
          0.59595,
          0.5963,
          0.5967899999999999,
          0.59718,
          0.5969399999999999,
          0.59595,
          0.59442,
          0.59053,
          0.58208,
          0.50716,
          0.91669,
          2.1216,
          2.2048,
          2.1967,
          2.2049,
          1.806,
          0.98703,
          0.87277,
          0.8712,
          0.87225,
          0.8710399999999999,
          0.8712700000000001,
          0.87284,
          0.8726799999999999,
          0.8723200000000001,
          0.8712700000000001,
          0.87255,
          0.71337,
          0.71529,
          0.71973,
          0.7203,
          0.7209800000000001,
          0.71834,
          0.71017,
          0.7089300000000001,
          0.70508,
          0.7104699999999999,
          0.71281,
          0.71668,
          0.7197,
          0.7205600000000001,
          0.7205600000000001,
          0.72027,
          0.7205600000000001,
          0.71917,
          0.71763,
          0.7168,
          0.71503,
          0.71532,
          0.7125100000000001,
          0.70881,
          0.5905,
          0.51078,
          0.5269699999999999,
          0.57772,
          0.62621,
          0.62634,
          0.62589,
          0.6258699999999999,
          0.62507,
          0.6248,
          0.6230899999999999,
          0.62231,
          0.6221800000000001,
          0.62138,
          0.6197600000000001,
          0.61741,
          0.6167199999999999,
          0.6164,
          0.6150899999999999,
          0.60818,
          0.6061,
          0.60541,
          0.60493,
          0.60028,
          0.6007600000000001,
          0.6025,
          0.60434,
          0.60482,
          0.61002,
          0.61165,
          0.6136,
          0.6158100000000001,
          0.61843,
          0.61856,
          0.61888,
          0.62051,
          0.6209899999999999,
          0.62097,
          0.62211,
          0.6221399999999999,
          0.62248,
          0.62342,
          0.62513,
          0.62502,
          0.62537,
          0.62513,
          0.62227,
          0.61416,
          0.50512
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=ANN<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "ANN",
         "line": {
          "color": "#EF553B",
          "dash": "solid"
         },
         "mode": "lines",
         "name": "ANN",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          2,
          8,
          10,
          15,
          23,
          24,
          28,
          38,
          42,
          43,
          44,
          47,
          51,
          64,
          80,
          84,
          87,
          90,
          92,
          94,
          99,
          100,
          103,
          109,
          117,
          120,
          128,
          136,
          146,
          148,
          159,
          164,
          170,
          175,
          177,
          179,
          184,
          187,
          189,
          194,
          200,
          204,
          208,
          209,
          218,
          222,
          225,
          226,
          227,
          228,
          230,
          233,
          238,
          239,
          242,
          243,
          249,
          251,
          252,
          256,
          257,
          262,
          263,
          267,
          270,
          272,
          273,
          274,
          276,
          277,
          285,
          289,
          290,
          291,
          293,
          295,
          302,
          303,
          306,
          307,
          309,
          310,
          313,
          316,
          328,
          335,
          346,
          349,
          350,
          361,
          367,
          376,
          380,
          396,
          400,
          405,
          408,
          411,
          412,
          415,
          423,
          424,
          430,
          431,
          432,
          443,
          446,
          449,
          451,
          452,
          453,
          456,
          465,
          471,
          482,
          490,
          498,
          512,
          513,
          514,
          516,
          518,
          520,
          532,
          550,
          552,
          557,
          574,
          579,
          583,
          589,
          591,
          607,
          612,
          616,
          620,
          626,
          629,
          642,
          643,
          644,
          652,
          657,
          659,
          665,
          669,
          670,
          680,
          683,
          686,
          688,
          696,
          700,
          702,
          708,
          709,
          710,
          712,
          714,
          720,
          722,
          725,
          727,
          733,
          740,
          741,
          744,
          745,
          746,
          756,
          765,
          766,
          767,
          770,
          771,
          773,
          777,
          780,
          782,
          783,
          785,
          789,
          791,
          793,
          794,
          801,
          805,
          808,
          811,
          817,
          827,
          831,
          832,
          834,
          842,
          843,
          849,
          858,
          862,
          865,
          867,
          870,
          871,
          879,
          883,
          888,
          889,
          890,
          893,
          896,
          897,
          899,
          904,
          906,
          911,
          912,
          913,
          914,
          917,
          919,
          922,
          925,
          926,
          942,
          944,
          947,
          957,
          958,
          959,
          963,
          969,
          971,
          972,
          979,
          983,
          986,
          995,
          996,
          998,
          1001,
          1005,
          1007,
          1009,
          1016,
          1019,
          1023,
          1027,
          1029,
          1033,
          1036,
          1041,
          1042,
          1046,
          1049,
          1052,
          1054,
          1061,
          1065,
          1067,
          1074,
          1078,
          1082,
          1084,
          1087,
          1088,
          1104,
          1106,
          1107,
          1108,
          1114,
          1116,
          1119,
          1123,
          1126,
          1127,
          1138,
          1140,
          1149,
          1160,
          1161,
          1162,
          1171,
          1173,
          1179,
          1180,
          1182,
          1183,
          1185,
          1189,
          1196,
          1199,
          1204,
          1209,
          1215,
          1220,
          1224,
          1228,
          1229,
          1234,
          1235,
          1243,
          1254,
          1255,
          1258,
          1262,
          1264,
          1273,
          1274,
          1278,
          1290,
          1295,
          1298,
          1302,
          1304,
          1316,
          1318,
          1327,
          1328,
          1331,
          1334,
          1339,
          1344,
          1345,
          1346,
          1347,
          1353,
          1354,
          1358,
          1360,
          1362,
          1363,
          1367,
          1376,
          1385,
          1394,
          1396,
          1404,
          1405,
          1407,
          1415,
          1423,
          1424,
          1426,
          1432,
          1434,
          1438,
          1439,
          1441,
          1446,
          1452,
          1468,
          1470,
          1479,
          1482,
          1491,
          1492,
          1493,
          1502,
          1508,
          1511,
          1520,
          1522,
          1524,
          1528,
          1536,
          1537,
          1544,
          1545,
          1546,
          1551,
          1552,
          1557,
          1560,
          1561,
          1562,
          1563,
          1565,
          1567,
          1572,
          1576,
          1583,
          1588,
          1594,
          1597,
          1606,
          1609,
          1611,
          1616,
          1617,
          1629,
          1630,
          1631,
          1633,
          1635,
          1650,
          1652,
          1656,
          1657,
          1660,
          1668,
          1677,
          1678,
          1681,
          1692,
          1695,
          1697,
          1701,
          1704,
          1705,
          1706,
          1707,
          1710,
          1711,
          1712,
          1713,
          1714,
          1716,
          1717,
          1720,
          1721,
          1722,
          1723,
          1733,
          1737,
          1740,
          1741,
          1744,
          1745,
          1750,
          1752,
          1753,
          1756,
          1760,
          1769,
          1771,
          1772,
          1774,
          1780,
          1784,
          1785,
          1786,
          1792,
          1795,
          1797,
          1799,
          1800,
          1807,
          1808,
          1810,
          1814,
          1820,
          1821,
          1822,
          1827,
          1830,
          1831,
          1834,
          1835,
          1836,
          1839,
          1844,
          1846,
          1851,
          1852,
          1857,
          1861,
          1864
         ],
         "xaxis": "x",
         "y": [
          1.3489694595336914,
          2.1722378730773926,
          2.371692180633545,
          2.7939095497131348,
          2.92510724067688,
          2.926978588104248,
          2.826322555541992,
          0.8751901984214783,
          1.0255355834960938,
          1.0596016645431519,
          1.076078176498413,
          1.0787312984466553,
          1.0822633504867554,
          1.114633321762085,
          1.0831453800201416,
          1.0796149969100952,
          0.9104669094085693,
          0.9106797575950623,
          0.9108218550682068,
          0.9109637141227722,
          0.9113184213638306,
          0.9113894104957581,
          0.9120272994041443,
          0.9133017063140869,
          0.9149972796440125,
          0.9156322479248047,
          0.9173226356506348,
          0.9164780378341675,
          0.9143619537353516,
          0.9139379858970642,
          0.9116020798683167,
          0.9111056327819824,
          0.7878611087799072,
          0.7876328825950623,
          0.7875415682792664,
          0.7874504327774048,
          0.7867668271064758,
          0.7863571047782898,
          0.7860842943191528,
          0.785402774810791,
          0.7845863103866577,
          0.7840428948402405,
          0.7835002541542053,
          0.7833645939826965,
          0.7821462154388428,
          0.7816057801246643,
          0.7812009453773499,
          0.7810660600662231,
          0.7809311747550964,
          0.7807964086532593,
          0.7805269360542297,
          0.7801230549812317,
          0.7794510126113892,
          0.7793168425559998,
          0.7789142727851868,
          0.77886962890625,
          0.7795853614807129,
          0.7798541188240051,
          0.7799886465072632,
          0.7805269360542297,
          0.7806615829467773,
          0.7813358902931213,
          0.781470775604248,
          0.7820110321044922,
          0.7824166417121887,
          0.782687246799469,
          0.7828226089477539,
          0.7829581499099731,
          0.7832291722297668,
          0.7833645939826965,
          0.7844505310058594,
          0.78499436378479,
          0.7851304411888123,
          0.785266637802124,
          0.7855389714241028,
          0.7858115434646606,
          0.7867668271064758,
          0.7869035005569458,
          0.7873136401176453,
          0.7874504327774048,
          0.7875415682792664,
          0.7875872850418091,
          1.2088011503219604,
          1.5969455242156982,
          2.89963698387146,
          2.6881539821624756,
          1.319711685180664,
          0.9474200010299683,
          0.9853031039237976,
          1.1515371799468994,
          1.1646325588226318,
          1.1668012142181396,
          1.1581027507781982,
          0.8833231925964355,
          0.9225473999977112,
          0.9228243231773376,
          0.9239165782928467,
          0.9280393719673157,
          0.9294143319129944,
          0.9335275888442993,
          0.9444087147712708,
          0.9457600116729736,
          0.9538262486457825,
          0.955163836479187,
          0.956499457359314,
          0.9524868130683899,
          0.9484565854072571,
          0.9444087147712708,
          0.941700279712677,
          0.9403430223464966,
          0.9389839768409729,
          0.9348946213722229,
          0.9234489798545837,
          0.9225053787231445,
          0.8353671431541443,
          0.8330814838409424,
          0.8300453424453735,
          0.8247628808021545,
          0.8243871331214905,
          0.8240115642547607,
          0.8232608437538147,
          0.8225110173225403,
          0.8217619061470032,
          0.8172841668128967,
          0.8113571405410767,
          0.8120952248573303,
          0.8139440417289734,
          0.8202662467956543,
          0.822136402130127,
          0.8236360549926758,
          0.8258915543556213,
          0.8266449570655823,
          0.8327013254165649,
          0.8346045017242432,
          0.8353671431541443,
          0.8358760476112366,
          1.8680931329727173,
          2.373389959335327,
          3.5571353435516357,
          3.5755772590637207,
          3.5861916542053223,
          3.505262613296509,
          3.1840291023254395,
          2.9627561569213867,
          1.0718170404434204,
          1.28632390499115,
          1.3396053314208984,
          1.5178130865097046,
          1.5273535251617432,
          1.5368560552597046,
          1.542413592338562,
          1.542413592338562,
          1.5305253267288208,
          1.5241776704788208,
          1.5093004703521729,
          1.4933196306228638,
          1.4632893800735474,
          1.4030207395553589,
          1.3396053314208984,
          1.2471665143966675,
          1.2475886344909668,
          1.2482216358184814,
          1.2486435174942017,
          1.251173496246338,
          1.2555948495864868,
          1.256225824356079,
          1.258117914199829,
          1.258748173713684,
          1.259378433227539,
          1.2656718492507935,
          1.2650431394577026,
          1.2644143104553223,
          1.263785481452942,
          1.2618975639343262,
          1.261268138885498,
          1.260008454322815,
          1.257487416267395,
          1.2555948495864868,
          1.254332423210144,
          1.2537009716033936,
          1.2524374723434448,
          1.249908685684204,
          1.2490652799606323,
          1.2486435174942017,
          1.2484325170516968,
          1.0838290452957153,
          1.0834959745407104,
          1.083246111869812,
          1.0829962491989136,
          1.0816630125045776,
          1.0791605710983276,
          1.0781588554382324,
          1.0779082775115967,
          1.0774071216583252,
          1.0754010677337646,
          1.0751502513885498,
          1.0736441612243652,
          1.0713828802108765,
          1.070376992225647,
          1.069622278213501,
          1.0691190958023071,
          1.0683637857437134,
          1.0681118965148926,
          1.0676082372665405,
          1.068615436553955,
          1.0698739290237427,
          1.0701254606246948,
          1.070376992225647,
          1.0711314678192139,
          1.0718857049942017,
          1.0721368789672852,
          1.0726394653320312,
          1.0738953351974487,
          1.0743974447250366,
          1.0756518840789795,
          1.0759029388427734,
          1.0761535167694092,
          1.0764044523239136,
          1.0771565437316895,
          1.077657699584961,
          1.0784093141555786,
          1.0791605710983276,
          1.0794110298156738,
          1.0830795764923096,
          1.083246111869812,
          1.0834959745407104,
          3.2042644023895264,
          3.3747832775115967,
          3.5243496894836426,
          4.030196666717529,
          4.030196666717529,
          3.8035778999328613,
          3.667365312576294,
          2.3642358779907227,
          1.410412073135376,
          0.977009117603302,
          1.4752275943756104,
          1.5116591453552246,
          1.57150137424469,
          1.5950523614883423,
          1.6207568645477295,
          1.6303867101669312,
          1.635500431060791,
          1.6158065795898438,
          1.5950523614883423,
          1.563462495803833,
          1.4022102355957031,
          1.2908506393432617,
          1.0397911071777344,
          1.3009076118469238,
          1.303430438041687,
          1.3039345741271973,
          1.305950403213501,
          1.3094732761383057,
          1.31399405002594,
          1.317002773284912,
          1.3274993896484375,
          1.3334742784500122,
          1.3364553451538086,
          1.3468562364578247,
          1.3498185873031616,
          1.3438897132873535,
          1.3409191370010376,
          1.3364553451538086,
          1.3349652290344238,
          1.3109811544418335,
          1.3079642057418823,
          1.306454062461853,
          1.305950403213501,
          1.3029260635375977,
          1.3019171953201294,
          1.1477570533752441,
          1.1475869417190552,
          1.1474592685699463,
          1.1474167108535767,
          1.1461812257766724,
          1.1459251642227173,
          1.1434468030929565,
          1.1371735334396362,
          1.1366021633148193,
          1.1360307931900024,
          1.130879521369934,
          1.1297333240509033,
          1.1262907981872559,
          1.1257166862487793,
          1.1245678663253784,
          1.1239932775497437,
          1.122843623161316,
          1.1205427646636963,
          1.1199672222137451,
          1.12169349193573,
          1.1245678663253784,
          1.1274387836456299,
          1.130879521369934,
          1.1337429285049438,
          1.1360307931900024,
          1.1383156776428223,
          1.1388864517211914,
          1.141737937927246,
          1.1423077583312988,
          1.1455410718917847,
          1.1469484567642212,
          1.14707612991333,
          1.147374153137207,
          1.1475443840026855,
          1.1476294994354248,
          1.4079689979553223,
          1.4921408891677856,
          1.8767503499984741,
          2.484039306640625,
          2.3270111083984375,
          2.0681045055389404,
          1.678504228591919,
          0.8138205409049988,
          0.8196969628334045,
          0.8232535719871521,
          0.8415554165840149,
          0.8463862538337708,
          0.8767416477203369,
          0.8373695015907288,
          0.8285678625106812,
          0.8196969628334045,
          0.8191030025482178,
          0.8185088038444519,
          0.8179143071174622,
          0.6849040985107422,
          0.6848509907722473,
          0.6846387386322021,
          0.684532642364502,
          0.684426486492157,
          0.6842673420906067,
          0.6836311221122742,
          0.6822015047073364,
          0.6807745695114136,
          0.6799829006195068,
          0.6802994608879089,
          0.6815669536590576,
          0.6817255616188049,
          0.6820428371429443,
          0.6833131909370422,
          0.6844794750213623,
          0.684532642364502,
          0.6846387386322021,
          0.5928082466125488,
          0.592524528503418,
          0.5919575095176697,
          0.5918158292770386,
          0.5915325284004211,
          0.5908246040344238,
          0.589975893497467,
          0.587716817855835,
          0.5874348282814026,
          0.5861672759056091,
          0.5857452154159546,
          0.5844801068305969,
          0.5843396782875061,
          0.5841993093490601,
          0.5846206545829773,
          0.5854639410972595,
          0.5858858227729797,
          0.5871530771255493,
          0.5874348282814026,
          0.587716817855835,
          0.5882810354232788,
          0.5894105434417725,
          0.5895518064498901,
          0.5905415415763855,
          0.590683102607727,
          0.5908246040344238,
          0.5915325284004211,
          0.5916742086410522,
          0.5923827886581421,
          0.5926664471626282,
          0.5928082466125488,
          0.5928555727005005,
          0.5929028391838074,
          0.5929974317550659,
          0.966635525226593,
          1.2785089015960693,
          1.7698358297348022,
          2.193575382232666,
          1.7698358297348022,
          1.0861636400222778,
          0.966635525226593,
          0.8509452939033508,
          0.8553227186203003,
          0.8590647578239441,
          0.8683784008026123,
          0.870233952999115,
          0.8627972602844238,
          0.8609322309494019,
          0.8590647578239441,
          0.8553226590156555,
          0.8515713810920715,
          0.7192452549934387,
          0.7190156579017639,
          0.7178689241409302,
          0.7175252437591553,
          0.7164950966835022,
          0.7137547731399536,
          0.7106835246086121,
          0.7103430032730103,
          0.7093223929405212,
          0.7117058634757996,
          0.7127296328544617,
          0.713412880897522,
          0.7147811651229858,
          0.7158091068267822,
          0.716152012348175,
          0.7164950966835022,
          0.7168383002281189,
          0.7178689241409302,
          0.7182127237319946,
          0.7185567617416382,
          0.7189009785652161,
          0.7190156579017639,
          0.7192452549934387,
          0.7193600535392761,
          0.7197045087814331,
          0.7198194265365601,
          0.6248517036437988,
          0.6247498989105225,
          0.6229214072227478,
          0.621704638004303,
          0.620793342590332,
          0.6204898357391357,
          0.6195797920227051,
          0.6192767024040222,
          0.6177629828453064,
          0.6171581745147705,
          0.616856038570404,
          0.6159501075744629,
          0.6147437691688538,
          0.6120362281799316,
          0.6114357709884644,
          0.6111356616020203,
          0.6105358600616455,
          0.6087391376495361,
          0.6075435280799866,
          0.6072448492050171,
          0.6069462895393372,
          0.6051574945449829,
          0.6054553389549255,
          0.6060514450073242,
          0.6066479086875916,
          0.6069462895393372,
          0.6090382933616638,
          0.6093376278877258,
          0.609936535358429,
          0.6111356616020203,
          0.6129377484321594,
          0.6132384538650513,
          0.6135393381118774,
          0.6150451898574829,
          0.6159501075744629,
          0.6162519454956055,
          0.6171581745147705,
          0.6174604892730713,
          0.6177629828453064,
          0.6186708211898804,
          0.6201862692832947,
          0.620793342590332,
          0.6223127841949463,
          0.6226170063018799,
          0.6241399645805359,
          0.6245465874671936,
          0.6248517036437988
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=RFC<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "RFC",
         "line": {
          "color": "#00cc96",
          "dash": "solid"
         },
         "mode": "lines",
         "name": "RFC",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          2,
          8,
          10,
          15,
          23,
          24,
          28,
          38,
          42,
          43,
          44,
          47,
          51,
          64,
          80,
          84,
          87,
          90,
          92,
          94,
          99,
          100,
          103,
          109,
          117,
          120,
          128,
          136,
          146,
          148,
          159,
          164,
          170,
          175,
          177,
          179,
          184,
          187,
          189,
          194,
          200,
          204,
          208,
          209,
          218,
          222,
          225,
          226,
          227,
          228,
          230,
          233,
          238,
          239,
          242,
          243,
          249,
          251,
          252,
          256,
          257,
          262,
          263,
          267,
          270,
          272,
          273,
          274,
          276,
          277,
          285,
          289,
          290,
          291,
          293,
          295,
          302,
          303,
          306,
          307,
          309,
          310,
          313,
          316,
          328,
          335,
          346,
          349,
          350,
          361,
          367,
          376,
          380,
          396,
          400,
          405,
          408,
          411,
          412,
          415,
          423,
          424,
          430,
          431,
          432,
          443,
          446,
          449,
          451,
          452,
          453,
          456,
          465,
          471,
          482,
          490,
          498,
          512,
          513,
          514,
          516,
          518,
          520,
          532,
          550,
          552,
          557,
          574,
          579,
          583,
          589,
          591,
          607,
          612,
          616,
          620,
          626,
          629,
          642,
          643,
          644,
          652,
          657,
          659,
          665,
          669,
          670,
          680,
          683,
          686,
          688,
          696,
          700,
          702,
          708,
          709,
          710,
          712,
          714,
          720,
          722,
          725,
          727,
          733,
          740,
          741,
          744,
          745,
          746,
          756,
          765,
          766,
          767,
          770,
          771,
          773,
          777,
          780,
          782,
          783,
          785,
          789,
          791,
          793,
          794,
          801,
          805,
          808,
          811,
          817,
          827,
          831,
          832,
          834,
          842,
          843,
          849,
          858,
          862,
          865,
          867,
          870,
          871,
          879,
          883,
          888,
          889,
          890,
          893,
          896,
          897,
          899,
          904,
          906,
          911,
          912,
          913,
          914,
          917,
          919,
          922,
          925,
          926,
          942,
          944,
          947,
          957,
          958,
          959,
          963,
          969,
          971,
          972,
          979,
          983,
          986,
          995,
          996,
          998,
          1001,
          1005,
          1007,
          1009,
          1016,
          1019,
          1023,
          1027,
          1029,
          1033,
          1036,
          1041,
          1042,
          1046,
          1049,
          1052,
          1054,
          1061,
          1065,
          1067,
          1074,
          1078,
          1082,
          1084,
          1087,
          1088,
          1104,
          1106,
          1107,
          1108,
          1114,
          1116,
          1119,
          1123,
          1126,
          1127,
          1138,
          1140,
          1149,
          1160,
          1161,
          1162,
          1171,
          1173,
          1179,
          1180,
          1182,
          1183,
          1185,
          1189,
          1196,
          1199,
          1204,
          1209,
          1215,
          1220,
          1224,
          1228,
          1229,
          1234,
          1235,
          1243,
          1254,
          1255,
          1258,
          1262,
          1264,
          1273,
          1274,
          1278,
          1290,
          1295,
          1298,
          1302,
          1304,
          1316,
          1318,
          1327,
          1328,
          1331,
          1334,
          1339,
          1344,
          1345,
          1346,
          1347,
          1353,
          1354,
          1358,
          1360,
          1362,
          1363,
          1367,
          1376,
          1385,
          1394,
          1396,
          1404,
          1405,
          1407,
          1415,
          1423,
          1424,
          1426,
          1432,
          1434,
          1438,
          1439,
          1441,
          1446,
          1452,
          1468,
          1470,
          1479,
          1482,
          1491,
          1492,
          1493,
          1502,
          1508,
          1511,
          1520,
          1522,
          1524,
          1528,
          1536,
          1537,
          1544,
          1545,
          1546,
          1551,
          1552,
          1557,
          1560,
          1561,
          1562,
          1563,
          1565,
          1567,
          1572,
          1576,
          1583,
          1588,
          1594,
          1597,
          1606,
          1609,
          1611,
          1616,
          1617,
          1629,
          1630,
          1631,
          1633,
          1635,
          1650,
          1652,
          1656,
          1657,
          1660,
          1668,
          1677,
          1678,
          1681,
          1692,
          1695,
          1697,
          1701,
          1704,
          1705,
          1706,
          1707,
          1710,
          1711,
          1712,
          1713,
          1714,
          1716,
          1717,
          1720,
          1721,
          1722,
          1723,
          1733,
          1737,
          1740,
          1741,
          1744,
          1745,
          1750,
          1752,
          1753,
          1756,
          1760,
          1769,
          1771,
          1772,
          1774,
          1780,
          1784,
          1785,
          1786,
          1792,
          1795,
          1797,
          1799,
          1800,
          1807,
          1808,
          1810,
          1814,
          1820,
          1821,
          1822,
          1827,
          1830,
          1831,
          1834,
          1835,
          1836,
          1839,
          1844,
          1846,
          1851,
          1852,
          1857,
          1861,
          1864
         ],
         "xaxis": "x",
         "y": [
          1.3094181333333332,
          1.401216766666666,
          1.9107649533333322,
          2.9500445000000015,
          3.0550799999999985,
          3.0538039999999995,
          3.028871000000005,
          0.7208122999999995,
          0.9273881499999989,
          1.0150219999999985,
          1.0161109666666652,
          0.8035218499999996,
          1.0912144999999995,
          1.0987484999999997,
          1.0988045000000008,
          1.0256151499999993,
          0.6864948999999997,
          0.7995180999999993,
          0.8688317999999995,
          0.6613880066666669,
          0.88966715,
          0.8944477499999992,
          0.9064411566666667,
          0.9072646666666658,
          0.9006188666666657,
          0.8949801299999995,
          0.8889155400000008,
          0.8917658533333339,
          0.9037780333333341,
          0.9050451766666656,
          0.9044799416666671,
          0.852347233333334,
          0.7397316750000003,
          0.6662541033333342,
          0.7772498735714285,
          0.7774881685714286,
          0.7859087895238095,
          0.7865771416666664,
          0.7858829000000007,
          0.7851583999999999,
          0.7816746200000004,
          0.7806319833333331,
          0.7798011899999993,
          0.7780655549999999,
          0.7749585049999997,
          0.772833608333334,
          0.7717201999999999,
          0.7707394999999999,
          0.7699182000000007,
          0.7671069000000006,
          0.7619218804761916,
          0.7600121299999993,
          0.7567982666666666,
          0.7554196400000006,
          0.7534246200000001,
          0.7534246200000001,
          0.7563596000000001,
          0.7580908450000003,
          0.7585435999999998,
          0.7619218804761916,
          0.7640464533333337,
          0.7713873999999994,
          0.7715022000000001,
          0.7738142516666666,
          0.7753473499999999,
          0.7757435500000004,
          0.7764044499999991,
          0.7766757249999994,
          0.7771517750000001,
          0.7780655549999999,
          0.7804474333333334,
          0.7825562499999991,
          0.7827423999999995,
          0.7830811999999997,
          0.7849050749999997,
          0.7850607100000003,
          0.7859087895238095,
          0.7863803399999996,
          0.7854338369047624,
          0.7774881685714286,
          0.7772498735714285,
          0.7543359700000002,
          0.8118640333333338,
          1.695640316666665,
          2.9192608333333334,
          2.9248977000000043,
          1.0302033499999987,
          0.6984704333333346,
          0.7787229333333343,
          1.1537099499999985,
          1.154280583333332,
          1.1535505000000004,
          1.1567671666666648,
          0.6083723333333348,
          0.8372739249999998,
          0.9450349733333344,
          0.949510879999999,
          0.9526794166666666,
          0.952777539999999,
          0.9530254166666668,
          0.9454299488095239,
          0.9422587804761904,
          0.9384764999999992,
          0.9368324599999991,
          0.9344630600000002,
          0.9395669690476199,
          0.9401593366666674,
          0.9454299488095239,
          0.9490491166666658,
          0.9510561083333338,
          0.9517304083333343,
          0.9531639666666675,
          0.9483839333333343,
          0.9047878000000007,
          0.8127374040476194,
          0.8239384599999994,
          0.8232350416666668,
          0.818707376666666,
          0.8180539999999995,
          0.8178615999999991,
          0.8164430666666677,
          0.8157832166666684,
          0.8155231000000009,
          0.8084484466666662,
          0.7913401000000004,
          0.7922928800000002,
          0.7981339666666675,
          0.8142640750000005,
          0.8157985166666681,
          0.8173780000000005,
          0.8196050066666678,
          0.8204046866666665,
          0.824742906666666,
          0.823013616666667,
          0.8127374040476194,
          0.6673215583333332,
          1.6013775666666654,
          2.2205743366666657,
          3.6458435000000007,
          3.647417999999995,
          3.6445406666666598,
          3.6073274000000053,
          3.4061713333333348,
          3.174746891666666,
          0.9807800000000008,
          1.291868603333333,
          1.1294004883333335,
          1.5288328333333325,
          1.5304148333333323,
          1.5322713333333342,
          1.5284213333333332,
          1.5284213333333332,
          1.531105333333334,
          1.5313037499999984,
          1.5228108214285723,
          1.5281393333333342,
          1.5193627000000007,
          1.4449228166666659,
          1.1294004883333335,
          1.0566218,
          0.9638290190476191,
          0.8799728449999994,
          1.218888616666665,
          1.2561165095238087,
          1.2577939,
          1.2576364999999992,
          1.2554999999999983,
          1.2547896666666651,
          1.2518176666666685,
          1.2349297333333347,
          1.2334565333333334,
          1.2344193333333342,
          1.2358170833333326,
          1.239111166666664,
          1.240069499999998,
          1.245028666666666,
          1.2542086333333318,
          1.2577939,
          1.25775085,
          1.2582950833333346,
          1.2589427666666655,
          1.2546563333333351,
          1.242419166666666,
          1.218888616666665,
          1.1303227249999988,
          0.8470383000000007,
          0.9325239333333342,
          0.9765731376190473,
          1.0578793999999994,
          1.0850823333333321,
          1.081931199999999,
          1.0812200000000012,
          1.0805410000000009,
          1.0789059999999997,
          1.0752059999999999,
          1.0750334666666654,
          1.0707136666666655,
          1.0643716666666685,
          1.0519164666666678,
          1.0520161333333333,
          1.0488384999999996,
          1.0464049999999991,
          1.0457359523809542,
          1.039436933333335,
          1.0451145666666655,
          1.050312800000001,
          1.0503940000000014,
          1.0519164666666678,
          1.0605906666666653,
          1.0648214666666656,
          1.0655295000000005,
          1.0670900000000005,
          1.070843833333334,
          1.0713250833333325,
          1.0742459999999996,
          1.0743430000000007,
          1.0751573333333344,
          1.075946600000001,
          1.0768646000000013,
          1.0788209999999985,
          1.0801800000000021,
          1.081931199999999,
          1.0830354333333325,
          1.0570601633333323,
          0.9765731376190473,
          0.9325239333333342,
          3.8707651833333303,
          3.988005249999995,
          4.0233972499999995,
          4.043914483333336,
          4.043914483333336,
          4.046093750000003,
          4.041102000000003,
          2.784534133333336,
          1.1269039500000004,
          0.7155098666666668,
          1.5994768333333333,
          1.6005083333333325,
          1.6005178,
          1.6028841833333334,
          1.6019969999999975,
          1.598720478571429,
          1.5946484999999992,
          1.6030049166666642,
          1.6028841833333334,
          1.6005909999999994,
          1.5863885000000015,
          1.4605852499999998,
          0.9376653000000009,
          0.6606874638095239,
          1.0788829499999988,
          1.1915149000000016,
          1.3095960000000004,
          1.3166784999999976,
          1.3208891666666676,
          1.3213838666666675,
          1.3174533333333327,
          1.3072015833333341,
          1.3034390000000005,
          1.2944651666666667,
          1.293118166666666,
          1.2987923000000012,
          1.3016290500000016,
          1.3034390000000005,
          1.3050187500000012,
          1.3184799999999983,
          1.3131657499999998,
          1.310479250000001,
          1.3095960000000004,
          0.9889325733333326,
          0.775764589999999,
          0.565720933333333,
          0.9478961000000011,
          1.1062024666666677,
          1.1272429999999993,
          1.1376153000000013,
          1.1371489666666674,
          1.132945933333333,
          1.1277965000000005,
          1.1271180000000012,
          1.126497000000001,
          1.1202064999999994,
          1.1196428333333344,
          1.1095486666666685,
          1.1066750000000014,
          1.1038041428571423,
          1.1026762000000006,
          1.1010328666666647,
          1.0957873499999984,
          1.0933331666666657,
          1.099091499999998,
          1.1038041428571423,
          1.1146148857142861,
          1.1202064999999994,
          1.124689666666666,
          1.126497000000001,
          1.129128583333333,
          1.1294598333333319,
          1.1321599999999983,
          1.1324995666666644,
          1.1379623999999982,
          1.1383634999999983,
          1.1378969999999997,
          1.1321787500000007,
          1.0418638916666663,
          0.9049445000000016,
          1.883504159999999,
          1.7743486183333332,
          1.9765114166666677,
          2.61398513333333,
          2.6110427809523844,
          2.5179562500000032,
          2.4001137066666645,
          0.7097960000000003,
          0.8323271933333338,
          0.83258262,
          0.8311260000000006,
          0.8305476000000005,
          0.8297200000000001,
          0.8309346466666669,
          0.8339965749999995,
          0.8323271933333338,
          0.8323688000000005,
          0.8320283800000005,
          0.8323385800000002,
          0.6207107,
          0.6759209000000009,
          0.6251605050000003,
          0.6711099899999996,
          0.6795084749999998,
          0.6833902833333336,
          0.6869516249999991,
          0.6847748499999997,
          0.6763649600000002,
          0.6702243000000006,
          0.6711742100000009,
          0.6777188,
          0.6784222066666673,
          0.6810976809523812,
          0.6865686833333337,
          0.6794026749999987,
          0.6711099899999996,
          0.6251605050000003,
          0.5841725000000004,
          0.5964832000000007,
          0.5972331499999999,
          0.597157199999999,
          0.5963255333333329,
          0.5959951095238091,
          0.5942334199999995,
          0.589034703333334,
          0.5885392000000004,
          0.5850634000000008,
          0.5811914583333336,
          0.5745745566666653,
          0.5744277733333326,
          0.5732861150000006,
          0.5744439416666657,
          0.5779129666666664,
          0.5813383733333335,
          0.5880239333333328,
          0.5885392000000004,
          0.589034703333334,
          0.5895310249999995,
          0.5918379599999994,
          0.5921995666666672,
          0.5951252699999992,
          0.5950126833333326,
          0.5959951095238091,
          0.5963255333333329,
          0.5966989083333325,
          0.5962086,
          0.5958740000000007,
          0.5841725000000004,
          0.5817622000000005,
          0.5782976999999999,
          0.5274234999999997,
          1.1222315333333335,
          2.1332803333333303,
          2.203584199999998,
          2.2113717500000023,
          2.203584199999998,
          1.8025719333333337,
          1.1222315333333335,
          0.8728429566666661,
          0.8717840983333329,
          0.8725241716666666,
          0.872275625,
          0.8704514749999993,
          0.873406516666666,
          0.8731160666666669,
          0.8725241716666666,
          0.8717840983333329,
          0.8728159833333342,
          0.711963028333333,
          0.7148486452380952,
          0.7189499699999995,
          0.7197300099999999,
          0.7211828000000006,
          0.7180561500000004,
          0.7099847549999997,
          0.7088781666666663,
          0.7050347150000001,
          0.7107607576190484,
          0.713839475,
          0.7157778250000004,
          0.7201154933333328,
          0.72116387,
          0.7212159166666668,
          0.7211828000000006,
          0.7208729700000005,
          0.7189499699999995,
          0.7182113999999994,
          0.7172856999999996,
          0.7151784369047621,
          0.7148486452380952,
          0.711963028333333,
          0.7036361500000002,
          0.605419504999999,
          0.5389833333333339,
          0.5775351866666674,
          0.5775351866666674,
          0.6251295849999997,
          0.626084315000001,
          0.6254835464285713,
          0.6254568964285709,
          0.6248384383333327,
          0.6244685499999995,
          0.622909721666667,
          0.6220391833333322,
          0.6217705083333323,
          0.6210828633333336,
          0.6201866449999988,
          0.617202379999999,
          0.6166044333333341,
          0.6160204166666682,
          0.6152843250000005,
          0.6082889433333323,
          0.6058262333333336,
          0.6058324999999997,
          0.6049286599999993,
          0.6001743999999994,
          0.6003523999999991,
          0.6017227750000005,
          0.6042056599999994,
          0.6049286599999993,
          0.6094211999999993,
          0.611711,
          0.6137341950000001,
          0.6160204166666682,
          0.6183994049999993,
          0.6187311833333327,
          0.6187923499999991,
          0.6201797200000001,
          0.6210828633333336,
          0.6214057833333336,
          0.6220391833333322,
          0.622568766666666,
          0.622909721666667,
          0.624089775,
          0.625653681666666,
          0.6254835464285713,
          0.62617374,
          0.6259124999999994,
          0.623691400000001,
          0.6141613166666658,
          0.5775351866666674
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=GBC<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "GBC",
         "line": {
          "color": "#ab63fa",
          "dash": "solid"
         },
         "mode": "lines",
         "name": "GBC",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          2,
          8,
          10,
          15,
          23,
          24,
          28,
          38,
          42,
          43,
          44,
          47,
          51,
          64,
          80,
          84,
          87,
          90,
          92,
          94,
          99,
          100,
          103,
          109,
          117,
          120,
          128,
          136,
          146,
          148,
          159,
          164,
          170,
          175,
          177,
          179,
          184,
          187,
          189,
          194,
          200,
          204,
          208,
          209,
          218,
          222,
          225,
          226,
          227,
          228,
          230,
          233,
          238,
          239,
          242,
          243,
          249,
          251,
          252,
          256,
          257,
          262,
          263,
          267,
          270,
          272,
          273,
          274,
          276,
          277,
          285,
          289,
          290,
          291,
          293,
          295,
          302,
          303,
          306,
          307,
          309,
          310,
          313,
          316,
          328,
          335,
          346,
          349,
          350,
          361,
          367,
          376,
          380,
          396,
          400,
          405,
          408,
          411,
          412,
          415,
          423,
          424,
          430,
          431,
          432,
          443,
          446,
          449,
          451,
          452,
          453,
          456,
          465,
          471,
          482,
          490,
          498,
          512,
          513,
          514,
          516,
          518,
          520,
          532,
          550,
          552,
          557,
          574,
          579,
          583,
          589,
          591,
          607,
          612,
          616,
          620,
          626,
          629,
          642,
          643,
          644,
          652,
          657,
          659,
          665,
          669,
          670,
          680,
          683,
          686,
          688,
          696,
          700,
          702,
          708,
          709,
          710,
          712,
          714,
          720,
          722,
          725,
          727,
          733,
          740,
          741,
          744,
          745,
          746,
          756,
          765,
          766,
          767,
          770,
          771,
          773,
          777,
          780,
          782,
          783,
          785,
          789,
          791,
          793,
          794,
          801,
          805,
          808,
          811,
          817,
          827,
          831,
          832,
          834,
          842,
          843,
          849,
          858,
          862,
          865,
          867,
          870,
          871,
          879,
          883,
          888,
          889,
          890,
          893,
          896,
          897,
          899,
          904,
          906,
          911,
          912,
          913,
          914,
          917,
          919,
          922,
          925,
          926,
          942,
          944,
          947,
          957,
          958,
          959,
          963,
          969,
          971,
          972,
          979,
          983,
          986,
          995,
          996,
          998,
          1001,
          1005,
          1007,
          1009,
          1016,
          1019,
          1023,
          1027,
          1029,
          1033,
          1036,
          1041,
          1042,
          1046,
          1049,
          1052,
          1054,
          1061,
          1065,
          1067,
          1074,
          1078,
          1082,
          1084,
          1087,
          1088,
          1104,
          1106,
          1107,
          1108,
          1114,
          1116,
          1119,
          1123,
          1126,
          1127,
          1138,
          1140,
          1149,
          1160,
          1161,
          1162,
          1171,
          1173,
          1179,
          1180,
          1182,
          1183,
          1185,
          1189,
          1196,
          1199,
          1204,
          1209,
          1215,
          1220,
          1224,
          1228,
          1229,
          1234,
          1235,
          1243,
          1254,
          1255,
          1258,
          1262,
          1264,
          1273,
          1274,
          1278,
          1290,
          1295,
          1298,
          1302,
          1304,
          1316,
          1318,
          1327,
          1328,
          1331,
          1334,
          1339,
          1344,
          1345,
          1346,
          1347,
          1353,
          1354,
          1358,
          1360,
          1362,
          1363,
          1367,
          1376,
          1385,
          1394,
          1396,
          1404,
          1405,
          1407,
          1415,
          1423,
          1424,
          1426,
          1432,
          1434,
          1438,
          1439,
          1441,
          1446,
          1452,
          1468,
          1470,
          1479,
          1482,
          1491,
          1492,
          1493,
          1502,
          1508,
          1511,
          1520,
          1522,
          1524,
          1528,
          1536,
          1537,
          1544,
          1545,
          1546,
          1551,
          1552,
          1557,
          1560,
          1561,
          1562,
          1563,
          1565,
          1567,
          1572,
          1576,
          1583,
          1588,
          1594,
          1597,
          1606,
          1609,
          1611,
          1616,
          1617,
          1629,
          1630,
          1631,
          1633,
          1635,
          1650,
          1652,
          1656,
          1657,
          1660,
          1668,
          1677,
          1678,
          1681,
          1692,
          1695,
          1697,
          1701,
          1704,
          1705,
          1706,
          1707,
          1710,
          1711,
          1712,
          1713,
          1714,
          1716,
          1717,
          1720,
          1721,
          1722,
          1723,
          1733,
          1737,
          1740,
          1741,
          1744,
          1745,
          1750,
          1752,
          1753,
          1756,
          1760,
          1769,
          1771,
          1772,
          1774,
          1780,
          1784,
          1785,
          1786,
          1792,
          1795,
          1797,
          1799,
          1800,
          1807,
          1808,
          1810,
          1814,
          1820,
          1821,
          1822,
          1827,
          1830,
          1831,
          1834,
          1835,
          1836,
          1839,
          1844,
          1846,
          1851,
          1852,
          1857,
          1861,
          1864
         ],
         "xaxis": "x",
         "y": [
          1.4413480196246111,
          1.6817736680956321,
          2.0741253520509564,
          2.9837602533485774,
          3.0140336868063278,
          3.016332072637105,
          3.012124752364352,
          0.9084002186617329,
          0.9165131806369282,
          0.9165131806369282,
          0.9165131806369282,
          0.9165131806369282,
          1.050637630431406,
          1.1274709663957303,
          1.050637630431406,
          1.0408112806215937,
          0.82763998785568,
          0.8211677929541876,
          0.8211677929541876,
          0.8267281945640217,
          0.8392130792670344,
          0.8392130792670344,
          0.8392130792670344,
          0.8724947449620986,
          0.895636140018853,
          0.8926927816583998,
          0.9610907565600673,
          0.8904847646483097,
          0.8941902880385784,
          0.87137819089737,
          0.8392130792670344,
          0.8392130792670344,
          0.7266985970795709,
          0.7460637092693108,
          0.7676881557232096,
          0.7676881557232096,
          0.7676881557232096,
          0.7676881557232096,
          0.7730818261925848,
          0.7730818261925848,
          0.7730818261925848,
          0.7730818261925848,
          0.7730818261925848,
          0.7666096312910924,
          0.7801954312796229,
          0.7801954312796229,
          0.7801954312796229,
          0.7801954312796229,
          0.7801954312796229,
          0.7801954312796229,
          0.7794057382825397,
          0.7773640085728841,
          0.775155991562794,
          0.775155991562794,
          0.7731533805104698,
          0.7731533805104698,
          0.775155991562794,
          0.775155991562794,
          0.7773640085728841,
          0.7794057382825397,
          0.7782978617196726,
          0.7801954312796229,
          0.7801954312796229,
          0.7801954312796229,
          0.7801954312796229,
          0.7721700329009265,
          0.7721700329009265,
          0.7721700329009265,
          0.7721700329009265,
          0.7666096312910924,
          0.7730818261925848,
          0.7730818261925848,
          0.7730818261925848,
          0.7730818261925848,
          0.7730818261925848,
          0.7730818261925848,
          0.7676881557232096,
          0.7676881557232096,
          0.7676881557232096,
          0.7676881557232096,
          0.7676881557232096,
          0.7676881557232096,
          0.6701516690943574,
          1.602586642717014,
          2.9423802022687973,
          2.9135296383416227,
          1.0413751998355416,
          0.7708194280772646,
          0.8591564259544824,
          1.1235048500977016,
          1.1335495386149308,
          1.1335495386149308,
          1.1335495386149308,
          0.7708194280772646,
          0.8552857613061718,
          0.8882236129856963,
          0.8882236129856963,
          0.9293178329944262,
          0.9293178329944262,
          0.9293178329944262,
          0.930763684974701,
          0.9298620563239033,
          0.9256123096041576,
          0.9256123096041576,
          0.9256123096041576,
          0.9256123096041576,
          0.9278203266142477,
          0.930763684974701,
          0.9293178329944262,
          0.9293178329944262,
          0.9293178329944262,
          0.9293178329944262,
          0.8882236129856963,
          0.881221999777359,
          0.8034124108737368,
          0.8081927443609855,
          0.8129897046356814,
          0.8129897046356814,
          0.8129897046356814,
          0.8129897046356814,
          0.8120779113440231,
          0.8120779113440231,
          0.8120779113440231,
          0.8201033097227195,
          0.8082809254663177,
          0.8102835365186419,
          0.8102835365186419,
          0.8201033097227195,
          0.8120779113440231,
          0.806517509734189,
          0.8129897046356814,
          0.8129897046356814,
          0.8081927443609855,
          0.8081927443609855,
          0.8034124108737368,
          0.728138391448806,
          1.7937947982552553,
          2.2800385660810494,
          3.591573639021755,
          3.595780959294508,
          3.595780959294508,
          3.5455077719786394,
          3.2828769106667948,
          2.9755757287979656,
          1.2456677522270625,
          1.3221881152819384,
          1.3221881152819384,
          1.4786142666104083,
          1.4886589551276375,
          1.4886589551276375,
          1.5266941912693366,
          1.5266941912693366,
          1.4886589551276375,
          1.4886589551276375,
          1.4729673042948777,
          1.4078930071335756,
          1.397462413230191,
          1.3418923174923127,
          1.3221881152819384,
          1.064974896438853,
          1.064974896438853,
          1.064974896438853,
          1.2027013315295105,
          1.2027013315295105,
          1.2027013315295105,
          1.2027013315295105,
          1.1991848909520428,
          1.2265412929563608,
          1.2330168396694061,
          1.2287670929496604,
          1.2287670929496604,
          1.2287670929496604,
          1.2287670929496604,
          1.2309751099597506,
          1.2309751099597506,
          1.2330168396694061,
          1.2027013315295105,
          1.2027013315295105,
          1.2027013315295105,
          1.2027013315295105,
          1.2027013315295105,
          1.2027013315295105,
          1.2027013315295105,
          1.2027013315295105,
          1.2027013315295105,
          0.8938166108262954,
          0.9228157898814047,
          1.035669400232966,
          1.0574511185182844,
          1.0574511185182844,
          1.0574511185182844,
          1.0574511185182844,
          1.0574511185182844,
          1.0574511185182844,
          1.056539325226626,
          1.056539325226626,
          1.0645647236053224,
          1.0645647236053224,
          1.0935463154186575,
          1.091504585709002,
          1.0892965686989118,
          1.0892965686989118,
          1.0892965686989118,
          1.0892965686989118,
          1.0892965686989118,
          1.091504585709002,
          1.091504585709002,
          1.0935463154186575,
          1.062667154045372,
          1.0645647236053224,
          1.0645647236053224,
          1.0645647236053224,
          1.0645647236053224,
          1.0645647236053224,
          1.056539325226626,
          1.056539325226626,
          1.056539325226626,
          1.056539325226626,
          1.0574511185182844,
          1.0574511185182844,
          1.0574511185182844,
          1.0574511185182844,
          1.0574511185182844,
          1.0574511185182844,
          1.035669400232966,
          0.9228157898814047,
          3.8702714734443417,
          3.8952370546310715,
          3.8952370546310715,
          3.916506272872338,
          3.916506272872338,
          3.9046853515860533,
          3.9046853515860533,
          2.8591629098122877,
          1.1560361705812516,
          0.9965971747967023,
          1.5226814356513285,
          1.5226814356513285,
          1.5226814356513285,
          1.5251660950109724,
          1.5472466522958683,
          1.5472466522958683,
          1.5501661894116276,
          1.5472466522958683,
          1.5251660950109724,
          1.5226814356513285,
          1.5226814356513285,
          1.4640497142422326,
          1.117165410850216,
          0.8708968993542583,
          1.11503618978528,
          1.1847076349959733,
          1.2521780196830834,
          1.2521780196830834,
          1.2521780196830834,
          1.2521780196830834,
          1.2486615791056157,
          1.2804517981133234,
          1.2804517981133234,
          1.2782437811032332,
          1.276241170050909,
          1.2782437811032332,
          1.2782437811032332,
          1.2804517981133234,
          1.2804517981133234,
          1.2521780196830834,
          1.2521780196830834,
          1.2521780196830834,
          1.2521780196830834,
          1.1023968911329334,
          0.8925782326875917,
          0.5790243169174281,
          0.948085139617122,
          1.1096958453802828,
          1.120210578080728,
          1.1211193324873834,
          1.1211193324873834,
          1.1211193324873834,
          1.120207539195725,
          1.1226152958920288,
          1.1226152958920288,
          1.1226152958920288,
          1.1226152958920288,
          1.128965568031659,
          1.1255892495630684,
          1.1255892495630684,
          1.1255892495630684,
          1.1228999825529782,
          1.1210281943744538,
          1.119824043671054,
          1.1225819852860845,
          1.1255892495630684,
          1.127363795727488,
          1.1226152958920288,
          1.1226152958920288,
          1.1226152958920288,
          1.120207539195725,
          1.120207539195725,
          1.1211193324873834,
          1.1211193324873834,
          1.1211193324873834,
          1.1211193324873834,
          1.1211193324873834,
          1.1211193324873834,
          1.0572209630734677,
          0.9417797995331111,
          1.8890208579607008,
          1.7570592509984182,
          2.1035565207905287,
          2.62320516970114,
          2.621296235259164,
          2.360246451474126,
          2.310922758755142,
          0.7430669058769286,
          0.7650528690026628,
          0.8315417347252155,
          0.8384840470231387,
          0.8285196327006729,
          0.8285196327006729,
          0.8384840470231387,
          0.845858840230202,
          0.7650528690026628,
          0.7650528690026628,
          0.7650528690026628,
          0.7650528690026628,
          0.6430318142201776,
          0.6430318142201776,
          0.6430318142201776,
          0.6430318142201776,
          0.6430318142201776,
          0.6365596193186852,
          0.6491216341368565,
          0.68219045576689,
          0.6859506544454586,
          0.7398177613147785,
          0.6851950320640818,
          0.686943600764599,
          0.68219045576689,
          0.68219045576689,
          0.68219045576689,
          0.6430318142201776,
          0.6430318142201776,
          0.6430318142201776,
          0.5862250582671887,
          0.5862250582671887,
          0.5862250582671887,
          0.5882047395457417,
          0.5891374490327167,
          0.5891374490327167,
          0.5921081383117323,
          0.5911963450200739,
          0.5911963450200739,
          0.5911963450200739,
          0.5911963450200739,
          0.6026942182140014,
          0.6026942182140014,
          0.6026942182140014,
          0.6026942182140014,
          0.5892987754601237,
          0.5911963450200739,
          0.5911963450200739,
          0.5911963450200739,
          0.5911963450200739,
          0.5921081383117323,
          0.5921081383117323,
          0.5921081383117323,
          0.5921081383117323,
          0.5891374490327167,
          0.5891374490327167,
          0.5891374490327167,
          0.5891374490327167,
          0.5862250582671887,
          0.5862250582671887,
          0.5862250582671887,
          0.5862250582671887,
          0.5862250582671887,
          0.5784202469726393,
          0.9290664968440012,
          2.014235923667022,
          2.2881226993534645,
          2.2448315857624745,
          2.2881226993534645,
          1.7516359457113624,
          0.9290664968440012,
          0.815982946242669,
          0.8691512538471331,
          0.8839050368193696,
          0.8760935661450563,
          0.8760935661450563,
          0.8807888220223115,
          0.8839050368193696,
          0.8839050368193696,
          0.8691512538471331,
          0.844662632647778,
          0.6696920977492334,
          0.6696920977492334,
          0.6723360807821935,
          0.6951481779234019,
          0.7282169995534353,
          0.7282169995534353,
          0.7195491836054244,
          0.7148539277281692,
          0.7148539277281692,
          0.7217572006155145,
          0.7247005589759677,
          0.7282169995534353,
          0.7282169995534353,
          0.7282169995534353,
          0.7282169995534353,
          0.7282169995534353,
          0.6951481779234019,
          0.6723360807821935,
          0.6653344675738563,
          0.6597740659640222,
          0.6696920977492334,
          0.6696920977492334,
          0.6696920977492334,
          0.6696920977492334,
          0.6562025514118967,
          0.6562025514118967,
          0.6154278230532134,
          0.6154278230532134,
          0.621949313417716,
          0.6248617041832439,
          0.6248617041832439,
          0.6248617041832439,
          0.6248617041832439,
          0.6242516933354635,
          0.6242516933354635,
          0.6242516933354635,
          0.6242516933354635,
          0.6242516933354635,
          0.6242516933354635,
          0.6233399000438051,
          0.6233399000438051,
          0.6233399000438051,
          0.6233399000438051,
          0.6233399000438051,
          0.6155461847793241,
          0.6155461847793241,
          0.613338167769234,
          0.6066403008396546,
          0.6066403008396546,
          0.6086429118919788,
          0.6086429118919788,
          0.613338167769234,
          0.6233399000438051,
          0.6233399000438051,
          0.6233399000438051,
          0.6233399000438051,
          0.6233399000438051,
          0.6233399000438051,
          0.6233399000438051,
          0.6242516933354635,
          0.6242516933354635,
          0.6242516933354635,
          0.6242516933354635,
          0.6242516933354635,
          0.6242516933354635,
          0.6242516933354635,
          0.6248617041832439,
          0.6248617041832439,
          0.621949313417716,
          0.621949313417716,
          0.621949313417716,
          0.6190085231800094,
          0.6154278230532134
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "variable"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "index"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "value"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"9a4a6ee2-3778-41db-9965-59ef7308df7b\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"9a4a6ee2-3778-41db-9965-59ef7308df7b\")) {                    Plotly.newPlot(                        \"9a4a6ee2-3778-41db-9965-59ef7308df7b\",                        [{\"hovertemplate\": \"variable=Data<br>index=%{x}<br>value=%{y}<extra></extra>\", \"legendgroup\": \"Data\", \"line\": {\"color\": \"#636efa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"Data\", \"showlegend\": true, \"type\": \"scattergl\", \"x\": [2, 8, 10, 15, 23, 24, 28, 38, 42, 43, 44, 47, 51, 64, 80, 84, 87, 90, 92, 94, 99, 100, 103, 109, 117, 120, 128, 136, 146, 148, 159, 164, 170, 175, 177, 179, 184, 187, 189, 194, 200, 204, 208, 209, 218, 222, 225, 226, 227, 228, 230, 233, 238, 239, 242, 243, 249, 251, 252, 256, 257, 262, 263, 267, 270, 272, 273, 274, 276, 277, 285, 289, 290, 291, 293, 295, 302, 303, 306, 307, 309, 310, 313, 316, 328, 335, 346, 349, 350, 361, 367, 376, 380, 396, 400, 405, 408, 411, 412, 415, 423, 424, 430, 431, 432, 443, 446, 449, 451, 452, 453, 456, 465, 471, 482, 490, 498, 512, 513, 514, 516, 518, 520, 532, 550, 552, 557, 574, 579, 583, 589, 591, 607, 612, 616, 620, 626, 629, 642, 643, 644, 652, 657, 659, 665, 669, 670, 680, 683, 686, 688, 696, 700, 702, 708, 709, 710, 712, 714, 720, 722, 725, 727, 733, 740, 741, 744, 745, 746, 756, 765, 766, 767, 770, 771, 773, 777, 780, 782, 783, 785, 789, 791, 793, 794, 801, 805, 808, 811, 817, 827, 831, 832, 834, 842, 843, 849, 858, 862, 865, 867, 870, 871, 879, 883, 888, 889, 890, 893, 896, 897, 899, 904, 906, 911, 912, 913, 914, 917, 919, 922, 925, 926, 942, 944, 947, 957, 958, 959, 963, 969, 971, 972, 979, 983, 986, 995, 996, 998, 1001, 1005, 1007, 1009, 1016, 1019, 1023, 1027, 1029, 1033, 1036, 1041, 1042, 1046, 1049, 1052, 1054, 1061, 1065, 1067, 1074, 1078, 1082, 1084, 1087, 1088, 1104, 1106, 1107, 1108, 1114, 1116, 1119, 1123, 1126, 1127, 1138, 1140, 1149, 1160, 1161, 1162, 1171, 1173, 1179, 1180, 1182, 1183, 1185, 1189, 1196, 1199, 1204, 1209, 1215, 1220, 1224, 1228, 1229, 1234, 1235, 1243, 1254, 1255, 1258, 1262, 1264, 1273, 1274, 1278, 1290, 1295, 1298, 1302, 1304, 1316, 1318, 1327, 1328, 1331, 1334, 1339, 1344, 1345, 1346, 1347, 1353, 1354, 1358, 1360, 1362, 1363, 1367, 1376, 1385, 1394, 1396, 1404, 1405, 1407, 1415, 1423, 1424, 1426, 1432, 1434, 1438, 1439, 1441, 1446, 1452, 1468, 1470, 1479, 1482, 1491, 1492, 1493, 1502, 1508, 1511, 1520, 1522, 1524, 1528, 1536, 1537, 1544, 1545, 1546, 1551, 1552, 1557, 1560, 1561, 1562, 1563, 1565, 1567, 1572, 1576, 1583, 1588, 1594, 1597, 1606, 1609, 1611, 1616, 1617, 1629, 1630, 1631, 1633, 1635, 1650, 1652, 1656, 1657, 1660, 1668, 1677, 1678, 1681, 1692, 1695, 1697, 1701, 1704, 1705, 1706, 1707, 1710, 1711, 1712, 1713, 1714, 1716, 1717, 1720, 1721, 1722, 1723, 1733, 1737, 1740, 1741, 1744, 1745, 1750, 1752, 1753, 1756, 1760, 1769, 1771, 1772, 1774, 1780, 1784, 1785, 1786, 1792, 1795, 1797, 1799, 1800, 1807, 1808, 1810, 1814, 1820, 1821, 1822, 1827, 1830, 1831, 1834, 1835, 1836, 1839, 1844, 1846, 1851, 1852, 1857, 1861, 1864], \"xaxis\": \"x\", \"y\": [1.4552, 2.9538, 3.05, 3.0553, 3.0621, 3.0519, 3.0134, 0.50015, 0.98244, 1.0598, 1.0898, 1.1008, 1.1025, 1.0968, 1.0976, 0.8627, 0.535, 0.82124, 0.8921600000000001, 0.9006200000000001, 0.90507, 0.904, 0.90781, 0.90636, 0.8959, 0.89243, 0.88476, 0.8936700000000001, 0.9057200000000001, 0.9067799999999999, 0.90138, 0.72556, 0.7548699999999999, 0.7842399999999999, 0.7855, 0.78492, 0.7858, 0.7858, 0.7857, 0.78364, 0.7803800000000001, 0.78008, 0.77841, 0.77761, 0.77318, 0.7723, 0.7702899999999999, 0.76764, 0.7659699999999999, 0.7648699999999999, 0.76059, 0.7593, 0.75542, 0.75364, 0.7520899999999999, 0.75271, 0.75937, 0.75942, 0.76017, 0.76658, 0.76707, 0.7723, 0.77279, 0.77496, 0.7762399999999999, 0.77674, 0.77685, 0.77772, 0.77822, 0.77969, 0.7819699999999999, 0.78452, 0.78432, 0.78492, 0.78561, 0.786, 0.78601, 0.7864899999999999, 0.78432, 0.78128, 0.75536, 0.7089, 0.6539699999999999, 1.6153, 2.9172, 2.9248, 0.97016, 0.64904, 0.77644, 1.1536, 1.154, 1.1538, 1.1566, 0.52267, 0.84915, 0.9469799999999999, 0.95011, 0.9528, 0.9538399999999999, 0.9538399999999999, 0.9451, 0.9436899999999999, 0.93891, 0.93606, 0.9362299999999999, 0.93855, 0.93976, 0.9451200000000001, 0.9490700000000001, 0.9499, 0.95134, 0.95321, 0.9476100000000001, 0.89731, 0.81921, 0.8252200000000001, 0.82481, 0.8186399999999999, 0.81892, 0.81788, 0.81657, 0.8157399999999999, 0.8156, 0.80937, 0.79105, 0.7925800000000001, 0.79771, 0.81415, 0.8154600000000001, 0.81686, 0.81886, 0.82004, 0.8242799999999999, 0.82155, 0.81386, 0.6339899999999999, 1.9573, 2.8378, 3.6409, 3.6437, 3.6377, 3.5357, 2.3861, 1.7347, 0.92106, 1.4611, 1.5067, 1.5303, 1.5293, 1.5263, 1.5251, 1.5312, 1.5335, 1.5278, 1.3705, 1.2324, 1.0894, 0.80479, 0.5012, 1.1299, 1.2371, 1.2497, 1.2534, 1.258, 1.2562, 1.2554, 1.2487, 1.2475, 1.2448, 1.232, 1.2389, 1.2393, 1.2395, 1.2429, 1.2454, 1.2499, 1.2566, 1.2589, 1.26, 1.2607, 1.2585, 1.2507, 1.2117, 1.0347, 0.91755, 0.85883, 1.0777, 1.0829, 1.0834, 1.0846, 1.0811, 1.0789, 1.0787, 1.0774, 1.0732, 1.0728, 1.0686, 1.0603, 1.0513, 1.0492, 1.0452, 1.0433, 1.0418, 1.042, 1.0469, 1.0546, 1.0578, 1.059, 1.0649, 1.0676, 1.0674, 1.0693, 1.0735, 1.074, 1.0765, 1.0774, 1.0779, 1.0776, 1.0796, 1.0806, 1.0823, 1.0841, 1.0846, 0.99729, 0.8058, 0.5017199999999999, 3.9577, 4.0259, 4.041, 4.0382, 4.0465, 4.0533, 4.0506, 2.5568, 0.98345, 0.7747, 1.6006, 1.6005, 1.6001, 1.6032, 1.6016, 1.5998, 1.5966, 1.6052, 1.603, 1.6017, 1.5853, 1.4478, 0.86634, 0.54188, 1.1485, 1.2399, 1.3105, 1.3151, 1.3205, 1.3212, 1.3173, 1.3067, 1.3034, 1.2945, 1.2924, 1.2997, 1.3026, 1.304, 1.3052, 1.3183, 1.3138, 1.3111, 1.31, 0.95988, 0.7228, 0.60868, 1.0207, 1.1282, 1.1324, 1.139, 1.1388, 1.1337, 1.1275, 1.1269, 1.1266, 1.1216, 1.1199, 1.1096, 1.1069, 1.1023, 1.1025, 1.1013, 1.0946, 1.0944, 1.0994, 1.1024, 1.1146, 1.121, 1.1245, 1.1268, 1.1301, 1.1301, 1.132, 1.1321, 1.1364, 1.1354, 1.1347, 1.1299, 1.0221, 0.83091, 2.2026, 2.394, 2.614, 2.6169, 2.5994, 2.3007, 1.2188, 0.56183, 0.8327700000000001, 0.8318399999999999, 0.82934, 0.82965, 0.8302200000000001, 0.8342700000000001, 0.8345799999999999, 0.83265, 0.8269200000000001, 0.81481, 0.77994, 0.66274, 0.6765100000000001, 0.68219, 0.68376, 0.68455, 0.68533, 0.68791, 0.68258, 0.67374, 0.67237, 0.67375, 0.68083, 0.68219, 0.68555, 0.68693, 0.67788, 0.66804, 0.58855, 0.59442, 0.59665, 0.59665, 0.5965, 0.5967899999999999, 0.59531, 0.59334, 0.58934, 0.58841, 0.58368, 0.57904, 0.57326, 0.5716399999999999, 0.57213, 0.57587, 0.57953, 0.58305, 0.58885, 0.58905, 0.58919, 0.59102, 0.59299, 0.59285, 0.5956600000000001, 0.59595, 0.5963, 0.5967899999999999, 0.59718, 0.5969399999999999, 0.59595, 0.59442, 0.59053, 0.58208, 0.50716, 0.91669, 2.1216, 2.2048, 2.1967, 2.2049, 1.806, 0.98703, 0.87277, 0.8712, 0.87225, 0.8710399999999999, 0.8712700000000001, 0.87284, 0.8726799999999999, 0.8723200000000001, 0.8712700000000001, 0.87255, 0.71337, 0.71529, 0.71973, 0.7203, 0.7209800000000001, 0.71834, 0.71017, 0.7089300000000001, 0.70508, 0.7104699999999999, 0.71281, 0.71668, 0.7197, 0.7205600000000001, 0.7205600000000001, 0.72027, 0.7205600000000001, 0.71917, 0.71763, 0.7168, 0.71503, 0.71532, 0.7125100000000001, 0.70881, 0.5905, 0.51078, 0.5269699999999999, 0.57772, 0.62621, 0.62634, 0.62589, 0.6258699999999999, 0.62507, 0.6248, 0.6230899999999999, 0.62231, 0.6221800000000001, 0.62138, 0.6197600000000001, 0.61741, 0.6167199999999999, 0.6164, 0.6150899999999999, 0.60818, 0.6061, 0.60541, 0.60493, 0.60028, 0.6007600000000001, 0.6025, 0.60434, 0.60482, 0.61002, 0.61165, 0.6136, 0.6158100000000001, 0.61843, 0.61856, 0.61888, 0.62051, 0.6209899999999999, 0.62097, 0.62211, 0.6221399999999999, 0.62248, 0.62342, 0.62513, 0.62502, 0.62537, 0.62513, 0.62227, 0.61416, 0.50512], \"yaxis\": \"y\"}, {\"hovertemplate\": \"variable=ANN<br>index=%{x}<br>value=%{y}<extra></extra>\", \"legendgroup\": \"ANN\", \"line\": {\"color\": \"#EF553B\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"ANN\", \"showlegend\": true, \"type\": \"scattergl\", \"x\": [2, 8, 10, 15, 23, 24, 28, 38, 42, 43, 44, 47, 51, 64, 80, 84, 87, 90, 92, 94, 99, 100, 103, 109, 117, 120, 128, 136, 146, 148, 159, 164, 170, 175, 177, 179, 184, 187, 189, 194, 200, 204, 208, 209, 218, 222, 225, 226, 227, 228, 230, 233, 238, 239, 242, 243, 249, 251, 252, 256, 257, 262, 263, 267, 270, 272, 273, 274, 276, 277, 285, 289, 290, 291, 293, 295, 302, 303, 306, 307, 309, 310, 313, 316, 328, 335, 346, 349, 350, 361, 367, 376, 380, 396, 400, 405, 408, 411, 412, 415, 423, 424, 430, 431, 432, 443, 446, 449, 451, 452, 453, 456, 465, 471, 482, 490, 498, 512, 513, 514, 516, 518, 520, 532, 550, 552, 557, 574, 579, 583, 589, 591, 607, 612, 616, 620, 626, 629, 642, 643, 644, 652, 657, 659, 665, 669, 670, 680, 683, 686, 688, 696, 700, 702, 708, 709, 710, 712, 714, 720, 722, 725, 727, 733, 740, 741, 744, 745, 746, 756, 765, 766, 767, 770, 771, 773, 777, 780, 782, 783, 785, 789, 791, 793, 794, 801, 805, 808, 811, 817, 827, 831, 832, 834, 842, 843, 849, 858, 862, 865, 867, 870, 871, 879, 883, 888, 889, 890, 893, 896, 897, 899, 904, 906, 911, 912, 913, 914, 917, 919, 922, 925, 926, 942, 944, 947, 957, 958, 959, 963, 969, 971, 972, 979, 983, 986, 995, 996, 998, 1001, 1005, 1007, 1009, 1016, 1019, 1023, 1027, 1029, 1033, 1036, 1041, 1042, 1046, 1049, 1052, 1054, 1061, 1065, 1067, 1074, 1078, 1082, 1084, 1087, 1088, 1104, 1106, 1107, 1108, 1114, 1116, 1119, 1123, 1126, 1127, 1138, 1140, 1149, 1160, 1161, 1162, 1171, 1173, 1179, 1180, 1182, 1183, 1185, 1189, 1196, 1199, 1204, 1209, 1215, 1220, 1224, 1228, 1229, 1234, 1235, 1243, 1254, 1255, 1258, 1262, 1264, 1273, 1274, 1278, 1290, 1295, 1298, 1302, 1304, 1316, 1318, 1327, 1328, 1331, 1334, 1339, 1344, 1345, 1346, 1347, 1353, 1354, 1358, 1360, 1362, 1363, 1367, 1376, 1385, 1394, 1396, 1404, 1405, 1407, 1415, 1423, 1424, 1426, 1432, 1434, 1438, 1439, 1441, 1446, 1452, 1468, 1470, 1479, 1482, 1491, 1492, 1493, 1502, 1508, 1511, 1520, 1522, 1524, 1528, 1536, 1537, 1544, 1545, 1546, 1551, 1552, 1557, 1560, 1561, 1562, 1563, 1565, 1567, 1572, 1576, 1583, 1588, 1594, 1597, 1606, 1609, 1611, 1616, 1617, 1629, 1630, 1631, 1633, 1635, 1650, 1652, 1656, 1657, 1660, 1668, 1677, 1678, 1681, 1692, 1695, 1697, 1701, 1704, 1705, 1706, 1707, 1710, 1711, 1712, 1713, 1714, 1716, 1717, 1720, 1721, 1722, 1723, 1733, 1737, 1740, 1741, 1744, 1745, 1750, 1752, 1753, 1756, 1760, 1769, 1771, 1772, 1774, 1780, 1784, 1785, 1786, 1792, 1795, 1797, 1799, 1800, 1807, 1808, 1810, 1814, 1820, 1821, 1822, 1827, 1830, 1831, 1834, 1835, 1836, 1839, 1844, 1846, 1851, 1852, 1857, 1861, 1864], \"xaxis\": \"x\", \"y\": [1.3489694595336914, 2.1722378730773926, 2.371692180633545, 2.7939095497131348, 2.92510724067688, 2.926978588104248, 2.826322555541992, 0.8751901984214783, 1.0255355834960938, 1.0596016645431519, 1.076078176498413, 1.0787312984466553, 1.0822633504867554, 1.114633321762085, 1.0831453800201416, 1.0796149969100952, 0.9104669094085693, 0.9106797575950623, 0.9108218550682068, 0.9109637141227722, 0.9113184213638306, 0.9113894104957581, 0.9120272994041443, 0.9133017063140869, 0.9149972796440125, 0.9156322479248047, 0.9173226356506348, 0.9164780378341675, 0.9143619537353516, 0.9139379858970642, 0.9116020798683167, 0.9111056327819824, 0.7878611087799072, 0.7876328825950623, 0.7875415682792664, 0.7874504327774048, 0.7867668271064758, 0.7863571047782898, 0.7860842943191528, 0.785402774810791, 0.7845863103866577, 0.7840428948402405, 0.7835002541542053, 0.7833645939826965, 0.7821462154388428, 0.7816057801246643, 0.7812009453773499, 0.7810660600662231, 0.7809311747550964, 0.7807964086532593, 0.7805269360542297, 0.7801230549812317, 0.7794510126113892, 0.7793168425559998, 0.7789142727851868, 0.77886962890625, 0.7795853614807129, 0.7798541188240051, 0.7799886465072632, 0.7805269360542297, 0.7806615829467773, 0.7813358902931213, 0.781470775604248, 0.7820110321044922, 0.7824166417121887, 0.782687246799469, 0.7828226089477539, 0.7829581499099731, 0.7832291722297668, 0.7833645939826965, 0.7844505310058594, 0.78499436378479, 0.7851304411888123, 0.785266637802124, 0.7855389714241028, 0.7858115434646606, 0.7867668271064758, 0.7869035005569458, 0.7873136401176453, 0.7874504327774048, 0.7875415682792664, 0.7875872850418091, 1.2088011503219604, 1.5969455242156982, 2.89963698387146, 2.6881539821624756, 1.319711685180664, 0.9474200010299683, 0.9853031039237976, 1.1515371799468994, 1.1646325588226318, 1.1668012142181396, 1.1581027507781982, 0.8833231925964355, 0.9225473999977112, 0.9228243231773376, 0.9239165782928467, 0.9280393719673157, 0.9294143319129944, 0.9335275888442993, 0.9444087147712708, 0.9457600116729736, 0.9538262486457825, 0.955163836479187, 0.956499457359314, 0.9524868130683899, 0.9484565854072571, 0.9444087147712708, 0.941700279712677, 0.9403430223464966, 0.9389839768409729, 0.9348946213722229, 0.9234489798545837, 0.9225053787231445, 0.8353671431541443, 0.8330814838409424, 0.8300453424453735, 0.8247628808021545, 0.8243871331214905, 0.8240115642547607, 0.8232608437538147, 0.8225110173225403, 0.8217619061470032, 0.8172841668128967, 0.8113571405410767, 0.8120952248573303, 0.8139440417289734, 0.8202662467956543, 0.822136402130127, 0.8236360549926758, 0.8258915543556213, 0.8266449570655823, 0.8327013254165649, 0.8346045017242432, 0.8353671431541443, 0.8358760476112366, 1.8680931329727173, 2.373389959335327, 3.5571353435516357, 3.5755772590637207, 3.5861916542053223, 3.505262613296509, 3.1840291023254395, 2.9627561569213867, 1.0718170404434204, 1.28632390499115, 1.3396053314208984, 1.5178130865097046, 1.5273535251617432, 1.5368560552597046, 1.542413592338562, 1.542413592338562, 1.5305253267288208, 1.5241776704788208, 1.5093004703521729, 1.4933196306228638, 1.4632893800735474, 1.4030207395553589, 1.3396053314208984, 1.2471665143966675, 1.2475886344909668, 1.2482216358184814, 1.2486435174942017, 1.251173496246338, 1.2555948495864868, 1.256225824356079, 1.258117914199829, 1.258748173713684, 1.259378433227539, 1.2656718492507935, 1.2650431394577026, 1.2644143104553223, 1.263785481452942, 1.2618975639343262, 1.261268138885498, 1.260008454322815, 1.257487416267395, 1.2555948495864868, 1.254332423210144, 1.2537009716033936, 1.2524374723434448, 1.249908685684204, 1.2490652799606323, 1.2486435174942017, 1.2484325170516968, 1.0838290452957153, 1.0834959745407104, 1.083246111869812, 1.0829962491989136, 1.0816630125045776, 1.0791605710983276, 1.0781588554382324, 1.0779082775115967, 1.0774071216583252, 1.0754010677337646, 1.0751502513885498, 1.0736441612243652, 1.0713828802108765, 1.070376992225647, 1.069622278213501, 1.0691190958023071, 1.0683637857437134, 1.0681118965148926, 1.0676082372665405, 1.068615436553955, 1.0698739290237427, 1.0701254606246948, 1.070376992225647, 1.0711314678192139, 1.0718857049942017, 1.0721368789672852, 1.0726394653320312, 1.0738953351974487, 1.0743974447250366, 1.0756518840789795, 1.0759029388427734, 1.0761535167694092, 1.0764044523239136, 1.0771565437316895, 1.077657699584961, 1.0784093141555786, 1.0791605710983276, 1.0794110298156738, 1.0830795764923096, 1.083246111869812, 1.0834959745407104, 3.2042644023895264, 3.3747832775115967, 3.5243496894836426, 4.030196666717529, 4.030196666717529, 3.8035778999328613, 3.667365312576294, 2.3642358779907227, 1.410412073135376, 0.977009117603302, 1.4752275943756104, 1.5116591453552246, 1.57150137424469, 1.5950523614883423, 1.6207568645477295, 1.6303867101669312, 1.635500431060791, 1.6158065795898438, 1.5950523614883423, 1.563462495803833, 1.4022102355957031, 1.2908506393432617, 1.0397911071777344, 1.3009076118469238, 1.303430438041687, 1.3039345741271973, 1.305950403213501, 1.3094732761383057, 1.31399405002594, 1.317002773284912, 1.3274993896484375, 1.3334742784500122, 1.3364553451538086, 1.3468562364578247, 1.3498185873031616, 1.3438897132873535, 1.3409191370010376, 1.3364553451538086, 1.3349652290344238, 1.3109811544418335, 1.3079642057418823, 1.306454062461853, 1.305950403213501, 1.3029260635375977, 1.3019171953201294, 1.1477570533752441, 1.1475869417190552, 1.1474592685699463, 1.1474167108535767, 1.1461812257766724, 1.1459251642227173, 1.1434468030929565, 1.1371735334396362, 1.1366021633148193, 1.1360307931900024, 1.130879521369934, 1.1297333240509033, 1.1262907981872559, 1.1257166862487793, 1.1245678663253784, 1.1239932775497437, 1.122843623161316, 1.1205427646636963, 1.1199672222137451, 1.12169349193573, 1.1245678663253784, 1.1274387836456299, 1.130879521369934, 1.1337429285049438, 1.1360307931900024, 1.1383156776428223, 1.1388864517211914, 1.141737937927246, 1.1423077583312988, 1.1455410718917847, 1.1469484567642212, 1.14707612991333, 1.147374153137207, 1.1475443840026855, 1.1476294994354248, 1.4079689979553223, 1.4921408891677856, 1.8767503499984741, 2.484039306640625, 2.3270111083984375, 2.0681045055389404, 1.678504228591919, 0.8138205409049988, 0.8196969628334045, 0.8232535719871521, 0.8415554165840149, 0.8463862538337708, 0.8767416477203369, 0.8373695015907288, 0.8285678625106812, 0.8196969628334045, 0.8191030025482178, 0.8185088038444519, 0.8179143071174622, 0.6849040985107422, 0.6848509907722473, 0.6846387386322021, 0.684532642364502, 0.684426486492157, 0.6842673420906067, 0.6836311221122742, 0.6822015047073364, 0.6807745695114136, 0.6799829006195068, 0.6802994608879089, 0.6815669536590576, 0.6817255616188049, 0.6820428371429443, 0.6833131909370422, 0.6844794750213623, 0.684532642364502, 0.6846387386322021, 0.5928082466125488, 0.592524528503418, 0.5919575095176697, 0.5918158292770386, 0.5915325284004211, 0.5908246040344238, 0.589975893497467, 0.587716817855835, 0.5874348282814026, 0.5861672759056091, 0.5857452154159546, 0.5844801068305969, 0.5843396782875061, 0.5841993093490601, 0.5846206545829773, 0.5854639410972595, 0.5858858227729797, 0.5871530771255493, 0.5874348282814026, 0.587716817855835, 0.5882810354232788, 0.5894105434417725, 0.5895518064498901, 0.5905415415763855, 0.590683102607727, 0.5908246040344238, 0.5915325284004211, 0.5916742086410522, 0.5923827886581421, 0.5926664471626282, 0.5928082466125488, 0.5928555727005005, 0.5929028391838074, 0.5929974317550659, 0.966635525226593, 1.2785089015960693, 1.7698358297348022, 2.193575382232666, 1.7698358297348022, 1.0861636400222778, 0.966635525226593, 0.8509452939033508, 0.8553227186203003, 0.8590647578239441, 0.8683784008026123, 0.870233952999115, 0.8627972602844238, 0.8609322309494019, 0.8590647578239441, 0.8553226590156555, 0.8515713810920715, 0.7192452549934387, 0.7190156579017639, 0.7178689241409302, 0.7175252437591553, 0.7164950966835022, 0.7137547731399536, 0.7106835246086121, 0.7103430032730103, 0.7093223929405212, 0.7117058634757996, 0.7127296328544617, 0.713412880897522, 0.7147811651229858, 0.7158091068267822, 0.716152012348175, 0.7164950966835022, 0.7168383002281189, 0.7178689241409302, 0.7182127237319946, 0.7185567617416382, 0.7189009785652161, 0.7190156579017639, 0.7192452549934387, 0.7193600535392761, 0.7197045087814331, 0.7198194265365601, 0.6248517036437988, 0.6247498989105225, 0.6229214072227478, 0.621704638004303, 0.620793342590332, 0.6204898357391357, 0.6195797920227051, 0.6192767024040222, 0.6177629828453064, 0.6171581745147705, 0.616856038570404, 0.6159501075744629, 0.6147437691688538, 0.6120362281799316, 0.6114357709884644, 0.6111356616020203, 0.6105358600616455, 0.6087391376495361, 0.6075435280799866, 0.6072448492050171, 0.6069462895393372, 0.6051574945449829, 0.6054553389549255, 0.6060514450073242, 0.6066479086875916, 0.6069462895393372, 0.6090382933616638, 0.6093376278877258, 0.609936535358429, 0.6111356616020203, 0.6129377484321594, 0.6132384538650513, 0.6135393381118774, 0.6150451898574829, 0.6159501075744629, 0.6162519454956055, 0.6171581745147705, 0.6174604892730713, 0.6177629828453064, 0.6186708211898804, 0.6201862692832947, 0.620793342590332, 0.6223127841949463, 0.6226170063018799, 0.6241399645805359, 0.6245465874671936, 0.6248517036437988], \"yaxis\": \"y\"}, {\"hovertemplate\": \"variable=RFC<br>index=%{x}<br>value=%{y}<extra></extra>\", \"legendgroup\": \"RFC\", \"line\": {\"color\": \"#00cc96\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"RFC\", \"showlegend\": true, \"type\": \"scattergl\", \"x\": [2, 8, 10, 15, 23, 24, 28, 38, 42, 43, 44, 47, 51, 64, 80, 84, 87, 90, 92, 94, 99, 100, 103, 109, 117, 120, 128, 136, 146, 148, 159, 164, 170, 175, 177, 179, 184, 187, 189, 194, 200, 204, 208, 209, 218, 222, 225, 226, 227, 228, 230, 233, 238, 239, 242, 243, 249, 251, 252, 256, 257, 262, 263, 267, 270, 272, 273, 274, 276, 277, 285, 289, 290, 291, 293, 295, 302, 303, 306, 307, 309, 310, 313, 316, 328, 335, 346, 349, 350, 361, 367, 376, 380, 396, 400, 405, 408, 411, 412, 415, 423, 424, 430, 431, 432, 443, 446, 449, 451, 452, 453, 456, 465, 471, 482, 490, 498, 512, 513, 514, 516, 518, 520, 532, 550, 552, 557, 574, 579, 583, 589, 591, 607, 612, 616, 620, 626, 629, 642, 643, 644, 652, 657, 659, 665, 669, 670, 680, 683, 686, 688, 696, 700, 702, 708, 709, 710, 712, 714, 720, 722, 725, 727, 733, 740, 741, 744, 745, 746, 756, 765, 766, 767, 770, 771, 773, 777, 780, 782, 783, 785, 789, 791, 793, 794, 801, 805, 808, 811, 817, 827, 831, 832, 834, 842, 843, 849, 858, 862, 865, 867, 870, 871, 879, 883, 888, 889, 890, 893, 896, 897, 899, 904, 906, 911, 912, 913, 914, 917, 919, 922, 925, 926, 942, 944, 947, 957, 958, 959, 963, 969, 971, 972, 979, 983, 986, 995, 996, 998, 1001, 1005, 1007, 1009, 1016, 1019, 1023, 1027, 1029, 1033, 1036, 1041, 1042, 1046, 1049, 1052, 1054, 1061, 1065, 1067, 1074, 1078, 1082, 1084, 1087, 1088, 1104, 1106, 1107, 1108, 1114, 1116, 1119, 1123, 1126, 1127, 1138, 1140, 1149, 1160, 1161, 1162, 1171, 1173, 1179, 1180, 1182, 1183, 1185, 1189, 1196, 1199, 1204, 1209, 1215, 1220, 1224, 1228, 1229, 1234, 1235, 1243, 1254, 1255, 1258, 1262, 1264, 1273, 1274, 1278, 1290, 1295, 1298, 1302, 1304, 1316, 1318, 1327, 1328, 1331, 1334, 1339, 1344, 1345, 1346, 1347, 1353, 1354, 1358, 1360, 1362, 1363, 1367, 1376, 1385, 1394, 1396, 1404, 1405, 1407, 1415, 1423, 1424, 1426, 1432, 1434, 1438, 1439, 1441, 1446, 1452, 1468, 1470, 1479, 1482, 1491, 1492, 1493, 1502, 1508, 1511, 1520, 1522, 1524, 1528, 1536, 1537, 1544, 1545, 1546, 1551, 1552, 1557, 1560, 1561, 1562, 1563, 1565, 1567, 1572, 1576, 1583, 1588, 1594, 1597, 1606, 1609, 1611, 1616, 1617, 1629, 1630, 1631, 1633, 1635, 1650, 1652, 1656, 1657, 1660, 1668, 1677, 1678, 1681, 1692, 1695, 1697, 1701, 1704, 1705, 1706, 1707, 1710, 1711, 1712, 1713, 1714, 1716, 1717, 1720, 1721, 1722, 1723, 1733, 1737, 1740, 1741, 1744, 1745, 1750, 1752, 1753, 1756, 1760, 1769, 1771, 1772, 1774, 1780, 1784, 1785, 1786, 1792, 1795, 1797, 1799, 1800, 1807, 1808, 1810, 1814, 1820, 1821, 1822, 1827, 1830, 1831, 1834, 1835, 1836, 1839, 1844, 1846, 1851, 1852, 1857, 1861, 1864], \"xaxis\": \"x\", \"y\": [1.3094181333333332, 1.401216766666666, 1.9107649533333322, 2.9500445000000015, 3.0550799999999985, 3.0538039999999995, 3.028871000000005, 0.7208122999999995, 0.9273881499999989, 1.0150219999999985, 1.0161109666666652, 0.8035218499999996, 1.0912144999999995, 1.0987484999999997, 1.0988045000000008, 1.0256151499999993, 0.6864948999999997, 0.7995180999999993, 0.8688317999999995, 0.6613880066666669, 0.88966715, 0.8944477499999992, 0.9064411566666667, 0.9072646666666658, 0.9006188666666657, 0.8949801299999995, 0.8889155400000008, 0.8917658533333339, 0.9037780333333341, 0.9050451766666656, 0.9044799416666671, 0.852347233333334, 0.7397316750000003, 0.6662541033333342, 0.7772498735714285, 0.7774881685714286, 0.7859087895238095, 0.7865771416666664, 0.7858829000000007, 0.7851583999999999, 0.7816746200000004, 0.7806319833333331, 0.7798011899999993, 0.7780655549999999, 0.7749585049999997, 0.772833608333334, 0.7717201999999999, 0.7707394999999999, 0.7699182000000007, 0.7671069000000006, 0.7619218804761916, 0.7600121299999993, 0.7567982666666666, 0.7554196400000006, 0.7534246200000001, 0.7534246200000001, 0.7563596000000001, 0.7580908450000003, 0.7585435999999998, 0.7619218804761916, 0.7640464533333337, 0.7713873999999994, 0.7715022000000001, 0.7738142516666666, 0.7753473499999999, 0.7757435500000004, 0.7764044499999991, 0.7766757249999994, 0.7771517750000001, 0.7780655549999999, 0.7804474333333334, 0.7825562499999991, 0.7827423999999995, 0.7830811999999997, 0.7849050749999997, 0.7850607100000003, 0.7859087895238095, 0.7863803399999996, 0.7854338369047624, 0.7774881685714286, 0.7772498735714285, 0.7543359700000002, 0.8118640333333338, 1.695640316666665, 2.9192608333333334, 2.9248977000000043, 1.0302033499999987, 0.6984704333333346, 0.7787229333333343, 1.1537099499999985, 1.154280583333332, 1.1535505000000004, 1.1567671666666648, 0.6083723333333348, 0.8372739249999998, 0.9450349733333344, 0.949510879999999, 0.9526794166666666, 0.952777539999999, 0.9530254166666668, 0.9454299488095239, 0.9422587804761904, 0.9384764999999992, 0.9368324599999991, 0.9344630600000002, 0.9395669690476199, 0.9401593366666674, 0.9454299488095239, 0.9490491166666658, 0.9510561083333338, 0.9517304083333343, 0.9531639666666675, 0.9483839333333343, 0.9047878000000007, 0.8127374040476194, 0.8239384599999994, 0.8232350416666668, 0.818707376666666, 0.8180539999999995, 0.8178615999999991, 0.8164430666666677, 0.8157832166666684, 0.8155231000000009, 0.8084484466666662, 0.7913401000000004, 0.7922928800000002, 0.7981339666666675, 0.8142640750000005, 0.8157985166666681, 0.8173780000000005, 0.8196050066666678, 0.8204046866666665, 0.824742906666666, 0.823013616666667, 0.8127374040476194, 0.6673215583333332, 1.6013775666666654, 2.2205743366666657, 3.6458435000000007, 3.647417999999995, 3.6445406666666598, 3.6073274000000053, 3.4061713333333348, 3.174746891666666, 0.9807800000000008, 1.291868603333333, 1.1294004883333335, 1.5288328333333325, 1.5304148333333323, 1.5322713333333342, 1.5284213333333332, 1.5284213333333332, 1.531105333333334, 1.5313037499999984, 1.5228108214285723, 1.5281393333333342, 1.5193627000000007, 1.4449228166666659, 1.1294004883333335, 1.0566218, 0.9638290190476191, 0.8799728449999994, 1.218888616666665, 1.2561165095238087, 1.2577939, 1.2576364999999992, 1.2554999999999983, 1.2547896666666651, 1.2518176666666685, 1.2349297333333347, 1.2334565333333334, 1.2344193333333342, 1.2358170833333326, 1.239111166666664, 1.240069499999998, 1.245028666666666, 1.2542086333333318, 1.2577939, 1.25775085, 1.2582950833333346, 1.2589427666666655, 1.2546563333333351, 1.242419166666666, 1.218888616666665, 1.1303227249999988, 0.8470383000000007, 0.9325239333333342, 0.9765731376190473, 1.0578793999999994, 1.0850823333333321, 1.081931199999999, 1.0812200000000012, 1.0805410000000009, 1.0789059999999997, 1.0752059999999999, 1.0750334666666654, 1.0707136666666655, 1.0643716666666685, 1.0519164666666678, 1.0520161333333333, 1.0488384999999996, 1.0464049999999991, 1.0457359523809542, 1.039436933333335, 1.0451145666666655, 1.050312800000001, 1.0503940000000014, 1.0519164666666678, 1.0605906666666653, 1.0648214666666656, 1.0655295000000005, 1.0670900000000005, 1.070843833333334, 1.0713250833333325, 1.0742459999999996, 1.0743430000000007, 1.0751573333333344, 1.075946600000001, 1.0768646000000013, 1.0788209999999985, 1.0801800000000021, 1.081931199999999, 1.0830354333333325, 1.0570601633333323, 0.9765731376190473, 0.9325239333333342, 3.8707651833333303, 3.988005249999995, 4.0233972499999995, 4.043914483333336, 4.043914483333336, 4.046093750000003, 4.041102000000003, 2.784534133333336, 1.1269039500000004, 0.7155098666666668, 1.5994768333333333, 1.6005083333333325, 1.6005178, 1.6028841833333334, 1.6019969999999975, 1.598720478571429, 1.5946484999999992, 1.6030049166666642, 1.6028841833333334, 1.6005909999999994, 1.5863885000000015, 1.4605852499999998, 0.9376653000000009, 0.6606874638095239, 1.0788829499999988, 1.1915149000000016, 1.3095960000000004, 1.3166784999999976, 1.3208891666666676, 1.3213838666666675, 1.3174533333333327, 1.3072015833333341, 1.3034390000000005, 1.2944651666666667, 1.293118166666666, 1.2987923000000012, 1.3016290500000016, 1.3034390000000005, 1.3050187500000012, 1.3184799999999983, 1.3131657499999998, 1.310479250000001, 1.3095960000000004, 0.9889325733333326, 0.775764589999999, 0.565720933333333, 0.9478961000000011, 1.1062024666666677, 1.1272429999999993, 1.1376153000000013, 1.1371489666666674, 1.132945933333333, 1.1277965000000005, 1.1271180000000012, 1.126497000000001, 1.1202064999999994, 1.1196428333333344, 1.1095486666666685, 1.1066750000000014, 1.1038041428571423, 1.1026762000000006, 1.1010328666666647, 1.0957873499999984, 1.0933331666666657, 1.099091499999998, 1.1038041428571423, 1.1146148857142861, 1.1202064999999994, 1.124689666666666, 1.126497000000001, 1.129128583333333, 1.1294598333333319, 1.1321599999999983, 1.1324995666666644, 1.1379623999999982, 1.1383634999999983, 1.1378969999999997, 1.1321787500000007, 1.0418638916666663, 0.9049445000000016, 1.883504159999999, 1.7743486183333332, 1.9765114166666677, 2.61398513333333, 2.6110427809523844, 2.5179562500000032, 2.4001137066666645, 0.7097960000000003, 0.8323271933333338, 0.83258262, 0.8311260000000006, 0.8305476000000005, 0.8297200000000001, 0.8309346466666669, 0.8339965749999995, 0.8323271933333338, 0.8323688000000005, 0.8320283800000005, 0.8323385800000002, 0.6207107, 0.6759209000000009, 0.6251605050000003, 0.6711099899999996, 0.6795084749999998, 0.6833902833333336, 0.6869516249999991, 0.6847748499999997, 0.6763649600000002, 0.6702243000000006, 0.6711742100000009, 0.6777188, 0.6784222066666673, 0.6810976809523812, 0.6865686833333337, 0.6794026749999987, 0.6711099899999996, 0.6251605050000003, 0.5841725000000004, 0.5964832000000007, 0.5972331499999999, 0.597157199999999, 0.5963255333333329, 0.5959951095238091, 0.5942334199999995, 0.589034703333334, 0.5885392000000004, 0.5850634000000008, 0.5811914583333336, 0.5745745566666653, 0.5744277733333326, 0.5732861150000006, 0.5744439416666657, 0.5779129666666664, 0.5813383733333335, 0.5880239333333328, 0.5885392000000004, 0.589034703333334, 0.5895310249999995, 0.5918379599999994, 0.5921995666666672, 0.5951252699999992, 0.5950126833333326, 0.5959951095238091, 0.5963255333333329, 0.5966989083333325, 0.5962086, 0.5958740000000007, 0.5841725000000004, 0.5817622000000005, 0.5782976999999999, 0.5274234999999997, 1.1222315333333335, 2.1332803333333303, 2.203584199999998, 2.2113717500000023, 2.203584199999998, 1.8025719333333337, 1.1222315333333335, 0.8728429566666661, 0.8717840983333329, 0.8725241716666666, 0.872275625, 0.8704514749999993, 0.873406516666666, 0.8731160666666669, 0.8725241716666666, 0.8717840983333329, 0.8728159833333342, 0.711963028333333, 0.7148486452380952, 0.7189499699999995, 0.7197300099999999, 0.7211828000000006, 0.7180561500000004, 0.7099847549999997, 0.7088781666666663, 0.7050347150000001, 0.7107607576190484, 0.713839475, 0.7157778250000004, 0.7201154933333328, 0.72116387, 0.7212159166666668, 0.7211828000000006, 0.7208729700000005, 0.7189499699999995, 0.7182113999999994, 0.7172856999999996, 0.7151784369047621, 0.7148486452380952, 0.711963028333333, 0.7036361500000002, 0.605419504999999, 0.5389833333333339, 0.5775351866666674, 0.5775351866666674, 0.6251295849999997, 0.626084315000001, 0.6254835464285713, 0.6254568964285709, 0.6248384383333327, 0.6244685499999995, 0.622909721666667, 0.6220391833333322, 0.6217705083333323, 0.6210828633333336, 0.6201866449999988, 0.617202379999999, 0.6166044333333341, 0.6160204166666682, 0.6152843250000005, 0.6082889433333323, 0.6058262333333336, 0.6058324999999997, 0.6049286599999993, 0.6001743999999994, 0.6003523999999991, 0.6017227750000005, 0.6042056599999994, 0.6049286599999993, 0.6094211999999993, 0.611711, 0.6137341950000001, 0.6160204166666682, 0.6183994049999993, 0.6187311833333327, 0.6187923499999991, 0.6201797200000001, 0.6210828633333336, 0.6214057833333336, 0.6220391833333322, 0.622568766666666, 0.622909721666667, 0.624089775, 0.625653681666666, 0.6254835464285713, 0.62617374, 0.6259124999999994, 0.623691400000001, 0.6141613166666658, 0.5775351866666674], \"yaxis\": \"y\"}, {\"hovertemplate\": \"variable=GBC<br>index=%{x}<br>value=%{y}<extra></extra>\", \"legendgroup\": \"GBC\", \"line\": {\"color\": \"#ab63fa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"GBC\", \"showlegend\": true, \"type\": \"scattergl\", \"x\": [2, 8, 10, 15, 23, 24, 28, 38, 42, 43, 44, 47, 51, 64, 80, 84, 87, 90, 92, 94, 99, 100, 103, 109, 117, 120, 128, 136, 146, 148, 159, 164, 170, 175, 177, 179, 184, 187, 189, 194, 200, 204, 208, 209, 218, 222, 225, 226, 227, 228, 230, 233, 238, 239, 242, 243, 249, 251, 252, 256, 257, 262, 263, 267, 270, 272, 273, 274, 276, 277, 285, 289, 290, 291, 293, 295, 302, 303, 306, 307, 309, 310, 313, 316, 328, 335, 346, 349, 350, 361, 367, 376, 380, 396, 400, 405, 408, 411, 412, 415, 423, 424, 430, 431, 432, 443, 446, 449, 451, 452, 453, 456, 465, 471, 482, 490, 498, 512, 513, 514, 516, 518, 520, 532, 550, 552, 557, 574, 579, 583, 589, 591, 607, 612, 616, 620, 626, 629, 642, 643, 644, 652, 657, 659, 665, 669, 670, 680, 683, 686, 688, 696, 700, 702, 708, 709, 710, 712, 714, 720, 722, 725, 727, 733, 740, 741, 744, 745, 746, 756, 765, 766, 767, 770, 771, 773, 777, 780, 782, 783, 785, 789, 791, 793, 794, 801, 805, 808, 811, 817, 827, 831, 832, 834, 842, 843, 849, 858, 862, 865, 867, 870, 871, 879, 883, 888, 889, 890, 893, 896, 897, 899, 904, 906, 911, 912, 913, 914, 917, 919, 922, 925, 926, 942, 944, 947, 957, 958, 959, 963, 969, 971, 972, 979, 983, 986, 995, 996, 998, 1001, 1005, 1007, 1009, 1016, 1019, 1023, 1027, 1029, 1033, 1036, 1041, 1042, 1046, 1049, 1052, 1054, 1061, 1065, 1067, 1074, 1078, 1082, 1084, 1087, 1088, 1104, 1106, 1107, 1108, 1114, 1116, 1119, 1123, 1126, 1127, 1138, 1140, 1149, 1160, 1161, 1162, 1171, 1173, 1179, 1180, 1182, 1183, 1185, 1189, 1196, 1199, 1204, 1209, 1215, 1220, 1224, 1228, 1229, 1234, 1235, 1243, 1254, 1255, 1258, 1262, 1264, 1273, 1274, 1278, 1290, 1295, 1298, 1302, 1304, 1316, 1318, 1327, 1328, 1331, 1334, 1339, 1344, 1345, 1346, 1347, 1353, 1354, 1358, 1360, 1362, 1363, 1367, 1376, 1385, 1394, 1396, 1404, 1405, 1407, 1415, 1423, 1424, 1426, 1432, 1434, 1438, 1439, 1441, 1446, 1452, 1468, 1470, 1479, 1482, 1491, 1492, 1493, 1502, 1508, 1511, 1520, 1522, 1524, 1528, 1536, 1537, 1544, 1545, 1546, 1551, 1552, 1557, 1560, 1561, 1562, 1563, 1565, 1567, 1572, 1576, 1583, 1588, 1594, 1597, 1606, 1609, 1611, 1616, 1617, 1629, 1630, 1631, 1633, 1635, 1650, 1652, 1656, 1657, 1660, 1668, 1677, 1678, 1681, 1692, 1695, 1697, 1701, 1704, 1705, 1706, 1707, 1710, 1711, 1712, 1713, 1714, 1716, 1717, 1720, 1721, 1722, 1723, 1733, 1737, 1740, 1741, 1744, 1745, 1750, 1752, 1753, 1756, 1760, 1769, 1771, 1772, 1774, 1780, 1784, 1785, 1786, 1792, 1795, 1797, 1799, 1800, 1807, 1808, 1810, 1814, 1820, 1821, 1822, 1827, 1830, 1831, 1834, 1835, 1836, 1839, 1844, 1846, 1851, 1852, 1857, 1861, 1864], \"xaxis\": \"x\", \"y\": [1.4413480196246111, 1.6817736680956321, 2.0741253520509564, 2.9837602533485774, 3.0140336868063278, 3.016332072637105, 3.012124752364352, 0.9084002186617329, 0.9165131806369282, 0.9165131806369282, 0.9165131806369282, 0.9165131806369282, 1.050637630431406, 1.1274709663957303, 1.050637630431406, 1.0408112806215937, 0.82763998785568, 0.8211677929541876, 0.8211677929541876, 0.8267281945640217, 0.8392130792670344, 0.8392130792670344, 0.8392130792670344, 0.8724947449620986, 0.895636140018853, 0.8926927816583998, 0.9610907565600673, 0.8904847646483097, 0.8941902880385784, 0.87137819089737, 0.8392130792670344, 0.8392130792670344, 0.7266985970795709, 0.7460637092693108, 0.7676881557232096, 0.7676881557232096, 0.7676881557232096, 0.7676881557232096, 0.7730818261925848, 0.7730818261925848, 0.7730818261925848, 0.7730818261925848, 0.7730818261925848, 0.7666096312910924, 0.7801954312796229, 0.7801954312796229, 0.7801954312796229, 0.7801954312796229, 0.7801954312796229, 0.7801954312796229, 0.7794057382825397, 0.7773640085728841, 0.775155991562794, 0.775155991562794, 0.7731533805104698, 0.7731533805104698, 0.775155991562794, 0.775155991562794, 0.7773640085728841, 0.7794057382825397, 0.7782978617196726, 0.7801954312796229, 0.7801954312796229, 0.7801954312796229, 0.7801954312796229, 0.7721700329009265, 0.7721700329009265, 0.7721700329009265, 0.7721700329009265, 0.7666096312910924, 0.7730818261925848, 0.7730818261925848, 0.7730818261925848, 0.7730818261925848, 0.7730818261925848, 0.7730818261925848, 0.7676881557232096, 0.7676881557232096, 0.7676881557232096, 0.7676881557232096, 0.7676881557232096, 0.7676881557232096, 0.6701516690943574, 1.602586642717014, 2.9423802022687973, 2.9135296383416227, 1.0413751998355416, 0.7708194280772646, 0.8591564259544824, 1.1235048500977016, 1.1335495386149308, 1.1335495386149308, 1.1335495386149308, 0.7708194280772646, 0.8552857613061718, 0.8882236129856963, 0.8882236129856963, 0.9293178329944262, 0.9293178329944262, 0.9293178329944262, 0.930763684974701, 0.9298620563239033, 0.9256123096041576, 0.9256123096041576, 0.9256123096041576, 0.9256123096041576, 0.9278203266142477, 0.930763684974701, 0.9293178329944262, 0.9293178329944262, 0.9293178329944262, 0.9293178329944262, 0.8882236129856963, 0.881221999777359, 0.8034124108737368, 0.8081927443609855, 0.8129897046356814, 0.8129897046356814, 0.8129897046356814, 0.8129897046356814, 0.8120779113440231, 0.8120779113440231, 0.8120779113440231, 0.8201033097227195, 0.8082809254663177, 0.8102835365186419, 0.8102835365186419, 0.8201033097227195, 0.8120779113440231, 0.806517509734189, 0.8129897046356814, 0.8129897046356814, 0.8081927443609855, 0.8081927443609855, 0.8034124108737368, 0.728138391448806, 1.7937947982552553, 2.2800385660810494, 3.591573639021755, 3.595780959294508, 3.595780959294508, 3.5455077719786394, 3.2828769106667948, 2.9755757287979656, 1.2456677522270625, 1.3221881152819384, 1.3221881152819384, 1.4786142666104083, 1.4886589551276375, 1.4886589551276375, 1.5266941912693366, 1.5266941912693366, 1.4886589551276375, 1.4886589551276375, 1.4729673042948777, 1.4078930071335756, 1.397462413230191, 1.3418923174923127, 1.3221881152819384, 1.064974896438853, 1.064974896438853, 1.064974896438853, 1.2027013315295105, 1.2027013315295105, 1.2027013315295105, 1.2027013315295105, 1.1991848909520428, 1.2265412929563608, 1.2330168396694061, 1.2287670929496604, 1.2287670929496604, 1.2287670929496604, 1.2287670929496604, 1.2309751099597506, 1.2309751099597506, 1.2330168396694061, 1.2027013315295105, 1.2027013315295105, 1.2027013315295105, 1.2027013315295105, 1.2027013315295105, 1.2027013315295105, 1.2027013315295105, 1.2027013315295105, 1.2027013315295105, 0.8938166108262954, 0.9228157898814047, 1.035669400232966, 1.0574511185182844, 1.0574511185182844, 1.0574511185182844, 1.0574511185182844, 1.0574511185182844, 1.0574511185182844, 1.056539325226626, 1.056539325226626, 1.0645647236053224, 1.0645647236053224, 1.0935463154186575, 1.091504585709002, 1.0892965686989118, 1.0892965686989118, 1.0892965686989118, 1.0892965686989118, 1.0892965686989118, 1.091504585709002, 1.091504585709002, 1.0935463154186575, 1.062667154045372, 1.0645647236053224, 1.0645647236053224, 1.0645647236053224, 1.0645647236053224, 1.0645647236053224, 1.056539325226626, 1.056539325226626, 1.056539325226626, 1.056539325226626, 1.0574511185182844, 1.0574511185182844, 1.0574511185182844, 1.0574511185182844, 1.0574511185182844, 1.0574511185182844, 1.035669400232966, 0.9228157898814047, 3.8702714734443417, 3.8952370546310715, 3.8952370546310715, 3.916506272872338, 3.916506272872338, 3.9046853515860533, 3.9046853515860533, 2.8591629098122877, 1.1560361705812516, 0.9965971747967023, 1.5226814356513285, 1.5226814356513285, 1.5226814356513285, 1.5251660950109724, 1.5472466522958683, 1.5472466522958683, 1.5501661894116276, 1.5472466522958683, 1.5251660950109724, 1.5226814356513285, 1.5226814356513285, 1.4640497142422326, 1.117165410850216, 0.8708968993542583, 1.11503618978528, 1.1847076349959733, 1.2521780196830834, 1.2521780196830834, 1.2521780196830834, 1.2521780196830834, 1.2486615791056157, 1.2804517981133234, 1.2804517981133234, 1.2782437811032332, 1.276241170050909, 1.2782437811032332, 1.2782437811032332, 1.2804517981133234, 1.2804517981133234, 1.2521780196830834, 1.2521780196830834, 1.2521780196830834, 1.2521780196830834, 1.1023968911329334, 0.8925782326875917, 0.5790243169174281, 0.948085139617122, 1.1096958453802828, 1.120210578080728, 1.1211193324873834, 1.1211193324873834, 1.1211193324873834, 1.120207539195725, 1.1226152958920288, 1.1226152958920288, 1.1226152958920288, 1.1226152958920288, 1.128965568031659, 1.1255892495630684, 1.1255892495630684, 1.1255892495630684, 1.1228999825529782, 1.1210281943744538, 1.119824043671054, 1.1225819852860845, 1.1255892495630684, 1.127363795727488, 1.1226152958920288, 1.1226152958920288, 1.1226152958920288, 1.120207539195725, 1.120207539195725, 1.1211193324873834, 1.1211193324873834, 1.1211193324873834, 1.1211193324873834, 1.1211193324873834, 1.1211193324873834, 1.0572209630734677, 0.9417797995331111, 1.8890208579607008, 1.7570592509984182, 2.1035565207905287, 2.62320516970114, 2.621296235259164, 2.360246451474126, 2.310922758755142, 0.7430669058769286, 0.7650528690026628, 0.8315417347252155, 0.8384840470231387, 0.8285196327006729, 0.8285196327006729, 0.8384840470231387, 0.845858840230202, 0.7650528690026628, 0.7650528690026628, 0.7650528690026628, 0.7650528690026628, 0.6430318142201776, 0.6430318142201776, 0.6430318142201776, 0.6430318142201776, 0.6430318142201776, 0.6365596193186852, 0.6491216341368565, 0.68219045576689, 0.6859506544454586, 0.7398177613147785, 0.6851950320640818, 0.686943600764599, 0.68219045576689, 0.68219045576689, 0.68219045576689, 0.6430318142201776, 0.6430318142201776, 0.6430318142201776, 0.5862250582671887, 0.5862250582671887, 0.5862250582671887, 0.5882047395457417, 0.5891374490327167, 0.5891374490327167, 0.5921081383117323, 0.5911963450200739, 0.5911963450200739, 0.5911963450200739, 0.5911963450200739, 0.6026942182140014, 0.6026942182140014, 0.6026942182140014, 0.6026942182140014, 0.5892987754601237, 0.5911963450200739, 0.5911963450200739, 0.5911963450200739, 0.5911963450200739, 0.5921081383117323, 0.5921081383117323, 0.5921081383117323, 0.5921081383117323, 0.5891374490327167, 0.5891374490327167, 0.5891374490327167, 0.5891374490327167, 0.5862250582671887, 0.5862250582671887, 0.5862250582671887, 0.5862250582671887, 0.5862250582671887, 0.5784202469726393, 0.9290664968440012, 2.014235923667022, 2.2881226993534645, 2.2448315857624745, 2.2881226993534645, 1.7516359457113624, 0.9290664968440012, 0.815982946242669, 0.8691512538471331, 0.8839050368193696, 0.8760935661450563, 0.8760935661450563, 0.8807888220223115, 0.8839050368193696, 0.8839050368193696, 0.8691512538471331, 0.844662632647778, 0.6696920977492334, 0.6696920977492334, 0.6723360807821935, 0.6951481779234019, 0.7282169995534353, 0.7282169995534353, 0.7195491836054244, 0.7148539277281692, 0.7148539277281692, 0.7217572006155145, 0.7247005589759677, 0.7282169995534353, 0.7282169995534353, 0.7282169995534353, 0.7282169995534353, 0.7282169995534353, 0.6951481779234019, 0.6723360807821935, 0.6653344675738563, 0.6597740659640222, 0.6696920977492334, 0.6696920977492334, 0.6696920977492334, 0.6696920977492334, 0.6562025514118967, 0.6562025514118967, 0.6154278230532134, 0.6154278230532134, 0.621949313417716, 0.6248617041832439, 0.6248617041832439, 0.6248617041832439, 0.6248617041832439, 0.6242516933354635, 0.6242516933354635, 0.6242516933354635, 0.6242516933354635, 0.6242516933354635, 0.6242516933354635, 0.6233399000438051, 0.6233399000438051, 0.6233399000438051, 0.6233399000438051, 0.6233399000438051, 0.6155461847793241, 0.6155461847793241, 0.613338167769234, 0.6066403008396546, 0.6066403008396546, 0.6086429118919788, 0.6086429118919788, 0.613338167769234, 0.6233399000438051, 0.6233399000438051, 0.6233399000438051, 0.6233399000438051, 0.6233399000438051, 0.6233399000438051, 0.6233399000438051, 0.6242516933354635, 0.6242516933354635, 0.6242516933354635, 0.6242516933354635, 0.6242516933354635, 0.6242516933354635, 0.6242516933354635, 0.6248617041832439, 0.6248617041832439, 0.621949313417716, 0.621949313417716, 0.621949313417716, 0.6190085231800094, 0.6154278230532134], \"yaxis\": \"y\"}],                        {\"legend\": {\"title\": {\"text\": \"variable\"}, \"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"index\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"value\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('9a4a6ee2-3778-41db-9965-59ef7308df7b');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torep = pd.DataFrame(y_test.values, columns = ['Data'], index=y_test.index)\n",
    "torep['ANN'] = DenseAdamPrediction5\n",
    "torep['RFC'] = RFC\n",
    "torep['GBC'] = GBC\n",
    "TOREP = torep.iloc[:, :].sort_index()\n",
    "TOREP.plot(x=TOREP.index, y=['Data', 'ANN', 'RFC', 'GBC'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "variable=señal<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "señal",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "mode": "lines",
         "name": "señal",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39
         ],
         "xaxis": "x",
         "y": [
          0.6061,
          0.5017199999999999,
          1.0674,
          0.61416,
          0.6765100000000001,
          0.61165,
          0.9469799999999999,
          0.6339899999999999,
          0.8710399999999999,
          0.7925800000000001,
          0.81788,
          0.93976,
          0.71763,
          1.1096,
          0.7520899999999999,
          1.1566,
          1.1199,
          0.62097,
          1.2945,
          1.2389,
          1.0221,
          0.6221800000000001,
          0.8186399999999999,
          0.7819699999999999,
          1.0676,
          1.26,
          0.50716,
          0.71532,
          0.82004,
          0.90781,
          0.93891,
          1.3026,
          0.71337,
          0.75536,
          0.77772,
          0.7104699999999999,
          0.85883,
          0.9490700000000001,
          0.68693,
          1.0944
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "variable"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "index"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "value"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"518ba2ab-0a00-4b25-ac96-0c5eda3843e4\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"518ba2ab-0a00-4b25-ac96-0c5eda3843e4\")) {                    Plotly.newPlot(                        \"518ba2ab-0a00-4b25-ac96-0c5eda3843e4\",                        [{\"hovertemplate\": \"variable=se\\u00f1al<br>index=%{x}<br>value=%{y}<extra></extra>\", \"legendgroup\": \"se\\u00f1al\", \"line\": {\"color\": \"#636efa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"se\\u00f1al\", \"orientation\": \"v\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], \"xaxis\": \"x\", \"y\": [0.6061, 0.5017199999999999, 1.0674, 0.61416, 0.6765100000000001, 0.61165, 0.9469799999999999, 0.6339899999999999, 0.8710399999999999, 0.7925800000000001, 0.81788, 0.93976, 0.71763, 1.1096, 0.7520899999999999, 1.1566, 1.1199, 0.62097, 1.2945, 1.2389, 1.0221, 0.6221800000000001, 0.8186399999999999, 0.7819699999999999, 1.0676, 1.26, 0.50716, 0.71532, 0.82004, 0.90781, 0.93891, 1.3026, 0.71337, 0.75536, 0.77772, 0.7104699999999999, 0.85883, 0.9490700000000001, 0.68693, 1.0944], \"yaxis\": \"y\"}],                        {\"legend\": {\"title\": {\"text\": \"variable\"}, \"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"index\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"value\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('518ba2ab-0a00-4b25-ac96-0c5eda3843e4');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(y_test).reset_index(drop=True)[:40].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "variable=0<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "0",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "mode": "lines",
         "name": "0",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39
         ],
         "xaxis": "x",
         "y": [
          0.6189404726028442,
          1.094630479812622,
          1.0906702280044556,
          0.6185215711593628,
          0.590030312538147,
          0.6208969354629517,
          0.9496179819107056,
          0.8302586078643799,
          0.8576993942260742,
          0.8156758546829224,
          0.8423682451248169,
          0.9631247520446777,
          0.6975315809249878,
          1.135913372039795,
          0.7642502784729004,
          1.1687802076339722,
          1.1404016017913818,
          0.6271200776100159,
          1.3069610595703125,
          1.2515740394592285,
          1.1515867710113525,
          0.627106785774231,
          0.8442994356155396,
          0.7917983531951904,
          1.0895501375198364,
          1.278951644897461,
          0.5907986164093018,
          0.6681436896324158,
          0.8459084033966064,
          0.9054857492446899,
          0.950581431388855,
          1.3224619626998901,
          0.6594197750091553,
          0.7809392213821411,
          0.7910295724868774,
          0.7079041004180908,
          1.0935983657836914,
          0.9774333238601685,
          0.6767206192016602,
          1.1304681301116943
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "variable"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "index"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "value"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"c1ac112b-7705-4ee5-8f46-9d482f5c3e53\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c1ac112b-7705-4ee5-8f46-9d482f5c3e53\")) {                    Plotly.newPlot(                        \"c1ac112b-7705-4ee5-8f46-9d482f5c3e53\",                        [{\"hovertemplate\": \"variable=0<br>index=%{x}<br>value=%{y}<extra></extra>\", \"legendgroup\": \"0\", \"line\": {\"color\": \"#636efa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"0\", \"orientation\": \"v\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], \"xaxis\": \"x\", \"y\": [0.6189404726028442, 1.094630479812622, 1.0906702280044556, 0.6185215711593628, 0.590030312538147, 0.6208969354629517, 0.9496179819107056, 0.8302586078643799, 0.8576993942260742, 0.8156758546829224, 0.8423682451248169, 0.9631247520446777, 0.6975315809249878, 1.135913372039795, 0.7642502784729004, 1.1687802076339722, 1.1404016017913818, 0.6271200776100159, 1.3069610595703125, 1.2515740394592285, 1.1515867710113525, 0.627106785774231, 0.8442994356155396, 0.7917983531951904, 1.0895501375198364, 1.278951644897461, 0.5907986164093018, 0.6681436896324158, 0.8459084033966064, 0.9054857492446899, 0.950581431388855, 1.3224619626998901, 0.6594197750091553, 0.7809392213821411, 0.7910295724868774, 0.7079041004180908, 1.0935983657836914, 0.9774333238601685, 0.6767206192016602, 1.1304681301116943], \"yaxis\": \"y\"}],                        {\"legend\": {\"title\": {\"text\": \"variable\"}, \"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"index\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"value\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('c1ac112b-7705-4ee5-8f46-9d482f5c3e53');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(DenseRMSpropPrediction4)[:40].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1784    0.60610\n",
       "947     0.50172\n",
       "897     1.06740\n",
       "1861    0.61416\n",
       "1354    0.67651\n",
       "         ...   \n",
       "843     1.07280\n",
       "1769    0.61741\n",
       "90      0.82124\n",
       "1316    0.83277\n",
       "963     4.03820\n",
       "Name: señal, Length: 467, dtype: float64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "426.667px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
